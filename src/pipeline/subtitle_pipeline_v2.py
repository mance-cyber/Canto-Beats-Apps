"""
Subtitle Pipeline V2 - AI-powered transcription with context-aware conversion.

Uses Whisper for ASR + Qwen2 LLM for intelligent colloquial-to-written conversion.
"""

import tempfile
from pathlib import Path
from typing import List, Optional, Callable
from dataclasses import dataclass

from core.config import Config
from core.hardware_detector import HardwareDetector, PerformanceProfile
from models.whisper_asr import WhisperASR
from models.qwen_llm import QwenLLM
from models.vad_processor import VADProcessor
from utils.logger import setup_logger

# Try to import MLX Whisper for Apple Silicon acceleration
try:
    from utils.whisper_mlx import MLXWhisperASR, get_best_whisper_backend
    HAS_MLX_WHISPER = MLXWhisperASR.is_available()
except ImportError:
    HAS_MLX_WHISPER = False

logger = setup_logger()


@dataclass
class SubtitleEntryV2:
    """A single subtitle entry with dual-language support."""
    start: float
    end: float
    colloquial: str  # å£èªž - spoken Cantonese
    formal: Optional[str] = None  # æ›¸é¢èªž - written Chinese


class SubtitlePipelineV2:
    """
    V2 pipeline with AI-powered colloquial-to-written conversion.
    
    Pipeline stages:
    1. Hardware detection
    2. Model loading (Whisper ASR + optional Qwen LLM)
    3. Audio extraction (if video)
    4. Whisper transcription (native segmentation)
    5. Optional LLM refinement (context-aware conversion)
    """
    
    def __init__(self, config: Config, force_cpu: bool = False, enable_llm: bool = True):
        """
        Initialize pipeline.
        
        Args:
            config: Application configuration
            force_cpu: Force CPU mode
            enable_llm: Enable LLM refinement for better conversion
        """
        self.config = config
        self.force_cpu = force_cpu
        self.enable_llm = enable_llm
        self.profile = None
        self.asr = None
        self.llm = None
        self.vad = None  # VAD processor for smart segmentation
        self._models_loaded = False
        
        # Create temp directory
        self.temp_dir = Path(tempfile.gettempdir()) / "canto_beats_v2"
        self.temp_dir.mkdir(exist_ok=True)
        
        # Initialize hardware detection
        self._setup_hardware()
    
    def _setup_hardware(self):
        """Detect hardware and determine optimal configuration."""
        logger.info("Detecting hardware configuration...")
        detector = HardwareDetector()
        self.profile = detector.detect(force_cpu=self.force_cpu)
        
        logger.info(f"Hardware tier: {self.profile.tier.value}")
        logger.info(f"Device: {self.profile.device}")
        logger.info(f"VRAM: {self.profile.vram_gb} GB")
        logger.info(f"LLM refinement: {'enabled' if self.enable_llm else 'disabled'}")
    
    def _load_asr(self, progress_callback: Optional[Callable] = None, status_callback: Optional[Callable] = None):
        """Load ASR model with Apple Silicon priority: CoreML > MPS > CPU.
        
        Args:
            progress_callback: Callback for progress percentage (0-100)
            status_callback: Callback for status message updates (e.g., "æ­£åœ¨ä¸‹è¼‰æ¨¡åž‹...")
        """
        if self.asr is not None and self.asr.is_loaded:
            return

        if progress_callback:
            progress_callback(10)

        # Priority: MLX Whisper (CoreML/MPS) > faster-whisper (CPU)
        if HAS_MLX_WHISPER:
            logger.info(f"ðŸŽ Loading MLX Whisper (Apple Silicon optimized): {self.profile.asr_model}")
            try:
                if status_callback:
                    status_callback("æ­£åœ¨æº–å‚™ AI å·¥å…·...")
                
                self.asr = MLXWhisperASR(model_size=self.profile.asr_model)
                
                # Pass status callback to load_model for download progress
                self.asr.load_model(progress_callback=status_callback)
                
                logger.info(f"âš¡ MLX Whisper loaded on {self.asr.get_backend_type().upper()}")
                
                if status_callback:
                    status_callback("AI å·¥å…·åŠ è¼‰å®Œæˆï¼")
                return
            except Exception as e:
                logger.warning(f"MLX Whisper failed, falling back to faster-whisper: {e}")
                if status_callback:
                    status_callback("æ­£åœ¨åˆ‡æ› AI å·¥å…·...")

        # Fallback: faster-whisper (CPU)
        logger.info(f"Loading faster-whisper ASR model: {self.profile.asr_model}")
        if status_callback:
            status_callback("æ­£åœ¨åŠ è¼‰ AI å·¥å…·...")
        self.asr = WhisperASR(self.config, model_size=self.profile.asr_model)
        self.asr.load_model()
        logger.info("ASR model loaded successfully (CPU mode)")
    
    def _unload_asr(self):
        """Unload ASR model to free GPU memory for LLM."""
        import gc
        import torch
        
        if self.asr:
            logger.info("Unloading ASR model to free GPU memory...")
            try:
                self.asr.unload_model()
            except Exception as e:
                logger.warning(f"ASR unload error: {e}")
            self.asr = None
            
            # Force garbage collection and clear GPU cache
            gc.collect()
            # Cross-platform GPU memory cleanup
            if torch.backends.mps.is_available():
                torch.mps.empty_cache()
            elif torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
            logger.info("ASR unloaded, GPU memory freed")
    
    def _load_llm(self, progress_callback: Optional[Callable] = None):
        """Load LLM model (call after ASR is unloaded for memory efficiency)."""
        if not self.enable_llm or not self.profile.llm_a_enabled:
            logger.info("LLM refinement disabled, skipping LLM load")
            return
            
        if self.llm is not None:
            return
            
        logger.info("Loading LLM for AI refinement...")
        if progress_callback:
            progress_callback(82)
        self.llm = QwenLLM(self.config, self.profile)
        self.llm.load_models()
        logger.info("LLM loaded successfully")
    
    def _extract_audio(self, video_path: Path) -> str:
        """Extract audio from video file."""
        import subprocess
        from core.path_setup import get_ffmpeg_path
        
        audio_path = self.temp_dir / f"{video_path.stem}_audio.wav"
        
        logger.info(f"Extracting audio from video: {video_path}")
        
        # Use get_ffmpeg_path() to get full path (works in packaged app)
        ffmpeg_path = get_ffmpeg_path()
        logger.info(f"Using FFmpeg: {ffmpeg_path}")
        
        # Check if FFmpeg actually exists at that path
        ffmpeg_exists = Path(ffmpeg_path).exists() if ffmpeg_path != 'ffmpeg' else True
        if not ffmpeg_exists and ffmpeg_path != 'ffmpeg':
            logger.error(f"FFmpeg not found at: {ffmpeg_path}")
            raise FileNotFoundError(
                f"FFmpeg æœªæ‰¾åˆ°ï¼è«‹å®‰è£ FFmpeg:\n"
                f"  macOS: brew install ffmpeg\n"
                f"  é æœŸè·¯å¾‘: {ffmpeg_path}"
            )
        
        cmd = [
            ffmpeg_path, '-y', '-i', str(video_path),
            '-vn', '-acodec', 'pcm_s16le', '-ar', '16000', '-ac', '1',
            str(audio_path)
        ]
        
        logger.info(f"FFmpeg command: {' '.join(cmd)}")
        
        try:
            # Add timeout (5 minutes should be enough for most videos)
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=300  # 5 minute timeout
            )
            
            if result.returncode != 0:
                logger.error(f"FFmpeg stderr: {result.stderr}")
                raise RuntimeError(f"FFmpeg å¤±æ•—: {result.stderr[:500]}")
            
            # Verify output file exists
            if not audio_path.exists():
                raise RuntimeError(f"éŸ³é »æª”æ¡ˆæœªç”Ÿæˆ: {audio_path}")
            
            logger.info(f"Audio extracted to: {audio_path}")
            return str(audio_path)
            
        except subprocess.TimeoutExpired:
            logger.error("FFmpeg timeout after 5 minutes")
            raise RuntimeError("FFmpeg è¶…æ™‚ï¼ˆè¶…éŽ5åˆ†é˜ï¼‰ï¼Œè«‹æª¢æŸ¥å½±ç‰‡æª”æ¡ˆæ˜¯å¦æ­£ç¢º")
        except FileNotFoundError as e:
            logger.error(f"FFmpeg not found: {e}")
            raise FileNotFoundError(
                f"FFmpeg æœªæ‰¾åˆ°ï¼è«‹å®‰è£ FFmpeg:\n"
                f"  macOS: brew install ffmpeg"
            )
    
    # ==================== ç²µèªžéŒ¯å­—æ¸…å–® (æ“´å±•ç‰ˆ) ====================
    # Whisper å¸¸è¦‹ç²µèªžè½‰éŒ„éŒ¯èª¤ â†’ æ­£ç¢ºå¯«æ³•
    CANTONESE_CORRECTIONS = {
        # ===== å¸¸è¦‹å­—ç¬¦éŒ¯èª¤ =====
        "æ—¢": "å˜…",      # çš„ (possessive)
        "ç³»": "ä¿‚",      # æ˜¯ (to be)
        "è·": "ä½¢",      # ä»–/å¥¹ (he/she)
        "å°¼": "å‘¢",      # é€™ (this)
        "é»Ž": "åšŸ",      # ä¾† (come)
        "ç•€": "ä¿¾",      # çµ¦ (give)
        "é‡Ž": "å˜¢",      # æ±è¥¿ (thing)
        "åšŸ": "ä¾†",      # æœ‰æ™‚ Whisper æœƒåå‘éŒ¯
        "å¾": "å””",      # ä¸
        "è¾¹": "é‚Š",      # å“ªè£¡
        "å™‰": "å’",      # é€™æ¨£
        "å—°": "å€‹",      # é‚£å€‹ (sometimes reversed)
        "D": "å•²",       # äº›
        "d": "å•²",
        "ä»²": "é‡",      # still (æœ‰æ™‚æœƒåå‘)
        "å˜›": "å–å˜›",    # èªžæ°£è©ž
        
        # ===== åŒéŸ³å­—/è«§éŸ³éŒ¯èª¤ =====
        "ç·Šä¿‚": "æ¢—ä¿‚",   # ç•¶ç„¶ (of course)
        "æ¢—ç³»": "æ¢—ä¿‚",
        "ç‚¹è§£": "é»žè§£",   # ç‚ºä»€éº¼
        "ç‚¹æ ·": "é»žæ¨£",   # æ€Žæ¨£
        "ç‚¹ç®—": "é»žç®—",   # æ€Žéº¼è¾¦
        "å’©äº‹": "ä¹œäº‹",   # ä»€éº¼äº‹
        "ä¹œé‡Ž": "ä¹œå˜¢",   # ä»€éº¼æ±è¥¿
        "ç³»å’ª": "ä¿‚å’ª",   # æ˜¯ä¸æ˜¯
        "ç³»å””ç³»": "ä¿‚å””ä¿‚",
        "å””ç³»": "å””ä¿‚",   # ä¸æ˜¯
        "æœ‰ç„¡": "æœ‰å†‡",   # æœ‰æ²’æœ‰
        "æœ‰æ²¡": "æœ‰å†‡",
        "å†‡ç„¡": "å†‡",
        "æœ‰D": "æœ‰å•²",
        
        # ===== æˆèªžåŒéŸ³å­—æ ¡æ­£ (é‡è¦!) =====
        "å…‹è‹¦ä¾†è·¯": "åˆ»è‹¦è€å‹ž",
        "åˆ»è‹¦ä¾†å‹ž": "åˆ»è‹¦è€å‹ž",
        "å…‹è‹¦è€å‹ž": "åˆ»è‹¦è€å‹ž",
        "ä¸€è¦–åŒäºº": "ä¸€è¦–åŒä»",
        "å°ˆå¿ƒè‡³å¿—": "å°ˆå¿ƒè‡´å¿—",
        "äº‹å€å…¬åŠ": "äº‹å€åŠŸåŠ",
        "äº‹åŠå…¬å€": "äº‹åŠåŠŸå€",
        "å¿ƒæ› ç¥žå®œ": "å¿ƒæ› ç¥žæ€¡",
        "ç„¡å¾®ä¸è‡³": "ç„¡å¾®ä¸è‡³",
        "è¿«ä¸å¾—å·²": "è¿«ä¸å¾—å·²",
        "å …æŒä¸æ‡ˆ": "å …æŒä¸æ‡ˆ",
        "åŠ›ä¸å¾žå¿ƒ": "åŠ›ä¸å¾žå¿ƒ",
        "èŽ«åå¥‡å¦™": "èŽ«åå…¶å¦™",
        "ä¸€é³´é©šäºº": "ä¸€é³´é©šäºº",
        "è‡ªä½œè‡ªå—": "è‡ªä½œè‡ªå—",
        "ä¸çŸ¥æ‰€è¬‚": "ä¸çŸ¥æ‰€è¬‚",
        "è¦ªåŠ›è¦ªç¶­": "è¦ªåŠ›è¦ªç‚º",
        "è¦ªåŠ›è¦ªç‚º": "è¦ªåŠ›è¦ªç‚º",
        
        # ===== ç²µèªžä¿—èªž =====
        "ç”©åº•": "ç”©åº•",   # çˆ½ç´„ (ä¿æŒåŽŸæ¨£)
        "æ”¶çš®": "æ”¶çš®",   # é–‰å˜´ (ä¿æŒåŽŸæ¨£)
        "å±ˆæ©Ÿ": "å±ˆæ©Ÿ",   # ä½œå¼Š (ä¿æŒåŽŸæ¨£)
        "å¥½Kam": "å¥½kam", # å°·å°¬
        "å¥½kam": "å¥½å°·å°¬",
        "chur": "chur",   # è¾›è‹¦ (ä¿ç•™è‹±æ–‡)
        "Chur": "chur",
        
        # ===== ç°¡é«”å­— â†’ ç¹é«”å­— =====
        "è¿™": "å‘¢",
        "ä¸ª": "å€‹",
        "è¯´": "è¬›",
        "è¯": "è©±",
        "æ—¶": "æ™‚",
        "é—´": "é–“",
        "æ¥": "åšŸ",
        "ä¼š": "æœƒ",
        "æ²¡": "å†‡",
        "å¯¹": "å°",
        "äºŽ": "æ–¼",
        "ä¸º": "ç‚º",
        "é—®": "å•",
        "é¢˜": "é¡Œ",
        "ä¸œ": "æ±",
        "å„¿": "å…’",
        "ä»¬": "å“‹",
        "è®©": "ä¿¾",
        "ç»™": "ç•€",
        "è¿™ä¸ª": "å‘¢å€‹",
        "é‚£ä¸ª": "å—°å€‹",
        "ä»€ä¹ˆ": "ä¹œå˜¢",
        "æ€Žä¹ˆ": "é»žæ¨£",
        "ä¸ºä»€ä¹ˆ": "é»žè§£",
        "çŽ°åœ¨": "è€Œå®¶",
        "çŸ¥é“": "çŸ¥é“",
        "å¯ä»¥": "å¯ä»¥",
        "éœ€è¦": "éœ€è¦",
        "åº”è¯¥": "æ‡‰è©²",
        "è¿˜æ˜¯": "å®šä¿‚",
        "æˆ–è€…": "æˆ–è€…",
        "å› ä¸º": "å› ç‚º",
        "æ‰€ä»¥": "æ‰€ä»¥",
        "ä½†æ˜¯": "ä½†ä¿‚",
        "è™½ç„¶": "é›–ç„¶",
        "å¦‚æžœ": "å¦‚æžœ",
        "å½“ç„¶": "æ¢—ä¿‚",
        
        # ===== æ•¸å­—ç›¸é—œ =====
        "ä¸¤": "å…©",
        "ä¸‡": "è¬",
        "äº¿": "å„„",
        
        # ===== è‹±æ–‡ç›¸é—œ =====
        "o k": "OK",
        "o K": "OK",
        "O K": "OK",
        "O k": "OK",
        "bye bye": "bye bye",
        "thank you": "thank you",
        
        # ===== ç²µèªžåŒéŸ³å­—æ ¡æ­£ (Whisper å¸¸è¦‹éŒ¯èª¤) =====
        "åŸ·ç²’": "åŸ·ç¬ ",       # åŸ·ç¬  = çµæ¥­
        "åŸ·å·¦ç²’": "åŸ·å·¦ç¬ ",
        "åŸ·å’—ç²’": "åŸ·å’—ç¬ ",
        "æŠ½è–ª": "æŠ½èº«",       # æŠ½èº« (é‡‘èŸ¬è„«æ®¼)
        "å¤šæ—¥å€": "å¤šä¸€å€",   # å¤šä¸€å€
        "å¤šæ—¥è³ ": "å¤šä¸€å€",   # å¤šä¸€å€ (å¦ä¸€è®Šé«”)
        "ä¿‚çµ±": "ç³»çµ±",       # ç³»çµ±
        "å°ˆä½": "å°ˆç‚º",       # å°ˆç‚º
        "åŒ–å·§": "èŠ±å·§",       # èŠ±å·§
        "å†‡å·¨å¤§": "å†’å·¨å¤§",   # å†’å·¨å¤§é¢¨éšª
        "å†‡é¢¨éšª": "å†’é¢¨éšª",   # å†’é¢¨éšª
        "éŠæ„": "çŒ¶è±«",       # çŒ¶è±«
        
        # ===== æ¨™é»žç¬¦è™Ÿæ¸…ç† =====
        "ï¹š": "",         # ç§»é™¤å¤šé¤˜æ‹¬è™Ÿ
        "ï¹™": "",
        "ï¼ˆ": "",
        "ï¼‰": "",
        "(": "",
        ")": "",
    }
    
    # éœ€è¦ LLM æ ¡æ­£å˜…è§¸ç™¼å­—ç¬¦ï¼ˆå¿«é€ŸæŸ¥æ‰¾ï¼‰
    TRIGGER_CHARS = set("æ—¢ç³»è·å°¼é»Žä¿¾è¿™ä¸ªè¯´è¯æ—¶é—´æ¥ä¼šæ²¡å¯¹äºŽä¸ºé—®é¢˜ä¸œå„¿ä»¬è®©ç»™")
    
    def _needs_llm_correction(self, text: str) -> bool:
        """æª¢æŸ¥æ–‡å­—æ˜¯å¦åŒ…å«éœ€è¦ LLM æ ¡æ­£å˜…éŒ¯èª¤ã€‚
        
        [å…¨éƒ¨ LLM æ¨¡å¼] - æ‰€æœ‰å¥å­éƒ½ç¶“éŽ LLM æ ¡æ­£ï¼Œç¢ºä¿æˆèªžåŒä¿—èªžæ­£ç¢ºè™•ç†ã€‚
        """
        # å…¨éƒ¨å¥å­éƒ½éœ€è¦ LLM æ ¡æ­£
        return True
    
    def _apply_simple_corrections(self, text: str) -> str:
        """æ‡‰ç”¨ç°¡å–®å­—ç¬¦æ›¿æ›ï¼ˆç„¡éœ€ LLMï¼‰ã€‚"""
        result = text
        # åˆªé™¤å¤šé¤˜å˜…æ‹¬è™Ÿï¼ˆæ‰€æœ‰é¡žåž‹ï¼‰
        result = result.replace(")", "").replace("(", "")
        result = result.replace("ï¼‰", "").replace("ï¼ˆ", "")
        result = result.replace("ï¹š", "").replace("ï¹™", "")  # ç‰¹æ®Šå…¨è§’æ‹¬è™Ÿ
        result = result.replace("ã€", "").replace("ã€Œ", "")  # å¼•è™Ÿ
        result = result.replace("ã€‘", "").replace("ã€", "")  # æ–¹æ‹¬è™Ÿ
        # æ‡‰ç”¨éŒ¯å­—æ ¡æ­£
        for error, correction in self.CANTONESE_CORRECTIONS.items():
            result = result.replace(error, correction)
        return result
    
    def _refine_with_llm(self, segments: List, progress_callback: Optional[Callable] = None) -> List[SubtitleEntryV2]:
        """
        ä½¿ç”¨ LLM æ™ºèƒ½æ ¡æ­£ Whisper segmentsã€‚
        
        ã€æ™ºèƒ½éŽæ¿¾æ¨¡å¼ã€‘ï¼š
        - æœ‰éŒ¯å­— â†’ èª¿ç”¨ LLM æ ¡æ­£
        - å†‡éŒ¯å­— â†’ ç›´æŽ¥åšç°¡å–®æ›¿æ›ï¼ˆè¶…å¿«ï¼‰
        
        Args:
            segments: Whisper TranscriptionSegment åˆ—è¡¨
            progress_callback: é€²åº¦å›žèª¿ (80-95%)
            
        Returns:
            SubtitleEntryV2 åˆ—è¡¨
        """
        # ç„¡ LLM æ™‚ï¼Œåªåšç°¡å–®æ›¿æ›
        if not self.llm:
            logger.info("No LLM available, applying simple corrections only...")
            return [
                SubtitleEntryV2(
                    start=seg.start,
                    end=seg.end,
                    colloquial=self._apply_simple_corrections(seg.text.strip()),
                    formal=None
                )
                for seg in segments
            ]
        
        # çµ±è¨ˆéœ€è¦ LLM å˜… segment æ•¸é‡
        needs_llm_count = sum(1 for seg in segments if self._needs_llm_correction(seg.text.strip()))
        logger.info(f"Smart LLM: {needs_llm_count}/{len(segments)} segments need LLM correction")
        
        refined_entries = []
        total = len(segments)
        llm_calls = 0
        
        # æå–æ‰€æœ‰æ–‡æœ¬ç”¨æ–¼ä¸Šä¸‹æ–‡
        all_texts = [seg.text.strip() for seg in segments]
        
        for i, seg in enumerate(segments):
            # æ›´æ–°é€²åº¦ (80-95%)
            if progress_callback:
                pct = 80 + int((i / total) * 15)
                progress_callback(pct)
            
            original_text = seg.text.strip()
            
            # æª¢æŸ¥æ˜¯å¦éœ€è¦ LLM æ ¡æ­£
            if self._needs_llm_correction(original_text):
                llm_calls += 1
                try:
                    # æ§‹å»ºå‰æ–‡å¾Œç† context (å‰2å¥ + å¾Œ2å¥)
                    context_before = all_texts[max(0, i-2):i]
                    context_after = all_texts[i+1:min(len(all_texts), i+3)]
                    
                    # çµ„åˆä¸Šä¸‹æ–‡
                    context_text = ""
                    if context_before:
                        context_text += "ã€å‰æ–‡ã€‘" + " | ".join(context_before) + "\n"
                    context_text += "ã€ç•¶å‰å¥å­ã€‘" + original_text + "\n"
                    if context_after:
                        context_text += "ã€å¾Œæ–‡ã€‘" + " | ".join(context_after)
                    
                    # èª¿ç”¨ LLM æ ¡æ­£ï¼ˆå¸¶ä¸Šä¸‹æ–‡ï¼‰
                    result = self.llm.refine_text_with_context(original_text, context_text)
                    refined_sentences = result.get('sentences', [])
                    
                    if refined_sentences:
                        refined_text = refined_sentences[0] if len(refined_sentences) == 1 else ''.join(refined_sentences)
                        formal = refined_text if refined_text != original_text else None
                    else:
                        formal = None
                    
                    # åŒæ™‚æ‡‰ç”¨ç°¡å–®æ›¿æ›
                    corrected = self._apply_simple_corrections(original_text)
                    
                    entry = SubtitleEntryV2(
                        start=seg.start,
                        end=seg.end,
                        colloquial=corrected,
                        formal=formal
                    )
                except Exception as e:
                    logger.warning(f"LLM error for segment {i}: {e}")
                    entry = SubtitleEntryV2(
                        start=seg.start,
                        end=seg.end,
                        colloquial=self._apply_simple_corrections(original_text),
                        formal=None
                    )
            else:
                # å””éœ€è¦ LLMï¼Œç›´æŽ¥ç°¡å–®æ›¿æ›
                entry = SubtitleEntryV2(
                    start=seg.start,
                    end=seg.end,
                    colloquial=self._apply_simple_corrections(original_text),
                    formal=None
                )
            
            refined_entries.append(entry)
        
        saved = len(segments) - llm_calls
        logger.info(f"LLM refinement complete: {llm_calls} LLM calls, saved {saved} calls ({saved/len(segments)*100:.0f}% faster)")
        return refined_entries
    
    def process(
        self,
        input_path: str,
        progress_callback: Optional[Callable] = None,
        status_callback: Optional[Callable] = None
    ) -> List[SubtitleEntryV2]:
        """
        Run the subtitle generation pipeline with sequential model loading.
        
        Pipeline stages (memory efficient - one model at a time):
        1. Load ASR (Whisper)
        2. Extract audio if needed
        3. Transcribe with Whisper
        4. Unload Whisper to free GPU memory
        5. Load LLM (if enabled)
        6. Refine with LLM
        
        Args:
            input_path: Path to audio/video file
            progress_callback: Progress callback (0-100)
            status_callback: Status message callback for UI updates
            
        Returns:
            List of SubtitleEntryV2 with colloquial (and optional formal) text
        """
        logger.info(f"Starting V2 pipeline for: {input_path}")
        logger.info("Using sequential model loading (memory efficient mode)")
        input_file = Path(input_path)
        
        # Step 0: Pre-clear GPU memory (0-5%)
        import gc
        import torch
        
        if progress_callback:
            progress_callback(0)
        
        logger.info("Pre-clearing GPU memory before pipeline start...")
        gc.collect()
        # Cross-platform GPU memory cleanup
        if torch.backends.mps.is_available():
            torch.mps.empty_cache()
            logger.info("MPS memory cache cleared, ready for model loading")
        elif torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
            logger.info("CUDA memory cache cleared, ready for model loading")
        
        if progress_callback:
            progress_callback(5)
        
        # Step 1: Load ASR model only (5-15%)
        self._load_asr(progress_callback, status_callback=status_callback)
        
        # Step 2: Prepare audio (15-20%)
        if progress_callback:
            progress_callback(15)
        
        # Check if video needs audio extraction
        video_extensions = {'.mp4', '.avi', '.mov', '.mkv', '.webm', '.m4v'}
        if input_file.suffix.lower() in video_extensions:
            audio_path = self._extract_audio(input_file)
        else:
            audio_path = str(input_file)
        
        # Step 3: Transcribe with Whisper (20-60%)
        if progress_callback:
            progress_callback(20)
        
        logger.info("Running Whisper transcription...")
        
        # Get custom prompt from config (user-defined vocabulary for better recognition)
        custom_prompt = self.config.get("whisper_custom_prompt", "")
        result = self.asr.transcribe(audio_path, language='yue', custom_prompt=custom_prompt)
        
        whisper_segments = result.get('segments', [])
        logger.info(f"Whisper produced {len(whisper_segments)} segments")
        
        if not whisper_segments:
            logger.warning("No segments from Whisper")
            return []
        
        if progress_callback:
            progress_callback(60)
        
        # Step 4: VAD-based smart segmentation (60-75%)
        logger.info("Running VAD-based smart segmentation...")
        if progress_callback:
            progress_callback(62)
        
        try:
            # Initialize VAD processor (é«˜æ•æ„Ÿåº¦é…ç½®)
            if self.vad is None:
                self.vad = VADProcessor(
                    self.config,
                    threshold=0.1,              # é™ä½Žé–¾å€¼ (0.5->0.35)ï¼Œæ›´æ•æ„Ÿæª¢æ¸¬èªžéŸ³
                    min_silence_duration_ms=20, # é™ä½ŽéœéŸ³é–€æª» (200->150)ï¼Œæ›´ç²¾ç´°æ–·å¥
                    min_speech_duration_ms=50,  # é™ä½ŽèªžéŸ³é–€æª» (250->200)ï¼Œä¿ç•™çŸ­å¥
                    speech_pad_ms=300            # æ¸›å°‘å¡«å…… (400->300)ï¼Œæ›´è²¼è¿‘å¯¦éš›é‚Šç•Œ
                )
            
            # Detect voice segments
            voice_segments = self.vad.detect_voice_segments(audio_path)
            logger.info(f"VAD detected {len(voice_segments)} voice segments")
            
            if progress_callback:
                progress_callback(68)
            
            # Merge Whisper + VAD for smart segmentation
            optimized_segments = self.vad.merge_with_transcription(
                whisper_segments,
                voice_segments,
                max_gap=0.2,  # ç¸®çŸ­åˆä½µé–“éš” (0.8->0.5s)ï¼Œæ›´å¤šç¨ç«‹å¥å­
                max_chars=22  # ç¸®çŸ­æ¯æ®µæœ€å¤§å­—æ•¸ (25->22)ï¼Œæ›´é©åˆå­—å¹•
            )
            logger.info(f"VAD optimization: {len(whisper_segments)} -> {len(optimized_segments)} segments")
            
            # Use optimized segments
            whisper_segments = optimized_segments
            
        except Exception as e:
            logger.warning(f"VAD segmentation failed, using original Whisper segments: {e}")
            # Continue with original Whisper segments
        
        if progress_callback:
            progress_callback(75)
        
        # Step 5: Unload Whisper to free GPU memory (75-80%)
        if self.enable_llm and self.profile.llm_a_enabled:
            self._unload_asr()
            if progress_callback:
                progress_callback(80)
        
        # Step 6: LLM refinement (80-95%)
        if self.enable_llm:
            # Load LLM now that Whisper is unloaded
            self._load_llm(progress_callback)
            
            logger.info("Starting LLM refinement...")
            final_subtitles = self._refine_with_llm(whisper_segments, progress_callback)
        else:
            # No LLM, but still apply simple corrections
            final_subtitles = [
                SubtitleEntryV2(
                    start=seg.start,
                    end=seg.end,
                    colloquial=self._apply_simple_corrections(seg.text.strip()),
                    formal=None
                )
                for seg in whisper_segments
            ]
        
        if progress_callback:
            progress_callback(100)
        
        logger.info(f"Pipeline complete. Generated {len(final_subtitles)} subtitles")
        return final_subtitles
    
    def cleanup(self):
        """Release all resources."""
        import gc
        import torch
        
        logger.info("Cleaning up pipeline resources...")
        
        if self.asr:
            try:
                self.asr.unload_model()
            except Exception as e:
                logger.warning(f"ASR cleanup error: {e}")
            self.asr = None
        
        if self.llm:
            try:
                self.llm.unload_models()
            except Exception as e:
                logger.warning(f"LLM cleanup error: {e}")
            self.llm = None
        
        if self.vad:
            try:
                self.vad.unload_model()
            except Exception as e:
                logger.warning(f"VAD cleanup error: {e}")
            self.vad = None
        
        self._models_loaded = False
        
        # Force garbage collection
        gc.collect()
        
        # Cross-platform GPU memory cleanup
        try:
            if torch.backends.mps.is_available():
                torch.mps.empty_cache()
                logger.info("MPS memory cache cleared")
            elif torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
                logger.info("CUDA memory cache cleared")
        except Exception as e:
            logger.warning(f"GPU cache clear error: {e}")
        
        logger.info("Pipeline cleanup complete")
    
    def get_profile(self) -> Optional[PerformanceProfile]:
        """Get the current hardware profile."""
        return self.profile
