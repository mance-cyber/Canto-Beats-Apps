"""
Subtitle Pipeline V2 - AI-powered transcription with context-aware conversion.

Uses Whisper for ASR + Qwen2 LLM for intelligent colloquial-to-written conversion.
"""

import tempfile
from pathlib import Path
from typing import List, Optional, Callable
from dataclasses import dataclass

from core.config import Config
from core.hardware_detector import HardwareDetector, PerformanceProfile
from models.whisper_asr import WhisperASR
# QwenLLM removed - æ›¸é¢èª conversion handled by StyleControlPanel
from models.vad_processor import VADProcessor
from utils.logger import setup_logger

# Try to import MLX Whisper for Apple Silicon acceleration
try:
    from utils.whisper_mlx import MLXWhisperASR, get_best_whisper_backend
    HAS_MLX_WHISPER = MLXWhisperASR.is_available()
except ImportError:
    HAS_MLX_WHISPER = False

# é«˜ç´šè½‰éŒ„æ¨¡çµ„ï¼ˆå¯é¸ï¼‰
try:
    from utils.audio_enhancer import AudioEnhancer
    from utils.advanced_transcription import AdvancedTranscriber
    from utils.vocabulary_learner import get_vocabulary_learner, auto_correct_text
    HAS_ADVANCED_FEATURES = True
except ImportError:
    HAS_ADVANCED_FEATURES = False

logger = setup_logger()


@dataclass
class SubtitleEntryV2:
    """A single subtitle entry with dual-language support."""
    start: float
    end: float
    colloquial: str  # å£èª - spoken Cantonese
    formal: Optional[str] = None  # æ›¸é¢èª - written Chinese


class SubtitlePipelineV2:
    """
    V2 pipeline with AI-powered colloquial-to-written conversion.
    
    Pipeline stages:
    1. Hardware detection
    2. Model loading (Whisper ASR + optional Qwen LLM)
    3. Audio extraction (if video)
    4. Whisper transcription (native segmentation)
    5. Optional LLM refinement (context-aware conversion)
    """
    
    def __init__(self, config: Config, force_cpu: bool = False, enable_llm: bool = False):
        """
        Initialize pipeline.
        
        Args:
            config: Application configuration
            force_cpu: Force CPU mode
            enable_llm: Deprecated, kept for API compatibility (always False)
        """
        self.config = config
        self.force_cpu = force_cpu
        self.enable_llm = False  # LLM is handled by StyleControlPanel, not here
        self.profile = None
        self.asr = None
        self.vad = None  # VAD processor for smart segmentation
        self._models_loaded = False
        
        # Create temp directory
        self.temp_dir = Path(tempfile.gettempdir()) / "canto_beats_v2"
        self.temp_dir.mkdir(exist_ok=True)
        
        # Initialize hardware detection
        self._setup_hardware()
    
    def _setup_hardware(self):
        """Detect hardware and determine optimal configuration."""
        logger.info("Detecting hardware configuration...")
        detector = HardwareDetector()
        self.profile = detector.detect(force_cpu=self.force_cpu)
        
        logger.info(f"Hardware tier: {self.profile.tier.value}")
        logger.info(f"Device: {self.profile.device}")
        logger.info(f"VRAM: {self.profile.vram_gb} GB")
        logger.info(f"LLM refinement: {'enabled' if self.enable_llm else 'disabled'}")
    
    def _load_asr(self, progress_callback: Optional[Callable] = None, status_callback: Optional[Callable] = None):
        """Load ASR model with Apple Silicon priority: CoreML > MPS > CPU.
        
        Args:
            progress_callback: Callback for progress percentage (0-100)
            status_callback: Callback for status message updates (e.g., "æ­£åœ¨ä¸‹è¼‰æ¨¡å‹...")
        """
        if self.asr is not None and self.asr.is_loaded:
            return

        if progress_callback:
            progress_callback(10)

        # Priority: MLX Whisper (CoreML/MPS) > faster-whisper (CPU)
        if HAS_MLX_WHISPER:
            logger.info(f"ğŸ Loading MLX Whisper (Apple Silicon optimized): {self.profile.asr_model}")
            try:
                if status_callback:
                    status_callback("æ­£åœ¨æº–å‚™ AI å·¥å…·...")
                
                self.asr = MLXWhisperASR(model_size=self.profile.asr_model)
                
                # Pass status callback to load_model for download progress
                self.asr.load_model(progress_callback=status_callback)
                
                logger.info(f"âš¡ MLX Whisper loaded on {self.asr.get_backend_type().upper()}")
                
                if status_callback:
                    status_callback("AI å·¥å…·åŠ è¼‰å®Œæˆï¼")
                return
            except Exception as e:
                logger.warning(f"MLX Whisper failed, falling back to faster-whisper: {e}")
                if status_callback:
                    status_callback("æ­£åœ¨åˆ‡æ› AI å·¥å…·...")

        # Fallback: faster-whisper (CPU)
        logger.info(f"Loading faster-whisper ASR model: {self.profile.asr_model}")
        if status_callback:
            status_callback("æ­£åœ¨åŠ è¼‰ AI å·¥å…·...")
        self.asr = WhisperASR(self.config, model_size=self.profile.asr_model)
        self.asr.load_model()
        logger.info("ASR model loaded successfully (CPU mode)")
    
    def _unload_asr(self):
        """Unload ASR model to free GPU memory for LLM."""
        import gc
        import torch
        
        if self.asr:
            logger.info("Unloading ASR model to free GPU memory...")
            try:
                self.asr.unload_model()
            except Exception as e:
                logger.warning(f"ASR unload error: {e}")
            self.asr = None
            
            # Force garbage collection and clear GPU cache
            gc.collect()
            # Cross-platform GPU memory cleanup
            if torch.backends.mps.is_available():
                torch.mps.empty_cache()
            elif torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
            logger.info("ASR unloaded, GPU memory freed")
    
    # _load_llm removed - æ›¸é¢èª conversion is handled by StyleControlPanel

    
    def _extract_audio(self, video_path: Path) -> str:
        """Extract audio from video file."""
        import numpy as np
        
        audio_path = self.temp_dir / f"{video_path.stem}_audio.wav"
        
        logger.info(f"Extracting audio from video: {video_path}")
        
        try:
            # Use PyAV for audio extraction (includes FFmpeg libraries internally)
            import av
            import soundfile as sf
            
            container = av.open(str(video_path))
            
            # Get audio stream
            if not container.streams.audio:
                raise RuntimeError("å½±ç‰‡ä¸­æ²’æœ‰æ‰¾åˆ°éŸ³é »è»Œé“")
            
            audio_stream = container.streams.audio[0]
            logger.info(f"Audio stream: {audio_stream.rate}Hz, {audio_stream.channels} channels")
            
            # Extract and decode audio frames
            audio_frames = []
            for frame in container.decode(audio_stream):
                # Convert frame to numpy array
                audio_frames.append(frame.to_ndarray())
            
            container.close()
            
            if not audio_frames:
                raise RuntimeError("ç„¡æ³•å¾å½±ç‰‡æå–éŸ³é »å¹€")
            
            # Concatenate all frames
            audio_data = np.concatenate(audio_frames, axis=1)
            
            # Resample to 16kHz if needed
            target_sr = 16000
            if audio_stream.rate != target_sr:
                try:
                    import scipy.signal
                    num_samples = int(len(audio_data[0]) * target_sr / audio_stream.rate)
                    audio_data = scipy.signal.resample(audio_data, num_samples, axis=1)
                    sample_rate = target_sr
                    logger.info(f"Resampled from {audio_stream.rate}Hz to {target_sr}Hz")
                except ImportError:
                    # Fallback: use original sample rate if scipy not available
                    sample_rate = audio_stream.rate
                    logger.warning("scipy not available, using original sample rate")
            else:
                sample_rate = audio_stream.rate
            
            # Convert to mono if stereo
            if audio_data.shape[0] > 1:
                audio_data = np.mean(audio_data, axis=0)
            else:
                audio_data = audio_data[0]
            
            # Save as WAV
            sf.write(str(audio_path), audio_data, sample_rate, subtype='PCM_16')
            
            logger.info(f"Audio extracted to: {audio_path}")
            return str(audio_path)
            
        except ImportError as e:
            logger.error(f"Required library not available: {e}")
            raise RuntimeError(f"éŸ³é »è™•ç†åº«æœªæ‰¾åˆ°: {e}")
        except Exception as e:
            logger.error(f"Audio extraction failed: {e}")
            raise RuntimeError(f"éŸ³é »æå–å¤±æ•—: {e}")
    
    # ==================== ç²µèªéŒ¯å­—æ¸…å–® (æ“´å±•ç‰ˆ) ====================
    # Whisper å¸¸è¦‹ç²µèªè½‰éŒ„éŒ¯èª¤ â†’ æ­£ç¢ºå¯«æ³•
    CANTONESE_CORRECTIONS = {
        # ===== å¸¸è¦‹å­—ç¬¦éŒ¯èª¤ =====
        "æ—¢": "å˜…",      # çš„ (possessive)
        "ç³»": "ä¿‚",      # æ˜¯ (to be)
        "è·": "ä½¢",      # ä»–/å¥¹ (he/she)
        "å°¼": "å‘¢",      # é€™ (this)
        "é»": "åšŸ",      # ä¾† (come)
        "ç•€": "ä¿¾",      # çµ¦ (give)
        "é‡": "å˜¢",      # æ±è¥¿ (thing)
        "åšŸ": "ä¾†",      # æœ‰æ™‚ Whisper æœƒåå‘éŒ¯
        "å¾": "å””",      # ä¸
        "è¾¹": "é‚Š",      # å“ªè£¡
        "å™‰": "å’",      # é€™æ¨£
        "å—°": "å€‹",      # é‚£å€‹ (sometimes reversed)
        "D": "å•²",       # äº›
        "d": "å•²",
        "ä»²": "é‡",      # still (æœ‰æ™‚æœƒåå‘)
        "å˜›": "å–å˜›",    # èªæ°£è©
        
        # ===== åŒéŸ³å­—/è«§éŸ³éŒ¯èª¤ =====
        "ç·Šä¿‚": "æ¢—ä¿‚",   # ç•¶ç„¶ (of course)
        "æ¢—ç³»": "æ¢—ä¿‚",
        "ç‚¹è§£": "é»è§£",   # ç‚ºä»€éº¼
        "ç‚¹æ ·": "é»æ¨£",   # æ€æ¨£
        "ç‚¹ç®—": "é»ç®—",   # æ€éº¼è¾¦
        "å’©äº‹": "ä¹œäº‹",   # ä»€éº¼äº‹
        "ä¹œé‡": "ä¹œå˜¢",   # ä»€éº¼æ±è¥¿
        "ç³»å’ª": "ä¿‚å’ª",   # æ˜¯ä¸æ˜¯
        "ç³»å””ç³»": "ä¿‚å””ä¿‚",
        "å””ç³»": "å””ä¿‚",   # ä¸æ˜¯
        "æœ‰ç„¡": "æœ‰å†‡",   # æœ‰æ²’æœ‰
        "æœ‰æ²¡": "æœ‰å†‡",
        "å†‡ç„¡": "å†‡",
        "æœ‰D": "æœ‰å•²",
        
        # ===== æˆèªåŒéŸ³å­—æ ¡æ­£ (é‡è¦!) =====
        "å…‹è‹¦ä¾†è·¯": "åˆ»è‹¦è€å‹",
        "åˆ»è‹¦ä¾†å‹": "åˆ»è‹¦è€å‹",
        "å…‹è‹¦è€å‹": "åˆ»è‹¦è€å‹",
        "ä¸€è¦–åŒäºº": "ä¸€è¦–åŒä»",
        "å°ˆå¿ƒè‡³å¿—": "å°ˆå¿ƒè‡´å¿—",
        "äº‹å€å…¬åŠ": "äº‹å€åŠŸåŠ",
        "äº‹åŠå…¬å€": "äº‹åŠåŠŸå€",
        "å¿ƒæ› ç¥å®œ": "å¿ƒæ› ç¥æ€¡",
        "ç„¡å¾®ä¸è‡³": "ç„¡å¾®ä¸è‡³",
        "è¿«ä¸å¾—å·²": "è¿«ä¸å¾—å·²",
        "å …æŒä¸æ‡ˆ": "å …æŒä¸æ‡ˆ",
        "åŠ›ä¸å¾å¿ƒ": "åŠ›ä¸å¾å¿ƒ",
        "è«åå¥‡å¦™": "è«åå…¶å¦™",
        "ä¸€é³´é©šäºº": "ä¸€é³´é©šäºº",
        "è‡ªä½œè‡ªå—": "è‡ªä½œè‡ªå—",
        "ä¸çŸ¥æ‰€è¬‚": "ä¸çŸ¥æ‰€è¬‚",
        "è¦ªåŠ›è¦ªç¶­": "è¦ªåŠ›è¦ªç‚º",
        "è¦ªåŠ›è¦ªç‚º": "è¦ªåŠ›è¦ªç‚º",
        
        # ===== ç²µèªä¿—èª =====
        "ç”©åº•": "ç”©åº•",   # çˆ½ç´„ (ä¿æŒåŸæ¨£)
        "æ”¶çš®": "æ”¶çš®",   # é–‰å˜´ (ä¿æŒåŸæ¨£)
        "å±ˆæ©Ÿ": "å±ˆæ©Ÿ",   # ä½œå¼Š (ä¿æŒåŸæ¨£)
        "å¥½Kam": "å¥½kam", # å°·å°¬
        "å¥½kam": "å¥½å°·å°¬",
        "chur": "chur",   # è¾›è‹¦ (ä¿ç•™è‹±æ–‡)
        "Chur": "chur",
        
        # ===== ç°¡é«”å­— â†’ ç¹é«”å­— =====
        "è¿™": "å‘¢",
        "ä¸ª": "å€‹",
        "è¯´": "è¬›",
        "è¯": "è©±",
        "æ—¶": "æ™‚",
        "é—´": "é–“",
        "æ¥": "åšŸ",
        "ä¼š": "æœƒ",
        "æ²¡": "å†‡",
        "å¯¹": "å°",
        "äº": "æ–¼",
        "ä¸º": "ç‚º",
        "é—®": "å•",
        "é¢˜": "é¡Œ",
        "ä¸œ": "æ±",
        "å„¿": "å…’",
        "ä»¬": "å“‹",
        "è®©": "ä¿¾",
        "ç»™": "ç•€",
        "è¿™ä¸ª": "å‘¢å€‹",
        "é‚£ä¸ª": "å—°å€‹",
        "ä»€ä¹ˆ": "ä¹œå˜¢",
        "æ€ä¹ˆ": "é»æ¨£",
        "ä¸ºä»€ä¹ˆ": "é»è§£",
        "ç°åœ¨": "è€Œå®¶",
        "çŸ¥é“": "çŸ¥é“",
        "å¯ä»¥": "å¯ä»¥",
        "éœ€è¦": "éœ€è¦",
        "åº”è¯¥": "æ‡‰è©²",
        "è¿˜æ˜¯": "å®šä¿‚",
        "æˆ–è€…": "æˆ–è€…",
        "å› ä¸º": "å› ç‚º",
        "æ‰€ä»¥": "æ‰€ä»¥",
        "ä½†æ˜¯": "ä½†ä¿‚",
        "è™½ç„¶": "é›–ç„¶",
        "å¦‚æœ": "å¦‚æœ",
        "å½“ç„¶": "æ¢—ä¿‚",
        
        # ===== æ•¸å­—ç›¸é—œ =====
        "ä¸¤": "å…©",
        "ä¸‡": "è¬",
        "äº¿": "å„„",
        
        # ===== è‹±æ–‡ç›¸é—œ =====
        "o k": "OK",
        "o K": "OK",
        "O K": "OK",
        "O k": "OK",
        "bye bye": "bye bye",
        "thank you": "thank you",
        
        # ===== ç²µèªåŒéŸ³å­—æ ¡æ­£ (Whisper å¸¸è¦‹éŒ¯èª¤) =====
        "åŸ·ç²’": "åŸ·ç¬ ",       # åŸ·ç¬  = çµæ¥­
        "åŸ·å·¦ç²’": "åŸ·å·¦ç¬ ",
        "åŸ·å’—ç²’": "åŸ·å’—ç¬ ",
        "æŠ½è–ª": "æŠ½èº«",       # æŠ½èº« (é‡‘èŸ¬è„«æ®¼)
        "å¤šæ—¥å€": "å¤šä¸€å€",   # å¤šä¸€å€
        "å¤šæ—¥è³ ": "å¤šä¸€å€",   # å¤šä¸€å€ (å¦ä¸€è®Šé«”)
        "ä¿‚çµ±": "ç³»çµ±",       # ç³»çµ±
        "å°ˆä½": "å°ˆç‚º",       # å°ˆç‚º
        "åŒ–å·§": "èŠ±å·§",       # èŠ±å·§
        "å†‡å·¨å¤§": "å†’å·¨å¤§",   # å†’å·¨å¤§é¢¨éšª
        "å†‡é¢¨éšª": "å†’é¢¨éšª",   # å†’é¢¨éšª
        "éŠæ„": "çŒ¶è±«",       # çŒ¶è±«
        
        # New User Reported (2025-12-20)
        "æœ‰å†¤ç„¡è€è¨´": "æœ‰å†¤ç„¡è·¯è¨´",
        "å‡Œå°„": "é›¶èˆ",
        "å¿ƒå¸Œæ„Ÿå—": "èº«åŒæ„Ÿå—",
        "å¿ƒç—›æ„Ÿå—": "èº«åŒæ„Ÿå—",
        "ç‰©å¤–": "é»˜å“€",
        "Porn": "Point",
        "porn": "point",
        "Grip": "Group",
        "grip": "group",
        
        # ===== User Reported Errors (2025-12-28) =====
        "èœæ•¸": "å½©æ•¸",       # å½©æ•¸
        "æ›¬å…‰": "æ¡å…‰",       # æ¡å…‰
        "ç¡å””å¾—": "æ¨ä¸å¾—",   # æ¨ä¸å¾—
        "å–‡ä¸å–‡": "ä½ä¸€ä½",   # ä½ä¸€ä½ (çœ‹ä¸€çœ‹)
        "æ€•æ°£": "å®¢æ°£",       # å®¢æ°£
        "ç´«ä¸Š": "å¸‚å ´",       # å¸‚å ´
        "æ½›ç§»é£Ÿ": "æ½›æ„è­˜",   # æ½›æ„è­˜
        "ç¶“éè€åœ‹": "ç¶“éè·¯é", # ç¶“éè·¯é
        "æ€ªå˜¢": "è²´å˜¢",       # è²´å˜¢ (è²´æ±è¥¿)
        
        # ===== è‹±æ–‡å­—ä¸­ d è¢«è­˜åˆ¥ç‚º å•² çš„ä¿®æ­£ =====
        "IPAå•²": "iPad",
        "enå•²ing": "ending",
        "proå•²uct": "product",
        "proå•²ucts": "products",
        "gooå•²": "good",
        "neeå•²": "need",
        "reaå•²y": "ready",
        "boå•²y": "body",
        "toå•²ay": "today",
        "alreaå•²y": "already",
        "frienå•²": "friend",
        "frienå•²s": "friends",
        "holå•²": "hold",
        "builå•²": "build",
        "fielå•²": "field",
        "worå•²": "word",
        "worlå•²": "world",
        "branå•²": "brand",
        "stanå•²": "stand",
        "hanå•²": "hand",
        "hanå•²s": "hands",
        "minå•²": "mind",
        "finå•²": "find",
        "kinå•²": "kind",
        "sounå•²": "sound",
        "attenå•²": "attend",
        "demanå•²": "demand",
        "expanå•²": "expand",
        "depenå•²": "depend",
        "responå•²": "respond",
        "recommenå•²": "recommend",
        "understanå•²": "understand",
        "backgrounå•²": "background",
        "grounå•²": "ground",
        
        # ===== æ¨™é»ç¬¦è™Ÿæ¸…ç† =====
        "ï¹š": "",         # ç§»é™¤å¤šé¤˜æ‹¬è™Ÿ
        "ï¹™": "",
        "ï¼ˆ": "",
        "ï¼‰": "",
        "(": "",
        ")": "",
    }
    
    # éœ€è¦ LLM æ ¡æ­£å˜…è§¸ç™¼å­—ç¬¦ï¼ˆå¿«é€ŸæŸ¥æ‰¾ï¼‰
    TRIGGER_CHARS = set("æ—¢ç³»è·å°¼é»ä¿¾è¿™ä¸ªè¯´è¯æ—¶é—´æ¥ä¼šæ²¡å¯¹äºä¸ºé—®é¢˜ä¸œå„¿ä»¬è®©ç»™")

    # ç²µèªå¥å°¾èªæ°£è©ï¼ˆç”¨æ–¼å¾Œè™•ç†ï¼‰
    # ç•¶å‘¢å•²å­—å‡ºç¾å–ºå¥é¦–ï¼Œé€šå¸¸ä¿‚ VAD æ–·å¥éŒ¯èª¤ï¼Œæ‡‰è©²ç§»åˆ°ä¸Šä¸€å¥å¥å°¾
    SENTENCE_FINAL_PARTICLES = {
        'å—', 'å‘€', 'å•¦', 'å–', 'å›‰', 'å’©', 'å˜…', 'å•Š', 'å‘¢', 'å–‡',
        'ã—', 'å’‹', 'å•©', 'å˜›', 'å’¯', 'å™ƒ', 'å’§', 'å–', 'åšŸ', 'ã–­',
        'å”„', 'å˜', 'å–', 'å“‡', 'å–”', 'å“¦', 'è€¶', 'å˜¢'
    }

    # _needs_llm_correction removed - LLM is handled by StyleControlPanel
    
    def _apply_simple_corrections(self, text: str) -> str:
        """æ‡‰ç”¨ç°¡å–®å­—ç¬¦æ›¿æ›ï¼ˆç„¡éœ€ LLMï¼‰ã€‚"""
        result = text
        # åˆªé™¤å¤šé¤˜å˜…æ‹¬è™Ÿï¼ˆæ‰€æœ‰é¡å‹ï¼‰
        result = result.replace(")", "").replace("(", "")
        result = result.replace("ï¼‰", "").replace("ï¼ˆ", "")
        result = result.replace("ï¹š", "").replace("ï¹™", "")  # ç‰¹æ®Šå…¨è§’æ‹¬è™Ÿ
        result = result.replace("ã€", "").replace("ã€Œ", "")  # å¼•è™Ÿ
        result = result.replace("ã€‘", "").replace("ã€", "")  # æ–¹æ‹¬è™Ÿ
        # æ‡‰ç”¨éŒ¯å­—æ ¡æ­£
        for error, correction in self.CANTONESE_CORRECTIONS.items():
            result = result.replace(error, correction)
        return result

    def _fix_particle_punctuation(self, text: str) -> str:
        """
        Fix punctuation appearing before sentence-final particles.
        
        Transforms: "å˜…è©±,å‘¢" â†’ "å˜…è©±å‘¢,"
                    "å’æ¨£ï¼Œå•¦" â†’ "å’æ¨£å•¦ï¼Œ"
        
        Does NOT transform: "å˜…è©±,å‘¢å€‹" â†’ keeps as-is (å‘¢å€‹ is a word)
        
        Args:
            text: Input text
        
        Returns:
            Corrected text
        """
        result = text
        
        # Define punctuation that can appear after particles
        movable_punctuation = ['ï¼Œ', ',', 'ã€‚', '.', 'ï¼', '!', 'ï¼Ÿ', '?']
        
        # Common word patterns that should NOT be treated as particles
        # å‘¢å€‹, å‘¢å•², å—°å€‹, å—°å•², etc.
        demonstrative_words = {
            'å‘¢å€‹', 'å‘¢å•²', 'å‘¢åº¦', 'å‘¢é‚Š', 'å‘¢æ¬¡', 'å‘¢æ¨£', 'å‘¢é™£',
            'å—°å€‹', 'å—°å•²', 'å—°åº¦', 'å—°é‚Š', 'å—°æ¬¡', 'å—°æ¨£', 'å—°é™£',
        }
        
        # For each punctuation
        for punct in movable_punctuation:
            # For each particle
            for particle in self.SENTENCE_FINAL_PARTICLES:
                # Pattern: [punct][particle]
                wrong_pattern = f"{punct}{particle}"
                
                # Find all occurrences
                index = 0
                while True:
                    index = result.find(wrong_pattern, index)
                    if index == -1:
                        break
                    
                    # Check what comes after the particle
                    char_after_index = index + len(wrong_pattern)
                    
                    # If at end of string, it's a true particle
                    if char_after_index >= len(result):
                        result = result[:index] + f"{particle}{punct}" + result[char_after_index:]
                        index += len(f"{particle}{punct}")
                        continue
                    
                    char_after = result[char_after_index]
                    
                    # Check if this forms a demonstrative word
                    is_word = False
                    for word in demonstrative_words:
                        if result[index + len(punct):].startswith(word):
                            is_word = True
                            break
                    
                    if is_word:
                        # Skip this occurrence (it's part of a word like å‘¢å€‹)
                        index += len(wrong_pattern)
                        continue
                    
                    # If followed by punctuation, whitespace, or end of string, it's a true particle
                    if char_after in movable_punctuation or char_after.isspace():
                        # Move punctuation after particle
                        result = result[:index] + f"{particle}{punct}" + result[char_after_index:]
                        index += len(f"{particle}{punct}")
                    else:
                        # Followed by another character, might be part of a word
                        # Be conservative and skip
                        index += len(wrong_pattern)
        
        return result

    def _fix_sentence_final_particles(
        self,
        subtitles: List[SubtitleEntryV2]
    ) -> List[SubtitleEntryV2]:
        """
        ä¿®æ­£å¥é¦–èªæ°£è©å•é¡Œã€‚

        VAD æœ‰æ™‚æœƒå°‡å¥å°¾èªæ°£è©ï¼ˆå¦‚ã€Œå—ã€ã€Œå‘€ã€ã€Œå•¦ã€ï¼‰åˆ‡åˆ†åˆ°ä¸‹ä¸€å¥é–‹é ­ï¼Œ
        ä¾‹å¦‚ã€Œå—èŒ¶è‘‰è›‹çš„å‘³é“å‘€ã€æ‡‰è©²ä¿‚ã€ŒèŒ¶è‘‰è›‹çš„å‘³é“å‘€ã€ï¼Œè€Œã€Œå—ã€æ‡‰è©²å–ºä¸Šä¸€å¥ã€‚

        ä¿®æ­£é‚è¼¯ï¼š
        1. æª¢æ¸¬å¥é¦–æ˜¯å¦æœ‰èªæ°£è©
        2. å¦‚æœæœ‰ï¼Œå°‡èªæ°£è©ç§»åˆ°ä¸Šä¸€å¥å¥å°¾
        3. èª¿æ•´æ™‚é–“æˆ³ä»¥åæ˜ è®ŠåŒ–

        Args:
            subtitles: å­—å¹•åˆ—è¡¨

        Returns:
            ä¿®æ­£å¾Œçš„å­—å¹•åˆ—è¡¨
        """
        if len(subtitles) < 2:
            return subtitles

        fixed_subtitles = []
        fixed_count = 0

        for i, sub in enumerate(subtitles):
            text = sub.colloquial.strip()

            # æª¢æŸ¥æ˜¯å¦ä»¥èªæ°£è©é–‹é ­
            if text and text[0] in self.SENTENCE_FINAL_PARTICLES:
                # æ‰¾å‡ºé€£çºŒçš„èªæ°£è©ï¼ˆå¯èƒ½æœ‰å¤šå€‹ï¼Œå¦‚ã€Œå‘€å•¦ã€ï¼‰
                particle_end = 0
                for j, char in enumerate(text):
                    if char in self.SENTENCE_FINAL_PARTICLES:
                        particle_end = j + 1
                    else:
                        break

                particles = text[:particle_end]
                remaining_text = text[particle_end:].strip()

                # åªæœ‰ç•¶æœ‰ä¸Šä¸€å¥ä¸”å‰©é¤˜æ–‡å­—ä¸ç‚ºç©ºæ™‚æ‰ä¿®æ­£
                if fixed_subtitles and remaining_text:
                    # å°‡èªæ°£è©ç§»åˆ°ä¸Šä¸€å¥å¥å°¾
                    prev_sub = fixed_subtitles[-1]
                    prev_text = prev_sub.colloquial.strip()

                    # æ›´æ–°ä¸Šä¸€å¥ï¼ˆåŠ å…¥èªæ°£è©ï¼‰
                    fixed_subtitles[-1] = SubtitleEntryV2(
                        start=prev_sub.start,
                        end=prev_sub.end,  # ä¿æŒåŸæ™‚é–“
                        colloquial=prev_text + particles,
                        formal=prev_sub.formal
                    )

                    # æ›´æ–°ç•¶å‰å¥ï¼ˆç§»é™¤èªæ°£è©ï¼‰
                    fixed_subtitles.append(SubtitleEntryV2(
                        start=sub.start,
                        end=sub.end,
                        colloquial=remaining_text,
                        formal=sub.formal
                    ))

                    fixed_count += 1
                    logger.info(f"[èªæ°£è©ä¿®æ­£] ç§»å‹• '{particles}' å¾å¥ {i+1} åˆ°å¥ {i}: "
                               f"'{prev_text}' â†’ '{prev_text}{particles}'")
                elif not remaining_text:
                    # å¦‚æœç§»é™¤èªæ°£è©å¾Œæ²’æœ‰å‰©é¤˜æ–‡å­—ï¼Œæ•´å¥éƒ½æ˜¯èªæ°£è©
                    if fixed_subtitles:
                        # ç›´æ¥åŠ åˆ°ä¸Šä¸€å¥
                        prev_sub = fixed_subtitles[-1]
                        prev_text = prev_sub.colloquial.strip()
                        fixed_subtitles[-1] = SubtitleEntryV2(
                            start=prev_sub.start,
                            end=sub.end,  # å»¶é•·æ™‚é–“åˆ°ç•¶å‰å¥çµæŸ
                            colloquial=prev_text + particles,
                            formal=prev_sub.formal
                        )
                        fixed_count += 1
                        logger.info(f"[èªæ°£è©ä¿®æ­£] åˆä½µç´”èªæ°£è©å¥ '{particles}' åˆ°ä¸Šä¸€å¥")
                    else:
                        # ç¬¬ä¸€å¥å°±æ˜¯ç´”èªæ°£è©ï¼Œä¿æŒåŸæ¨£
                        fixed_subtitles.append(sub)
                else:
                    # æ²’æœ‰ä¸Šä¸€å¥ï¼Œä¿æŒåŸæ¨£
                    fixed_subtitles.append(sub)
            else:
                # ä¸æ˜¯ä»¥èªæ°£è©é–‹é ­ï¼Œä¿æŒåŸæ¨£
                fixed_subtitles.append(sub)

        if fixed_count > 0:
            logger.info(f"ğŸ“Š èªæ°£è©å¾Œè™•ç†ï¼šä¿®æ­£äº† {fixed_count} è™•å¥é¦–èªæ°£è©")

        # NEW: Also fix punctuation appearing before particles within segments
        punctuation_fixed_count = 0
        final_subtitles = []
        for sub in fixed_subtitles:
            text = sub.colloquial
            corrected_text = self._fix_particle_punctuation(text)
            
            if corrected_text != text:
                final_subtitles.append(SubtitleEntryV2(
                    start=sub.start,
                    end=sub.end,
                    colloquial=corrected_text,
                    formal=sub.formal
                ))
                punctuation_fixed_count += 1
                logger.info(f"[æ¨™é»ä¿®æ­£] '{text}' â†’ '{corrected_text}'")
            else:
                final_subtitles.append(sub)
        
        if punctuation_fixed_count > 0:
            logger.info(f"ğŸ“Š æ¨™é»ä¿®æ­£ï¼šä¿®æ­£äº† {punctuation_fixed_count} è™•æ¨™é»ä½ç½®")

        return final_subtitles

    def _remove_ending_hallucinations(self, subtitles: List[SubtitleEntryV2]) -> List[SubtitleEntryV2]:
        """
        Remove Whisper hallucinations at the end of subtitles.

        Whisper often hallucinates repetitive short words (like "ä¸æ˜¯ ä¸æ˜¯ ä¸æ˜¯")
        when only background music exists at the end of the video.

        Detection criteria:
        1. Short repetitive words within a single segment (e.g., "ä¸æ˜¯ ä¸æ˜¯ ä¸æ˜¯")
        2. Multiple consecutive segments with identical or similar text
        3. Occurring in the last 20% of the video duration

        Args:
            subtitles: List of subtitle entries

        Returns:
            Filtered subtitle list with hallucinations removed
        """
        if not subtitles:
            return subtitles

        # è¨ˆç®—å½±ç‰‡ç¸½æ™‚é•·å’Œæª¢æ¸¬ç¯„åœ
        total_duration = subtitles[-1].end if subtitles else 0
        # ä¿®å¾©ã€Œå˜©ã€å­—å¹»è¦ºå•é¡Œï¼šæ“´å¤§æª¢æ¸¬ç¯„åœ 95% â†’ 70%ï¼Œæ•æ‰ä¸­å¾Œæ®µçš„å¹»è¦ºé‡è¤‡
        hallucination_threshold = total_duration * 0.70  # æœ€å¾Œ 30% æ™‚é–“ç¯„åœï¼ˆå¹»è¦ºå¯èƒ½å¾ 70% é–‹å§‹ï¼‰

        # å¸¸è¦‹çš„å¹»è½è©ï¼ˆWhisper åœ¨èƒŒæ™¯éŸ³æ¨‚ä¸­ç¶“å¸¸èª¤è­˜åˆ¥çš„è©ï¼‰
        # ä¿®å¾©ã€Œå˜©ã€å­—å¹»è¦ºï¼šæ–°å¢ã€Œå˜©ã€ã€Œå˜»ã€ç­‰æ„Ÿå˜†è©åˆ°å¹»è¦ºåˆ—è¡¨
        common_hallucinations = {
            'ä¸æ˜¯', 'æ˜¯', 'å—¯', 'å•Š', 'å“¦', 'å–”', 'å‘ƒ', 'æ¬¸',
            'å¥½', 'å°', 'å””', 'å’¦', 'å‘€', 'å•¦', 'å›‰', 'å–',
            'å˜©', 'å˜»', 'å“‡', 'å–‚', 'å“', 'å—¨'  # æ–°å¢æ„Ÿå˜†è©
        }

        # æª¢æ¸¬é‡è¤‡æ®µè½çš„å‡½æ•¸
        def is_repetitive_text(text: str) -> bool:
            """æª¢æ¸¬æ–‡æœ¬æ˜¯å¦æ˜¯é‡è¤‡çš„çŸ­è©ï¼ˆå¦‚ã€Œå˜© å˜© å˜©ã€æˆ–ã€Œä¸æ˜¯ ä¸æ˜¯ ä¸æ˜¯ ä¸æ˜¯ ä¸æ˜¯ã€ï¼‰"""
            # ç§»é™¤æ¨™é»ç¬¦è™Ÿï¼Œåªä¿ç•™æ–‡å­—
            clean_text = text.replace('!', ' ').replace('ï¼', ' ').replace(',', ' ').replace('ï¼Œ', ' ').strip()
            words = [w for w in clean_text.split() if w]

            if len(words) < 2:
                return False

            # æª¢æŸ¥æ˜¯å¦æ‰€æœ‰è©éƒ½ç›¸åŒ
            first_word = words[0]
            if all(word == first_word for word in words):
                # æª¢æ¸¬å…©ç¨®æƒ…æ³ï¼š
                # 1. æ„Ÿå˜†è©ï¼ˆå˜©ã€å˜»ï¼‰ï¼šâ‰¥2 æ¬¡é‡è¤‡å³ç‚ºå¹»è¦º
                # 2. å…¶ä»–è©ï¼šâ‰¥5 æ¬¡é‡è¤‡æ‰ç®—å¹»è¦ºï¼ˆé¿å…èª¤åˆªã€Œå¥½å¥½å¥½ã€ã€Œä¿‚ä¿‚ä¿‚ã€ï¼‰
                if first_word in {'å˜©', 'å˜»', 'å“‡', 'å–‚', 'å“', 'å—¨'}:
                    if len(words) >= 2 and first_word in common_hallucinations:
                        return True
                else:
                    if len(words) >= 5 and first_word in common_hallucinations:
                        return True

            return False

        # ç¬¬ä¸€éšæ®µï¼šæ¨™è¨˜éœ€è¦ç§»é™¤çš„ç´¢å¼•
        indices_to_remove = set()

        # æª¢æ¸¬å–®å€‹æ®µè½å…§çš„é‡è¤‡ï¼ˆå¦‚ã€Œä¸æ˜¯ ä¸æ˜¯ ä¸æ˜¯ã€ï¼‰
        for i, sub in enumerate(subtitles):
            # åªæª¢æŸ¥æœ€å¾Œ 20% æ™‚é–“ç¯„åœå…§çš„å­—å¹•
            if sub.start < hallucination_threshold:
                continue

            if is_repetitive_text(sub.colloquial):
                indices_to_remove.add(i)
                logger.warning(f"Hallucination detected (repetitive): segment {i} - '{sub.colloquial}'")

        # æª¢æ¸¬é€£çºŒç›¸åŒæ®µè½ï¼ˆ3 å€‹åŠä»¥ä¸Šï¼‰
        consecutive_count = 1
        prev_text = None
        consecutive_start = -1

        for i, sub in enumerate(subtitles):
            # åªæª¢æŸ¥æœ€å¾Œ 20% æ™‚é–“ç¯„åœå…§çš„å­—å¹•
            if sub.start < hallucination_threshold:
                prev_text = sub.colloquial.strip()
                consecutive_count = 1
                continue

            current_text = sub.colloquial.strip()

            if current_text == prev_text and current_text:
                if consecutive_count == 1:
                    consecutive_start = i - 1
                consecutive_count += 1
            else:
                # å¦‚æœé€£çºŒ 3 å€‹åŠä»¥ä¸Šç›¸åŒï¼Œæ¨™è¨˜ç‚ºç§»é™¤ï¼ˆé™¤äº†ç¬¬ä¸€å€‹ï¼‰
                if consecutive_count >= 3:
                    for idx in range(consecutive_start + 1, consecutive_start + consecutive_count):
                        indices_to_remove.add(idx)
                        logger.warning(f"Hallucination detected (consecutive): segment {idx} - '{subtitles[idx].colloquial}'")

                consecutive_count = 1
                consecutive_start = -1

            prev_text = current_text

        # è™•ç†æœ€å¾Œä¸€çµ„é€£çºŒæ®µè½
        if consecutive_count >= 3:
            for idx in range(consecutive_start + 1, consecutive_start + consecutive_count):
                indices_to_remove.add(idx)
                logger.warning(f"Hallucination detected (consecutive end): segment {idx} - '{subtitles[idx].colloquial}'")

        # ç¬¬äºŒéšæ®µï¼šç§»é™¤æ¨™è¨˜çš„æ®µè½
        if indices_to_remove:
            filtered_subtitles = [sub for i, sub in enumerate(subtitles) if i not in indices_to_remove]
            # å¢å¼·æ—¥èªŒï¼šè©³ç´°è¨˜éŒ„å¹»è¦ºç§»é™¤æƒ…æ³
            logger.info(f"ğŸ“Š Hallucination check: {len(indices_to_remove)} segments flagged for removal")
            logger.info(f"Subtitle count: {len(subtitles)} -> {len(filtered_subtitles)}")
            return filtered_subtitles
        else:
            logger.info("ğŸ“Š Hallucination check: No hallucinations detected")
            return subtitles

    def _optimize_sentence_boundaries(
        self,
        subtitles: List[SubtitleEntryV2],
        progress_callback: Optional[Callable] = None
    ) -> List[SubtitleEntryV2]:
        """
        Use LLM to optimize sentence boundaries for semantic completeness.

        VAD splits by audio pauses, which may break sentences mid-thought.
        This method uses LLM to:
        1. Detect incomplete sentences (e.g., "ä½ ä»½ç©©å®šï¼Œ")
        2. Merge them with the next segment for completeness
        3. Split overly long segments if needed
        4. Preserve accurate timestamps

        Args:
            subtitles: List of subtitle entries from VAD
            progress_callback: Optional progress callback

        Returns:
            Optimized subtitle list with complete sentences
        """
        if not subtitles or len(subtitles) < 2:
            return subtitles

        # åˆå§‹åŒ– LLMï¼ˆä½¿ç”¨ MLX Qwenï¼‰
        from utils.qwen_mlx import MLXQwenLLM

        try:
            llm = MLXQwenLLM(model_id="mlx-community/Qwen2.5-3B-Instruct-bf16")
            logger.info("LLM loaded for sentence boundary optimization")
        except Exception as e:
            logger.warning(f"Failed to load LLM for sentence optimization: {e}")
            logger.info("Skipping sentence optimization, returning original subtitles")
            return subtitles

        # æ‰¹é‡è™•ç†ï¼ˆæ¯æ¬¡è™•ç† 5 å¥ï¼Œæä¾›å‰å¾Œæ›´å¤šä¸Šä¸‹æ–‡ï¼‰
        batch_size = 5
        context_window = 3  # å¢å¼·ä¸Šä¸‹æ–‡ï¼šå¾ 2 å¥æ“´å±•åˆ° 3 å¥
        optimized_subtitles = []
        merge_instructions = {}  # {index: "merge_with_next" / "keep" / "split"}

        total_batches = (len(subtitles) + batch_size - 1) // batch_size

        for batch_idx in range(0, len(subtitles), batch_size):
            batch_end = min(batch_idx + batch_size, len(subtitles))
            batch = subtitles[batch_idx:batch_end]

            # æ§‹å»ºä¸Šä¸‹æ–‡ï¼ˆæ“´å±•çª—å£ä»¥æä¾›æ›´å¤šèªå¢ƒï¼‰
            context_before = []
            if batch_idx > 0:
                context_start = max(0, batch_idx - context_window)
                context_before = [sub.colloquial for sub in subtitles[context_start:batch_idx]]

            context_after = []
            if batch_end < len(subtitles):
                context_end = min(len(subtitles), batch_end + context_window)
                context_after = [sub.colloquial for sub in subtitles[batch_end:context_end]]

            # æ§‹å»º Promptï¼ˆå¢å¼·ä¸Šä¸‹æ–‡åˆ©ç”¨ï¼‰
            batch_texts = "\n".join([f"{i+1}. {sub.colloquial}" for i, sub in enumerate(batch)])
            context_prompt = ""

            if context_before:
                context_prompt += "\nã€å‰æ–‡ä¸Šä¸‹æ–‡ã€‘ï¼ˆç”¨æ–¼ç†è§£èªå¢ƒå’Œè©±é¡Œé€£è²«æ€§ï¼‰\n" + "\n".join([f"  â€¢ {t}" for t in context_before])

            if context_after:
                context_prompt += "\nã€å¾ŒçºŒä¸Šä¸‹æ–‡ã€‘ï¼ˆç”¨æ–¼åˆ¤æ–·ç•¶å‰å¥æ˜¯å¦å®Œæ•´ï¼‰\n" + "\n".join([f"  â€¢ {t}" for t in context_after])

            prompt = f"""ä½ æ˜¯ç²µèªå­—å¹•æ–·å¥å„ªåŒ–å°ˆå®¶ã€‚ä»»å‹™ï¼šåˆ¤æ–·æ¯å¥è©±æ˜¯å¦éœ€è¦åˆä½µæˆ–æ‹†åˆ†ï¼Œç›®æ¨™æ˜¯ä¿æŒ 1-2 å¥çš„çŸ­å­—å¹•ã€‚

ã€æ ¸å¿ƒåŸå‰‡ã€‘
1. çŸ­å­—å¹•å„ªå…ˆï¼šæ¯æ¢å­—å¹•æœ€å¤š 1-2 å¥è©±ï¼ˆç†æƒ³æ˜¯ 1 å¥ï¼‰
2. æ¿€é€²æ‹†åˆ†ç­–ç•¥ï¼šé‡åˆ°é•·å¥ï¼ˆ>20 å­—ï¼‰å„ªå…ˆæ‹†åˆ†ï¼Œè€Œéåˆä½µ
3. èªæ„å®Œæ•´æ€§ï¼šåªåœ¨å¥å­æ˜é¡¯ä¸å®Œæ•´æ™‚æ‰åˆä½µï¼ˆå¦‚ç¼ºä¸»å¥ã€å¾å¥æœªå®Œï¼‰
4. ç²µèªåœé “ç‰¹æ€§ï¼šèªæ°£è©ï¼ˆå¦‚"å˜…è©±"ã€"å’æ¨£"ï¼‰é€šå¸¸æ˜¯è‡ªç„¶æ–·å¥é»

ã€åˆ¤æ–·æ¨™æº–ã€‘
âœ… keepï¼ˆä¿æŒåŸç‹€ï¼‰- ç¬¦åˆä»¥ä¸‹ä»»ä¸€æ¢ä»¶ï¼š
  â€¢ å¥å­èªæ„å®Œæ•´ï¼ˆå³ä½¿è¼ƒçŸ­ä¹Ÿ keepï¼ŒçŸ­å­—å¹•æ›´å¥½ï¼‰
  â€¢ æœ‰æ˜ç¢ºèªæ°£åœé “ï¼ˆå¦‚é€—è™Ÿã€èªæ°£è©å¾Œï¼‰
  â€¢ èˆ‡ä¸‹ä¸€å¥è©±é¡Œä¸åŒ
  â€¢ å¥å­é•·åº¦å·²ç¶“è¼ƒé•·ï¼ˆ>15 å­—ï¼‰ï¼Œä¸æ‡‰å†åˆä½µ

âŒ mergeï¼ˆåˆä½µä¸‹ä¸€å¥ï¼‰- **åƒ…åœ¨**ç¬¦åˆä»¥ä¸‹æ¢ä»¶æ™‚æ‰åˆä½µï¼š
  â€¢ å¥å­æ˜é¡¯ä¸å®Œæ•´ï¼ˆå¦‚"ä½ ä»½ç©©å®šï¼Œ"å¾Œç¼ºä¸»å¥ï¼‰
  â€¢ æ˜é¡¯çš„å¾å¥æˆ–æ¢ä»¶å¥æœªå®Œï¼ˆå¦‚"å¦‚æœä½ ..."ã€"å› ç‚º..."å¾Œæ²’ä¸»å¥ï¼‰
  â€¢ åˆä½µå¾Œç¸½é•·åº¦ â‰¤20 å­—

âš ï¸ splitï¼ˆå»ºè­°æ‹†åˆ†ï¼‰- ç¬¦åˆä»¥ä¸‹æ¢ä»¶æ™‚æ‡‰æ‹†åˆ†ï¼š
  â€¢ å¥å­éé•·ï¼ˆ>25 å­—ï¼‰
  â€¢ åŒ…å«å¤šå€‹å­å¥æˆ–ä¸¦åˆ—å¥
  â€¢ å¯åœ¨é€—è™Ÿã€èªæ°£è©è™•è‡ªç„¶æ‹†åˆ†

ã€ç¤ºä¾‹åˆ†æã€‘
ä¾‹ 1: "ä½ ä»½ç©©å®šï¼Œ" (8 å­—) + å¾ŒçºŒï¼š"åªæ˜¯ä¸€å€‹å‡è±¡"
  â†’ åˆ¤æ–·ï¼šmergeï¼ˆé€—è™Ÿå¾Œç¼ºä¸»å¥ï¼Œä¸”åˆä½µå¾Œ <20 å­—ï¼‰

ä¾‹ 2: "ä»Šå¤©é€™éƒ¨å½±ç‰‡æƒ³è¨˜éŒ„æˆ‘åœ¨éŸ“åœ‹çš„æ—¥å¸¸ç”Ÿæ´»" (18 å­—)
  â†’ åˆ¤æ–·ï¼škeepï¼ˆèªæ„å®Œæ•´ï¼Œé•·åº¦é©ä¸­ï¼Œä¸æ‡‰å†åˆä½µï¼‰

ä¾‹ 3: "å¦‚æœä½ ä»Šæ—¥å””å»ºç«‹ç¬¬äºŒç³§å€‰ï¼Œ10å¹´å¾Œä½ éƒ½é‡è¦ç‚ºä½ å˜…ç¬¬ä¸€ç³§å€‰å˜…åšæ®º" (28 å­—)
  â†’ åˆ¤æ–·ï¼šsplitï¼ˆéé•·ï¼Œæ‡‰æ‹†æˆ"å¦‚æœä½ ä»Šæ—¥å””å»ºç«‹ç¬¬äºŒç³§å€‰" + "10å¹´å¾Œä½ éƒ½é‡è¦ç‚ºä½ å˜…ç¬¬ä¸€ç³§å€‰å˜…åšæ®º"ï¼‰

ä¾‹ 4: "å¥½å¥½å¥½" (3 å­—) + å¾ŒçºŒï¼š"æˆ‘æ˜ç™½äº†"
  â†’ åˆ¤æ–·ï¼škeepï¼ˆé›–ç„¶çŸ­ï¼Œä½†èªæ„å®Œæ•´ï¼Œä¿æŒçŸ­å­—å¹•æ›´ä½³ï¼‰
{context_prompt}

ã€éœ€è¦åˆ†æçš„å¥å­ã€‘
{batch_texts}

ã€é‡è¦æç¤ºã€‘
- **å„ªå…ˆä¿æŒçŸ­å­—å¹•**ï¼ˆ1-2 å¥ï¼‰ï¼Œé¿å…éåº¦åˆä½µ
- åªæœ‰åœ¨å¥å­æ˜é¡¯ä¸å®Œæ•´æ™‚æ‰ merge
- é‡åˆ°é•·å¥å„ªå…ˆè€ƒæ…® keep æˆ– splitï¼Œè€Œé merge
- çŸ­å­—å¹•ï¼ˆ<10 å­—ï¼‰å³ä½¿çœ‹èµ·ä¾†ç°¡çŸ­ï¼Œä¹Ÿæ‡‰ keepï¼ˆçŸ­å­—å¹•æ›´æ˜“é–±è®€ï¼‰

ã€è¼¸å‡ºæ ¼å¼ã€‘ï¼ˆåªè¼¸å‡ºä»¥ä¸‹æ ¼å¼ï¼Œä¸è¦é¡å¤–è§£é‡‹ï¼‰
1. keep æˆ– merge
2. keep æˆ– merge
3. keep æˆ– merge
...

ã€è¼¸å‡ºã€‘"""

            try:
                response = llm.generate(prompt, max_tokens=256, temperature=0)
                lines = response.strip().split('\n')

                # è§£æ LLM çš„å»ºè­°
                for i, line in enumerate(lines[:len(batch)]):
                    parts = line.split('.')
                    if len(parts) >= 2:
                        action = parts[1].strip().lower()
                        global_idx = batch_idx + i
                        if 'merge' in action:
                            merge_instructions[global_idx] = 'merge_with_next'
                        elif 'split' in action:
                            merge_instructions[global_idx] = 'split'
                        else:
                            merge_instructions[global_idx] = 'keep'

            except Exception as e:
                logger.warning(f"LLM sentence analysis failed for batch {batch_idx}: {e}")
                # å¤±æ•—æ™‚ä¿æŒåŸæ¨£
                for i in range(len(batch)):
                    merge_instructions[batch_idx + i] = 'keep'

            # Report progress
            if progress_callback:
                progress = 90 + int((batch_idx / len(subtitles)) * 10)
                progress_callback(progress)

        # æ ¹æ“š LLM å»ºè­°åŸ·è¡Œåˆä½µ
        i = 0
        while i < len(subtitles):
            action = merge_instructions.get(i, 'keep')

            if action == 'merge_with_next' and i + 1 < len(subtitles):
                # åˆä½µç•¶å‰å’Œä¸‹ä¸€å¥
                merged_text = subtitles[i].colloquial + " " + subtitles[i + 1].colloquial
                
                # NEW: Validate merged length (max 40 chars)
                if len(merged_text) > 40:
                    logger.warning(f"Merged segment too long ({len(merged_text)} chars), keeping separate: '{merged_text[:30]}...'")
                    # Keep current segment and continue to next
                    optimized_subtitles.append(subtitles[i])
                    i += 1
                    continue
                
                merged_sub = SubtitleEntryV2(
                    start=subtitles[i].start,
                    end=subtitles[i + 1].end,
                    colloquial=merged_text.strip(),
                    formal=None
                )
                optimized_subtitles.append(merged_sub)
                logger.info(f"Merged segments {i} and {i+1}: '{merged_text[:50]}...'")
                i += 2  # è·³éä¸‹ä¸€å¥ï¼ˆå·²åˆä½µï¼‰
            else:
                # ä¿æŒåŸæ¨£
                optimized_subtitles.append(subtitles[i])
                i += 1

        # æ¸…ç† LLM
        del llm
        import gc
        gc.collect()

        # å¢å¼·æ—¥èªŒï¼šè¨˜éŒ„ LLM å„ªåŒ–çµæœ
        merged_count = len(subtitles) - len(optimized_subtitles)
        logger.info(f"ğŸ“Š LLM optimization: {len(subtitles)} -> {len(optimized_subtitles)} segments (merged {merged_count} segments)")
        return optimized_subtitles

    # _refine_with_llm removed - æ›¸é¢èª conversion is handled by StyleControlPanel

    # ==================== çµ‚æ¥µæ¨¡å¼ ====================

    def process_ultimate(
        self,
        input_path: str,
        progress_callback: Optional[Callable] = None,
        status_callback: Optional[Callable] = None
    ) -> List[SubtitleEntryV2]:
        """
        çµ‚æ¥µè½‰éŒ„æ¨¡å¼ - æ¥µè‡´æº–ç¢ºåº¦

        æ•´åˆæ‰€æœ‰é«˜ç´šå„ªåŒ–ç­–ç•¥ï¼š
        1. éŸ³é »é è™•ç†ï¼ˆé™å™ª + äººè²å¢å¼·ï¼‰
        2. VAD é åˆ†å‰²ï¼ˆç¢ºä¿ä¸åœ¨å¥ä¸­åˆ‡æ–·ï¼‰
        3. ä¸‰éšæ®µè½‰éŒ„ï¼ˆç²—è½‰éŒ„ â†’ é‡è½‰éŒ„ä½ä¿¡å¿ƒ â†’ LLM æ ¡æ­£ï¼‰
        4. éŒ¨é»ç³»çµ±ï¼ˆé«˜ä¿¡å¿ƒé©—è­‰ä½ä¿¡å¿ƒï¼‰
        5. ç”¨æˆ¶è©å½™å­¸ç¿’ï¼ˆå€‹äººåŒ–æ ¡æ­£ï¼‰

        æ³¨æ„ï¼šè™•ç†æ™‚é–“ç´„ç‚ºæ¨™æº–æ¨¡å¼çš„ 2-3 å€

        Args:
            input_path: éŸ³é »/è¦–é »æ–‡ä»¶è·¯å¾‘
            progress_callback: é€²åº¦å›èª¿ (0-100)
            status_callback: ç‹€æ…‹è¨Šæ¯å›èª¿

        Returns:
            å­—å¹•åˆ—è¡¨
        """
        if not HAS_ADVANCED_FEATURES:
            logger.warning("é«˜ç´šè½‰éŒ„æ¨¡çµ„æœªå®‰è£ï¼Œä½¿ç”¨æ¨™æº–æ¨¡å¼")
            return self.process(input_path, progress_callback, status_callback)

        logger.info("ğŸš€ å•Ÿå‹•çµ‚æ¥µè½‰éŒ„æ¨¡å¼...")
        input_file = Path(input_path)

        import gc
        import torch

        if progress_callback:
            progress_callback(0)

        # Step 1: æ¸…ç†å…§å­˜
        gc.collect()
        if torch.backends.mps.is_available():
            torch.mps.empty_cache()
        elif torch.cuda.is_available():
            torch.cuda.empty_cache()

        # Step 2: æº–å‚™éŸ³é »
        if status_callback:
            status_callback("æº–å‚™éŸ³é »...")

        video_extensions = {'.mp4', '.avi', '.mov', '.mkv', '.webm', '.m4v'}
        if input_file.suffix.lower() in video_extensions:
            audio_path = self._extract_audio(input_file)
        else:
            audio_path = str(input_file)

        if progress_callback:
            progress_callback(5)

        # Step 3: éŸ³é »é è™•ç†
        if status_callback:
            status_callback("éŸ³é »é è™•ç†ï¼ˆé™å™ªå¢å¼·ï¼‰...")

        enhancer = AudioEnhancer(self.temp_dir)
        quality = enhancer.analyze_audio_quality(audio_path)
        logger.info(f"éŸ³é »è³ªé‡: SNR={quality['snr_estimate']:.1f}dB")

        if quality['needs_enhancement']:
            logger.info("éŸ³é »éœ€è¦å¢å¼·...")
            audio_path = enhancer.quick_enhance(audio_path)

        if progress_callback:
            progress_callback(15)

        # Step 4: åŠ è¼‰ ASR æ¨¡å‹
        if status_callback:
            status_callback("åŠ è¼‰ AI æ¨¡å‹...")

        self._load_asr(progress_callback, status_callback)

        if progress_callback:
            progress_callback(25)

        # Step 5: åˆå§‹åŒ–é«˜ç´šè½‰éŒ„å™¨
        advanced_transcriber = AdvancedTranscriber(self.config)

        # åˆå§‹åŒ– VAD
        if self.vad is None:
            self.vad = VADProcessor(
                self.config,
                threshold=0.10,
                min_silence_duration_ms=300,
                min_speech_duration_ms=50,
                speech_pad_ms=500
            )

        # Step 6: åŸ·è¡Œä¸‰éšæ®µè½‰éŒ„
        if status_callback:
            status_callback("åŸ·è¡Œé«˜ç²¾åº¦è½‰éŒ„...")

        def transcribe_progress(p):
            if progress_callback:
                # æ˜ å°„ 0-100 åˆ° 25-80
                progress_callback(25 + int(p * 0.55))

        transcription_chunks = advanced_transcriber.three_stage_transcribe(
            audio_path,
            self.asr,
            self.vad,
            progress_callback=transcribe_progress,
            status_callback=status_callback
        )

        if progress_callback:
            progress_callback(80)

        # Step 7: è½‰æ›ç‚º SubtitleEntryV2 æ ¼å¼
        if status_callback:
            status_callback("å¾Œè™•ç†æ ¡æ­£...")

        # ç²å–ç”¨æˆ¶è©å½™
        vocab_learner = get_vocabulary_learner()
        user_prompt = vocab_learner.generate_whisper_prompt()
        if user_prompt:
            logger.info(f"æ‡‰ç”¨ç”¨æˆ¶è©å½™: {len(vocab_learner.vocabulary)} å€‹")

        final_subtitles = []
        for chunk in transcription_chunks:
            # æ‡‰ç”¨ç°¡å–®æ ¡æ­£
            text = self._apply_simple_corrections(chunk.text.strip())

            # æ‡‰ç”¨ç”¨æˆ¶è©å½™è‡ªå‹•æ ¡æ­£
            text = auto_correct_text(text)

            final_subtitles.append(SubtitleEntryV2(
                start=chunk.start,
                end=chunk.end,
                colloquial=text,
                formal=None
            ))

        # Step 8: èªæ°£è©ä¿®æ­£
        final_subtitles = self._fix_sentence_final_particles(final_subtitles)

        # Step 9: å¹»è¦ºç§»é™¤
        final_subtitles = self._remove_ending_hallucinations(final_subtitles)

        if progress_callback:
            progress_callback(90)

        # Step 10: LLM æ–·å¥å„ªåŒ–ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
        enable_llm_segmentation = self.config.get("enable_llm_sentence_optimization", True)
        if enable_llm_segmentation:
            if status_callback:
                status_callback("AI æ–·å¥å„ªåŒ–...")

            gc.collect()
            if torch.backends.mps.is_available():
                torch.mps.empty_cache()

            final_subtitles = self._optimize_sentence_boundaries(
                final_subtitles, progress_callback
            )

        if progress_callback:
            progress_callback(100)

        # æ¸…ç†
        advanced_transcriber.cleanup()
        enhancer.cleanup()

        logger.info(f"âœ… çµ‚æ¥µè½‰éŒ„å®Œæˆï¼š{len(final_subtitles)} å€‹å­—å¹•")
        return final_subtitles

    def process(
        self,
        input_path: str,
        progress_callback: Optional[Callable] = None,
        status_callback: Optional[Callable] = None
    ) -> List[SubtitleEntryV2]:
        """
        Run the subtitle generation pipeline with sequential model loading.
        
        Pipeline stages (memory efficient - one model at a time):
        1. Load ASR (Whisper)
        2. Extract audio if needed
        3. Transcribe with Whisper
        4. Unload Whisper to free GPU memory
        5. Load LLM (if enabled)
        6. Refine with LLM
        
        Args:
            input_path: Path to audio/video file
            progress_callback: Progress callback (0-100)
            status_callback: Status message callback for UI updates
            
        Returns:
            List of SubtitleEntryV2 with colloquial (and optional formal) text
        """
        # æª¢æŸ¥æ˜¯å¦å•Ÿç”¨çµ‚æ¥µæ¨¡å¼
        ultimate_mode = self.config.get("enable_ultimate_transcription", False)
        if ultimate_mode and HAS_ADVANCED_FEATURES:
            logger.info("ğŸš€ çµ‚æ¥µæ¨¡å¼å·²å•Ÿç”¨ï¼Œä½¿ç”¨é«˜ç²¾åº¦è½‰éŒ„...")
            return self.process_ultimate(input_path, progress_callback, status_callback)

        logger.info(f"Starting V2 pipeline for: {input_path}")
        logger.info("Using sequential model loading (memory efficient mode)")
        input_file = Path(input_path)

        # Step 0: Check LLM availability and download if needed (0-5%)
        # ä¿®å¾©é¦–æ¬¡ä½¿ç”¨å•é¡Œï¼šè½‰è­¯å‰æª¢æŸ¥ LLMï¼Œæœªä¸‹è¼‰æ™‚å…ˆæç¤ºä¸¦ä¸‹è¼‰
        enable_llm_optimization = self.config.get("enable_llm_sentence_optimization", True)

        if enable_llm_optimization:
            from huggingface_hub import try_to_load_from_cache

            # æª¢æŸ¥ LLM æ˜¯å¦å·²ä¸‹è¼‰
            llm_model_id = "mlx-community/Qwen2.5-3B-Instruct-bf16"
            cache_result = try_to_load_from_cache(llm_model_id, "config.json")
            llm_cached = cache_result is not None

            if not llm_cached:
                logger.warning(f"âš ï¸ LLM æ¨¡å‹æœªä¸‹è¼‰ï¼Œé¦–æ¬¡ä½¿ç”¨éœ€ä¸‹è¼‰ 3-6GB æ¨¡å‹")
                if status_callback:
                    status_callback("é¦–æ¬¡ä½¿ç”¨éœ€ä¸‹è¼‰ AI æ¨¡å‹ï¼ˆ3-6GBï¼‰ï¼Œè«‹ç¨å€™...")

                # é ä¸‹è¼‰ LLM æ¨¡å‹ï¼ˆé¿å…è½‰è­¯ä¸­é€”å¡ä½ï¼‰
                try:
                    from utils.qwen_mlx import MLXQwenLLM
                    llm_temp = MLXQwenLLM(model_id=llm_model_id)

                    def llm_progress_callback(msg):
                        if status_callback:
                            status_callback(f"æ­£åœ¨ä¸‹è¼‰ AI æ¨¡å‹ï¼š{msg}")
                        logger.info(msg)

                    llm_temp.load_model(progress_callback=llm_progress_callback)
                    del llm_temp
                    logger.info("âœ… LLM æ¨¡å‹é ä¸‹è¼‰å®Œæˆ")
                    if status_callback:
                        status_callback("AI æ¨¡å‹ä¸‹è¼‰å®Œæˆï¼Œé–‹å§‹è½‰è­¯...")
                except Exception as e:
                    logger.warning(f"LLM é ä¸‹è¼‰å¤±æ•—: {e}")
                    logger.info("å°‡åœ¨è½‰è­¯éç¨‹ä¸­è‡ªå‹•ä¸‹è¼‰ï¼ˆå¯èƒ½è¼ƒæ…¢ï¼‰")
                    if status_callback:
                        status_callback("AI æ¨¡å‹ä¸‹è¼‰å¤±æ•—ï¼Œå°‡ä½¿ç”¨åŸºç¤æ¨¡å¼...")

        # Step 0.5: Pre-clear GPU memory (0-5%)
        import gc
        import torch

        if progress_callback:
            progress_callback(0)
        
        logger.info("Pre-clearing GPU memory before pipeline start...")
        gc.collect()
        # Cross-platform GPU memory cleanup
        if torch.backends.mps.is_available():
            torch.mps.empty_cache()
            logger.info("MPS memory cache cleared, ready for model loading")
        elif torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
            logger.info("CUDA memory cache cleared, ready for model loading")
        
        if progress_callback:
            progress_callback(5)
        
        # Step 1: Load ASR model only (5-15%)
        self._load_asr(progress_callback, status_callback=status_callback)
        
        # Step 2: Prepare audio (15-20%)
        if progress_callback:
            progress_callback(15)
        
        # Check if video needs audio extraction
        video_extensions = {'.mp4', '.avi', '.mov', '.mkv', '.webm', '.m4v'}
        if input_file.suffix.lower() in video_extensions:
            audio_path = self._extract_audio(input_file)
        else:
            audio_path = str(input_file)
        
        # Step 3: Transcribe with Whisper (20-60%)
        if progress_callback:
            progress_callback(20)
        
        logger.info("Running Whisper transcription...")

        # Get language style from config
        language_style = self.config.get("subtitle_language_style", "formal")
        
        # Get custom prompt from config (user-defined vocabulary for better recognition)
        custom_prompt = self.config.get("whisper_custom_prompt", "")

        # Build kwargs for transcribe based on language style
        transcribe_kwargs = {
            'language': 'yue',
            'language_style': language_style,
        }

        # ä¿®å¾©å­—å¹•è¾¨è­˜éŒ¯èª¤ï¼šè‡ªå‹•å•Ÿç”¨æ—…éŠ+è³¼ç‰©è©å½™ï¼ˆæ¶µè“‹å¤§éƒ¨åˆ†æ—¥å¸¸å½±ç‰‡å ´æ™¯ï¼‰
        # å•Ÿç”¨ travel å’Œ shopping domain å¯å¤§å¹…æå‡åœ°åã€å•†å“åç¨±çš„è¾¨è­˜æº–ç¢ºåº¦
        transcribe_kwargs['domain'] = 'travel'  # é è¨­ä½¿ç”¨æ—…éŠè©å½™ï¼ˆæ¶µè“‹åœ°åã€æ™¯é»ã€é…’åº—ç­‰ï¼‰
        logger.info("ğŸ—ºï¸ Enabled travel domain vocabulary for better location/place name recognition")

        # Add custom prompt if provided
        if custom_prompt:
            transcribe_kwargs['custom_prompt'] = custom_prompt
            logger.info(f"Using custom prompt: {custom_prompt[:50]}...")
        
        logger.info(f"ğŸ“ Transcription language style: {language_style}")

        result = self.asr.transcribe(audio_path, **transcribe_kwargs)
        
        whisper_segments = result.get('segments', [])
        logger.info(f"Whisper produced {len(whisper_segments)} segments")
        
        if not whisper_segments:
            logger.warning("No segments from Whisper")
            return []
        
        if progress_callback:
            progress_callback(60)
        
        # Step 4: VAD-based smart segmentation (60-75%)
        logger.info("Running VAD-based smart segmentation...")
        if progress_callback:
            progress_callback(62)
        
        try:
            # Initialize VAD processor (å„ªåŒ–æ–·å¥é€£è²«æ€§)
            if self.vad is None:
                # ä¿®å¾©å­—å¹•éºæ¼å•é¡Œï¼šå„ªåŒ– VAD åƒæ•¸ï¼Œæ¸›å°‘æ¼æª¢
                self.vad = VADProcessor(
                    self.config,
                    threshold=0.10,                # é™ä½é–€æª» (0.15â†’0.10)ï¼Œæ›´æ•æ„Ÿï¼Œæ¸›å°‘æ¼æª¢è¼•è²èªªè©±
                    min_silence_duration_ms=300,   # ç¸®çŸ­éœéŸ³åˆ¤æ–· (500â†’300ms)ï¼Œé¿å…éåº¦æ‹†åˆ†
                    min_speech_duration_ms=50,     # å…è¨±æ›´çŸ­èªéŸ³ (100â†’50ms)ï¼Œä¿ç•™å¿«é€Ÿèªªè©±
                    speech_pad_ms=500              # å¢åŠ å¡«å…… (300â†’500ms)ï¼Œä¿ç•™å®Œæ•´èªå¥
                )

            # Detect voice segments
            voice_segments = self.vad.detect_voice_segments(audio_path)
            logger.info(f"VAD detected {len(voice_segments)} voice segments")

            if progress_callback:
                progress_callback(68)

            # Merge Whisper + VAD for smart segmentation
            # ä¿®å¾©å­—å¹•éåº¦åˆä½µå•é¡Œï¼šç¸®çŸ­åˆä½µåƒæ•¸ï¼Œä¿æŒ 1-2 å¥çš„çŸ­å­—å¹•
            # å•Ÿç”¨ä¿å®ˆæ¨¡å¼ï¼šä¿ç•™ç„¡ VAD é‡ç–Šä½†æœ‰æ„ç¾©çš„æ®µè½ï¼Œæ¸›å°‘æ–‡å­—éºæ¼
            optimized_segments = self.vad.merge_with_transcription(
                whisper_segments,
                voice_segments,
                max_gap=0.8,           # ç¸®çŸ­åœé “é–¾å€¼ (1.5â†’0.8s)ï¼Œæ¸›å°‘åˆä½µï¼Œä¿æŒçŸ­å¥
                max_chars=30,          # ç¸®çŸ­å­—æ•¸é™åˆ¶ (50â†’30å­—)ï¼Œå¼·åˆ¶æ‹†åˆ†é•·å¥
                conservative_mode=True # ä¿å®ˆæ¨¡å¼ï¼šä¿ç•™å¯ç–‘æ®µè½ï¼Œæ¸›å°‘æ–‡å­—éºæ¼
            )
            logger.info(f"VAD optimization: {len(whisper_segments)} -> {len(optimized_segments)} segments")
            
            # Use optimized segments
            whisper_segments = optimized_segments
            
        except Exception as e:
            logger.warning(f"VAD segmentation failed, using original Whisper segments: {e}")
            # Continue with original Whisper segments
        
        if progress_callback:
            progress_callback(75)
        
        # Step 5: Apply simple corrections and create final subtitles (75-95%)
        # Note: æ›¸é¢èª conversion is handled by StyleControlPanel, not here
        if progress_callback:
            progress_callback(85)

        final_subtitles = [
            SubtitleEntryV2(
                start=seg.start,
                end=seg.end,
                colloquial=self._apply_simple_corrections(seg.text.strip()),
                formal=None  # Will be filled by StyleControlPanel if user selects æ›¸é¢èª
            )
            for seg in whisper_segments
        ]

        # Step 5.4: Fix sentence-final particles at wrong position (82-85%)
        # ä¿®æ­£ VAD æ–·å¥éŒ¯èª¤å°è‡´èªæ°£è©å‡ºç¾å–ºå¥é¦–ï¼ˆå¦‚ã€Œå—èŒ¶è‘‰è›‹çš„å‘³é“å‘€ã€ï¼‰
        if progress_callback:
            progress_callback(82)

        logger.info("Fixing sentence-final particles at wrong position...")
        final_subtitles = self._fix_sentence_final_particles(final_subtitles)

        # Step 5.5: Remove Whisper hallucination at the end (85-90%)
        # Whisper often hallucinates repetitive short words at the end when only background music exists
        if progress_callback:
            progress_callback(85)

        final_subtitles = self._remove_ending_hallucinations(final_subtitles)

        # Step 6: LLM intelligent sentence boundary optimization (90-100%)
        # Use LLM to merge incomplete sentences and ensure semantic completeness
        if progress_callback:
            progress_callback(90)

        # Check if LLM sentence optimization is enabled
        enable_llm_segmentation = self.config.get("enable_llm_sentence_optimization", True)
        if enable_llm_segmentation:
            # æ¸…ç† VRAMï¼ˆåœ¨åŠ è¼‰ LLM ä¹‹å‰é‡‹æ”¾ Whisper/VAD ä½¿ç”¨çš„å…§å­˜ï¼‰
            logger.info("Pre-clearing VRAM before LLM sentence optimization...")
            gc.collect()
            if torch.backends.mps.is_available():
                torch.mps.empty_cache()
                logger.info("MPS cache cleared for LLM")
            elif torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
                logger.info("CUDA cache cleared for LLM")

            logger.info("Starting LLM sentence boundary optimization...")
            final_subtitles = self._optimize_sentence_boundaries(final_subtitles, progress_callback)
        else:
            logger.info("LLM sentence optimization disabled, skipping...")

        if progress_callback:
            progress_callback(100)

        logger.info(f"Pipeline complete. Generated {len(final_subtitles)} subtitles")
        return final_subtitles
    
    def cleanup(self):
        """Release all resources."""
        import gc
        import torch
        
        logger.info("Cleaning up pipeline resources...")
        
        if self.asr:
            try:
                self.asr.unload_model()
            except Exception as e:
                logger.warning(f"ASR cleanup error: {e}")
            self.asr = None
        
        # LLM cleanup removed - LLM is handled by StyleControlPanel
        
        if self.vad:
            try:
                self.vad.unload_model()
            except Exception as e:
                logger.warning(f"VAD cleanup error: {e}")
            self.vad = None
        
        self._models_loaded = False
        
        # Force garbage collection
        gc.collect()
        
        # Cross-platform GPU memory cleanup
        try:
            if torch.backends.mps.is_available():
                torch.mps.empty_cache()
                logger.info("MPS memory cache cleared")
            elif torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
                logger.info("CUDA memory cache cleared")
        except Exception as e:
            logger.warning(f"GPU cache clear error: {e}")
        
        logger.info("Pipeline cleanup complete")
    
    def get_profile(self) -> Optional[PerformanceProfile]:
        """Get the current hardware profile."""
        return self.profile
