"""
Background worker for AI transcription tasks.
"""

import os
from pathlib import Path
from typing import Optional, Dict, List
import time

from PySide6.QtCore import QThread, Signal, QObject

import re
from core.config import Config
from models.whisper_asr import WhisperASR, TranscriptionSegment
from models.vad_processor import VADProcessor
from models.llama_corrector import LlamaCorrector
from utils.audio_utils import AudioPreprocessor
from utils.whisper_mlx import get_best_whisper_backend, MLXWhisperASR
from utils.logger import setup_logger

logger = setup_logger()

class TranscribeWorker(QThread):
    """
    Worker thread for executing AI transcription pipeline.
    
    Signals:
        progress (str, int): Status message and progress percentage (0-100)
        finished (dict): Transcription results
        error (str): Error message
    """
    
    progress = Signal(str, int)
    finished = Signal(dict)
    error = Signal(str)
    
    def __init__(self, config: Config, input_path: str, ai_correction_enabled: bool = False, ai_model_path: Optional[str] = None):
        super().__init__()
        self.config = config
        self.input_path = input_path
        self.ai_correction_enabled = ai_correction_enabled
        self.ai_model_path = ai_model_path
        self._is_cancelled = False
        
        # Components
        self.asr = None
        self.vad = None
        self.corrector = None
        
    def run(self):
        """Execute the pipeline sequentially."""
        try:
            input_path = Path(self.input_path)
            if not input_path.exists():
                raise FileNotFoundError(f"File not found: {input_path}")
            
            # 1. Initialize & Load Models (0-30%)
            self.progress.emit("Ê≠£Âú®Âä†Ëºâ AI Ê®°Âûã...", 10)
            
            # Use best available Whisper backend (MLX on Apple Silicon, faster-whisper fallback)
            self.asr = get_best_whisper_backend(self.config)
            self.vad = VADProcessor(self.config)
            
            # Load models
            self.asr.load_model()
            if self._is_cancelled: return
            self.progress.emit("Ê≠£Âú®Âä†Ëºâ AI Ê®°Âûã...", 20)
            
            self.vad.load_model()
            if self._is_cancelled: return
            self.progress.emit("Ê®°ÂûãÂä†ËºâÂÆåÊàê", 30)
            
            # 2. Transcription (30-50%)
            self.progress.emit("Ê≠£Âú®ÁîüÊàêÂ≠óÂπï...", 30)
            
            # Check/Extract audio if needed
            audio_path = input_path
            # Quick check if video, though Whisper can handle many inputs, 
            # might be safer to ensure it's audio or let Whisper handle it.
            # Assuming input is compatible for now.
            
            # Start transcription
            # We don't have a callback for Whisper's internal progress easily without callbacks,
            # so we just show "Processing..." at 30% and jump to 50% when done.
            # Or effectively it stays at 30% until done, then 50%.
            # User said "Then 50% start processing", implying 30->50 transition.

            # Get custom prompt from config (user-defined vocabulary)
            custom_prompt = self.config.get("whisper_custom_prompt", "")
            
            # Get language style from config (formal or colloquial)
            language_style = self.config.get("subtitle_language_style", "formal")
            
            # Build kwargs for transcribe
            transcribe_kwargs = {
                'language_style': language_style,
            }
            
            # Add custom prompt if provided
            if custom_prompt:
                transcribe_kwargs['custom_prompt'] = custom_prompt

            transcription_result = self.asr.transcribe(
                str(audio_path),
                language='yue',
                **transcribe_kwargs
            )

            
            if self._is_cancelled: return
            self.progress.emit("Â≠óÂπïÁîüÊàêÂÆåÊàê", 50)
            
            # 3. Optimization / VAD Merge (Wait until 80%)
            # User said: "Then 80% start optimization"
            # So we might jump to 50, do some processing, then 80.
            
            segments = transcription_result['segments']
            full_text = transcription_result['text']
            
            self.progress.emit("Ê≠£Âú®ÂÑ™ÂåñÊñ∑Âè• (VAD)...", 80)
            
            # Detect voice segments
            voice_segments = self.vad.detect_voice_segments(audio_path)
            
            # Merge with 9:16 optimized parameters from config
            max_chars = self.config.get('subtitle_max_chars', 15)
            max_gap = self.config.get('subtitle_max_gap', 0.8)
            
            optimized_segments = self.vad.merge_with_transcription(
                segments,
                voice_segments,
                max_gap=max_gap,
                max_chars=max_chars
            )
            
            if self._is_cancelled: return
            
            # 4. AI Correction (80-100%)
            final_segments = optimized_segments
            
            if self.ai_correction_enabled and self.ai_model_path:
                self.progress.emit("Ê≠£Âú®ÈÄ≤Ë°å AI Ê†°Ê≠£...", 90)
                
                # Get language style from config for Qwen enhancement
                language_style = self.config.get("subtitle_language_style", "formal")
                logger.info(f"ü§ñ AI Correction with language style: {language_style}")
                
                try:
                    # Pass language_style to corrector for dual-layer optimization
                    self.corrector = LlamaCorrector(
                        self.ai_model_path,
                        language_style=language_style
                    )
                    
                    # Extract texts
                    texts = [seg.text for seg in final_segments]
                    
                    # Correct batch
                    corrected_texts = self.corrector.correct_batch(texts)
                    
                    # Update segments
                    for seg, new_text in zip(final_segments, corrected_texts):
                        seg.text = new_text
                        
                except Exception as e:
                    logger.error(f"AI Correction failed: {e}")
                    # Continue without correction
            
            if self._is_cancelled: return
            
            # Prepare result
            final_text = "\n".join([seg.text for seg in final_segments])
            
            result = {
                'text': final_text,
                'segments': final_segments,
                'language': transcription_result.get('language', 'yue'),
                'audio_path': str(input_path),
                'original_path': str(input_path)
            }
            
            self.progress.emit("ËôïÁêÜÂÆåÊàê!", 100)
            self.finished.emit(result)
            
        except Exception as e:
            logger.error(f"Transcription failed: {e}", exc_info=True)
            self.error.emit(f"ËôïÁêÜÂ§±Êïó: {str(e)}")
            
        finally:
            # Cleanup
            if self.asr: self.asr.unload_model()
            if self.vad: self.vad.unload_model()
            if self.corrector: self.corrector.unload_model()

    def cancel(self):
        """Cancel the operation."""
        self._is_cancelled = True
