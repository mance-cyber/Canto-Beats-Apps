"""
MLX Qwen backend for Apple Silicon.

Uses Apple's MLX framework for optimal performance on Apple Silicon,
replacing the Transformers-based qwen_llm.py for faster inference.
"""

import sys
import gc
from pathlib import Path
from typing import Optional, Dict, List

from utils.logger import setup_logger

logger = setup_logger()


class MLXQwenLLM:
    """
    MLX Qwen LLM for Apple Silicon optimized inference.
    
    Uses mlx-lm for fast text generation on Apple Silicon.
    Replaces Transformers-based QwenLLM for æ›¸é¢èª conversion.
    """
    
    _mlx_available = None
    
    def __init__(self, model_id: str = "mlx-community/Qwen2.5-3B-Instruct-bf16"):
        """
        Initialize MLX Qwen LLM.
        
        Args:
            model_id: MLX model ID from Hugging Face (mlx-community)
        """
        self.model_id = model_id
        self.model = None
        self.tokenizer = None
        self.is_loaded = False
    
    @classmethod
    def is_available(cls) -> bool:
        """Check if MLX LM is available on this system."""
        if cls._mlx_available is not None:
            return cls._mlx_available
        
        # Only available on macOS with Apple Silicon
        if sys.platform != 'darwin':
            logger.info("MLX Qwen not available: Not macOS")
            cls._mlx_available = False
            return False
        
        # Check for ARM64 architecture
        import platform
        if platform.machine() != 'arm64':
            logger.info("MLX Qwen not available: Not Apple Silicon (ARM64)")
            cls._mlx_available = False
            return False
        
        # Try to import mlx_lm
        try:
            import mlx_lm
            logger.info("âœ… MLX LM is available (Apple Silicon optimized)")
            cls._mlx_available = True
            return True
        except ImportError as e:
            logger.warning(f"MLX LM not available: {e}")
            cls._mlx_available = False
            return False
        except Exception as e:
            logger.warning(f"MLX LM failed to initialize: {e}")
            cls._mlx_available = False
            return False
    
    def load_model(self, progress_callback=None):
        """
        Load MLX Qwen model.
        
        Args:
            progress_callback: Optional callback(message: str) for progress updates
        """
        if self.is_loaded:
            logger.warning("Model already loaded")
            return
        
        if not self.is_available():
            raise RuntimeError("MLX LM is not available on this system")
        
        logger.info(f"ğŸ Loading MLX Qwen model: {self.model_id}")
        
        if progress_callback:
            progress_callback("æ­£åœ¨åŠ è¼‰ AI æ›¸é¢èªå·¥å…·...")
        
        try:
            from mlx_lm import load
            from huggingface_hub import snapshot_download, try_to_load_from_cache
            
            # Check if model is cached
            cache_result = try_to_load_from_cache(self.model_id, "config.json")
            model_cached = cache_result is not None
            
            if not model_cached:
                logger.info(f"Downloading model: {self.model_id}")
                if progress_callback:
                    progress_callback("æ­£åœ¨ä¸‹è¼‰ AI æ›¸é¢èªå·¥å…·...")
                
                # Download with progress tracking
                from tqdm import tqdm
                
                class ProgressTqdm(tqdm):
                    def __init__(self_tqdm, *args, **kwargs):
                        super().__init__(*args, **kwargs)
                        self_tqdm.callback = progress_callback
                    
                    def update(self_tqdm, n=1):
                        super().update(n)
                        if self_tqdm.total and self_tqdm.total > 0 and self_tqdm.callback:
                            downloaded_mb = self_tqdm.n / (1024 * 1024)
                            total_mb = self_tqdm.total / (1024 * 1024)
                            msg = f"ä¸‹è¼‰ä¸­... {downloaded_mb:.0f}MB / {total_mb:.0f}MB"
                            self_tqdm.callback(msg)
                
                snapshot_download(
                    repo_id=self.model_id,
                    tqdm_class=ProgressTqdm if progress_callback else tqdm
                )
                logger.info("âœ… Model downloaded successfully")
            else:
                logger.info("Model already cached, loading from cache...")
            
            if progress_callback:
                progress_callback("æ­£åœ¨åŠ è¼‰æ¨¡å‹...")
            
            # Load model and tokenizer with mlx_lm
            self.model, self.tokenizer = load(self.model_id)
            
            self.is_loaded = True
            logger.info(f"âš¡ MLX Qwen ready")
            
            if progress_callback:
                progress_callback("AI æ›¸é¢èªå·¥å…·å°±ç·’ï¼")
            
        except Exception as e:
            logger.error(f"Failed to load MLX Qwen model: {e}")
            raise
    
    def generate(
        self,
        prompt: str,
        max_tokens: int = 512,
        temperature: float = 0.0,
        **kwargs
    ) -> str:
        """
        Generate text using MLX Qwen.
        
        Args:
            prompt: Input prompt
            max_tokens: Maximum tokens to generate
            temperature: Sampling temperature (0 = deterministic)
            **kwargs: Additional generation parameters
            
        Returns:
            Generated text
        """
        if not self.is_loaded:
            self.load_model()
        
        try:
            from mlx_lm import generate
            
            # Format as chat message
            messages = [{"role": "user", "content": prompt}]
            
            # Apply chat template
            formatted_prompt = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            # Generate response
            response = generate(
                self.model,
                self.tokenizer,
                prompt=formatted_prompt,
                max_tokens=max_tokens,
                temp=temperature,
                verbose=False
            )
            
            return response.strip()
            
        except Exception as e:
            logger.error(f"MLX generation failed: {e}")
            raise
    
    def batch_convert_to_written(
        self,
        segments: List[str],
        batch_size: int = 5,
        progress_callback=None
    ) -> Dict[int, str]:
        """
        Batch convert segments from colloquial to written Chinese.
        
        Args:
            segments: List of text segments
            batch_size: Number of segments per batch
            progress_callback: Optional callback(current, total, message)
            
        Returns:
            Dict of {index: converted_text}
        """
        if not self.is_loaded:
            self.load_model()
        
        result = {}
        total_batches = (len(segments) + batch_size - 1) // batch_size
        
        for batch_idx, batch_start in enumerate(range(0, len(segments), batch_size)):
            batch_end = min(batch_start + batch_size, len(segments))
            batch_texts = segments[batch_start:batch_end]
            
            if progress_callback:
                progress_callback(batch_idx, total_batches, f"AI è½‰æ› {batch_idx + 1}/{total_batches}...")
            
            # Combine texts with markers
            combined = "\n".join([f"{i+1}. {t}" for i, t in enumerate(batch_texts)])
            
            prompt = f"""ä½ æ˜¯ä¸€ä½å°ˆæ¥­ä¸­æ–‡ç·¨è¼¯èˆ‡å­—å¹•è½‰å¯«å¸«ã€‚ä½ çš„ä»»å‹™æ˜¯æŠŠã€Œç²µèªå£èªå­—å¹•ã€å¾¹åº•è½‰è­¯æˆã€Œè‡ªç„¶æµæš¢çš„æ›¸é¢ä¸­æ–‡ã€ã€‚

ã€æ ¸å¿ƒç›®æ¨™ã€‘
- å®Œå…¨æ›¸é¢åŒ–ï¼šæŠŠå£èªã€ç²µèªèªæ°£è©ã€å£é ­ç¦ªã€æ½®èªæ”¹æˆæ­£å¼æ›¸é¢è¡¨é”ã€‚
- ä¸æ”¹æ„æ€ï¼šä¿ç•™åŸå¥è³‡è¨Šã€èªæ°£å¼·å¼±ï¼Œä½†ç”¨æ›¸é¢èªå‘ˆç¾ã€‚
- é©åˆåšå­—å¹•ï¼šå¥å­è¦ç°¡æ½”ã€æ˜“è®€ã€è‡ªç„¶ã€‚
- **è‹±æ–‡å¿…é ˆä¿ç•™**ï¼šæ‰€æœ‰è‹±æ–‡å–®è©ã€å“ç‰Œã€äººåã€è¡“èªç­‰ï¼Œçµ•å°ä¸è¦ç¿»è­¯æˆä¸­æ–‡ã€‚

ã€è½‰è­¯è¦å‰‡ã€‘
1. ç§»é™¤/æ”¹å¯«ç²µèªèªæ°£è©èˆ‡å¡«å……è©ï¼šå¦‚ã€Œå–ã€å•¦ã€å›‰ã€å’©ã€ã—ã€å“ã€å‘€ã€å–‡ã€å•«ã€ç­‰ã€‚
2. ç²—å£è™•ç†ï¼šæ”¹æˆè¼ƒæ–‡æ˜çš„åŒç­‰èªæ°£ã€‚
3. åªè¼¸å‡ºè½‰è­¯å¾Œæ–‡å­—ï¼Œä¿æŒç·¨è™Ÿæ ¼å¼ã€‚
4. **è‹±æ–‡å¿…é ˆä¿ç•™**ï¼šæ‰€æœ‰è‹±æ–‡å–®è©ä¸€å¾‹ä¿æŒåŸæ¨£ã€‚

ã€å¸¸è¦‹è½‰æ›ã€‘
ä¿‚â†’æ˜¯ã€å–ºâ†’åœ¨ã€å””â†’ä¸ã€å†‡â†’æ²’æœ‰ã€å˜…â†’çš„ã€å’—â†’äº†ã€åšŸâ†’ä¾†ã€ä½¢â†’ä»–/å¥¹
å¥½å½©â†’å¹¸é‹ã€é ­å…ˆâ†’å‰›æ‰ã€ç´æ—¥â†’æ˜¨å¤©ã€è½æ—¥â†’æ˜å¤©ã€ä»Šæ—¥â†’ä»Šå¤©ã€è€Œå®¶â†’ç¾åœ¨

ã€è¼¸å…¥ã€‘
{combined}

ã€è¼¸å‡ºã€‘ï¼ˆåªè¼¸å‡ºçµæœï¼Œä¿æŒç·¨è™Ÿï¼‰"""
            
            try:
                response = self.generate(prompt, max_tokens=1024, temperature=0)
                
                # Parse numbered response
                for line in response.strip().split('\n'):
                    line = line.strip()
                    if line and line[0].isdigit():
                        parts = line.split('.', 1)
                        if len(parts) == 2:
                            try:
                                num = int(parts[0]) - 1
                                text = parts[1].strip()
                                
                                # Clean up
                                if 'â†’' in text:
                                    text = text.split('â†’')[-1].strip()
                                for bracket in '()ï¼ˆï¼‰ï¹™ï¹š[]ã€ã€‘ã€Œã€':
                                    text = text.replace(bracket, '')
                                
                                if 0 <= num < len(batch_texts) and text:
                                    result[batch_start + num] = text
                            except ValueError:
                                pass
                
                logger.info(f"Batch {batch_idx + 1}: processed {len(batch_texts)} segments")
                
            except Exception as e:
                logger.warning(f"Batch processing failed: {e}")
        
        if progress_callback:
            progress_callback(total_batches, total_batches, "AI è½‰æ›å®Œæˆ")
        
        return result
    
    def cleanup(self):
        """Clean up resources."""
        self.model = None
        self.tokenizer = None
        self.is_loaded = False
        
        # Clear memory
        gc.collect()
        
        logger.info("MLX Qwen resources cleaned up")
    
    def unload_model(self):
        """Unload the model (alias for cleanup)."""
        self.cleanup()


def get_best_llm_backend(model_size: str = "3B"):
    """
    Get the best available LLM backend for the current system.
    
    Automatically detects hardware and selects optimal backend:
    - Apple Silicon: MLX Qwen (fastest, uses Neural Engine/GPU)
    - MPS available: Transformers Qwen with MPS (Apple GPU)
    - CUDA available: Transformers Qwen with CUDA (NVIDIA GPU)
    - CPU only: Transformers Qwen with CPU (slowest)
    
    Args:
        model_size: Model size (e.g., "3B", "0.5B")
        
    Returns:
        Tuple of (LLM instance, backend_type string)
    """
    import sys
    import platform
    
    backend_type = "unknown"
    
    # Check platform
    is_mac = sys.platform == 'darwin'
    is_apple_silicon = is_mac and platform.machine() == 'arm64'
    
    # Priority 1: MLX on Apple Silicon (fastest for Apple devices)
    if is_apple_silicon:
        try:
            import mlx_lm
            if MLXQwenLLM.is_available():
                logger.info("ğŸ Detected Apple Silicon - using MLX Qwen (fastest)")
                model_id = f"mlx-community/Qwen2.5-{model_size}-Instruct"
                return MLXQwenLLM(model_id=model_id), "mlx"
        except ImportError:
            logger.info("MLX not available, trying MPS fallback")
    
    # Priority 2: Check for GPU (MPS or CUDA)
    try:
        import torch
        
        # Apple MPS (Metal)
        if torch.backends.mps.is_available():
            logger.info("ğŸ Detected MPS (Metal) - using Transformers Qwen with GPU")
            backend_type = "mps"
        # NVIDIA CUDA
        elif torch.cuda.is_available():
            logger.info("ğŸ® Detected CUDA - using Transformers Qwen with GPU")
            backend_type = "cuda"
        else:
            logger.info("ğŸ’» No GPU detected - using Transformers Qwen with CPU")
            backend_type = "cpu"
            
    except ImportError:
        logger.warning("PyTorch not available, defaulting to CPU")
        backend_type = "cpu"
    
    # Fallback to Transformers Qwen
    logger.info(f"Using Transformers Qwen on {backend_type.upper()}")
    from models.qwen_llm import QwenLLM
    from core.config import Config
    from core.hardware_detector import HardwareDetector
    
    config = Config()
    detector = HardwareDetector()
    profile = detector.detect()
    return QwenLLM(config, profile), backend_type


def get_qwen_for_hardware():
    """
    Get the optimal Qwen model configuration based on hardware.
    
    Returns:
        Dict with:
        - model_id: The model to download
        - backend: "mlx", "mps", "cuda", or "cpu"
        - device: Torch device string
        - description: Human-readable description
    """
    import sys
    import platform
    
    is_mac = sys.platform == 'darwin'
    is_apple_silicon = is_mac and platform.machine() == 'arm64'
    
    # Apple Silicon with MLX
    if is_apple_silicon:
        try:
            import mlx_lm
            return {
                "model_id": "mlx-community/Qwen2.5-3B-Instruct-bf16",
                "backend": "mlx",
                "device": "mlx",
                "description": "ğŸ MLX Qwen (Apple Silicon å„ªåŒ–ï¼Œæœ€å¿«é€Ÿ)"
            }
        except ImportError:
            pass
    
    # Check GPU availability
    try:
        import torch
        
        if torch.backends.mps.is_available():
            return {
                "model_id": "Qwen/Qwen2.5-3B-Instruct",
                "backend": "mps",
                "device": "mps",
                "description": "ğŸ MPS Qwen (Apple Metal GPU)"
            }
        elif torch.cuda.is_available():
            vram = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            return {
                "model_id": "Qwen/Qwen2.5-3B-Instruct",
                "backend": "cuda",
                "device": "cuda",
                "description": f"ğŸ® CUDA Qwen (NVIDIA GPU, {vram:.1f}GB VRAM)"
            }
    except ImportError:
        pass
    
    # CPU fallback
    return {
        "model_id": "Qwen/Qwen2.5-3B-Instruct",
        "backend": "cpu",
        "device": "cpu",
        "description": "ğŸ’» CPU Qwen (ç„¡ GPUï¼Œè¼ƒæ…¢)"
    }


# Test function
if __name__ == "__main__":
    print(f"MLX Qwen available: {MLXQwenLLM.is_available()}")
    
    if MLXQwenLLM.is_available():
        llm = MLXQwenLLM()
        print("Loading model...")
        llm.load_model()
        
        # Test generation
        test_prompt = "å°‡ä»¥ä¸‹ç²µèªå£èªè½‰æˆæ›¸é¢èªï¼šã€Œä½ æˆæ—¥è©±ä½ åšç”Ÿæ„ä½ ä¿‚è€é—†ã€"
        result = llm.generate(test_prompt)
        print(f"Result: {result}")
        
        llm.cleanup()
    else:
        print("MLX Qwen not available")
