"""
MLX Whisper backend for Apple Silicon.

Uses Apple's MLX framework for optimal performance on Apple Silicon,
automatically leveraging CoreML (Neural Engine) or MPS (GPU).

Priority: CoreML (Neural Engine) > MPS (GPU) > CPU fallback
"""

import sys
import os
from pathlib import Path
from typing import Optional, Dict, List, Union
from dataclasses import dataclass

from utils.logger import setup_logger

logger = setup_logger()

# ============================================
# CRITICAL: Setup ffmpeg PATH at module load time
# This ensures mlx_whisper's subprocess calls find ffmpeg
# ============================================
def _setup_ffmpeg_path():
    """Ensure Homebrew ffmpeg is in PATH for subprocess calls."""
    homebrew_paths = [
        '/opt/homebrew/bin',  # Apple Silicon
        '/usr/local/bin',      # Intel Mac
    ]
    
    current_path = os.environ.get('PATH', '')
    paths_to_add = []
    
    for hp in homebrew_paths:
        if Path(hp).exists() and hp not in current_path:
            # Check if ffmpeg exists in this path
            if (Path(hp) / 'ffmpeg').exists():
                paths_to_add.append(hp)
                logger.debug(f"Found ffmpeg in {hp}")
    
    if paths_to_add:
        # Prepend to PATH so Homebrew ffmpeg is found first
        os.environ['PATH'] = os.pathsep.join(paths_to_add) + os.pathsep + current_path
        logger.info(f"Added Homebrew paths to PATH for ffmpeg: {paths_to_add}")

# Run at module load
_setup_ffmpeg_path()




@dataclass
class MLXTranscriptionSegment:
    """A segment of transcribed text with timing information."""
    
    id: int
    start: float  # Start time in seconds
    end: float    # End time in seconds
    text: str
    confidence: float = 1.0
    language: str = "yue"
    words: List[Dict] = None


class MLXWhisperASR:
    """
    MLX Whisper Automatic Speech Recognition for Apple Silicon.
    
    Uses Apple's MLX framework which automatically leverages:
    - CoreML (Neural Engine) when available - fastest
    - MPS (Metal GPU) as fallback - fast
    
    This is significantly faster than faster-whisper on Apple Silicon.
    """
    
    _mlx_available = None
    
    def __init__(self, model_size: str = "large-v3"):
        """
        Initialize MLX Whisper ASR.
        
        Args:
            model_size: Model size (e.g., "large-v3", "medium", "small", "base", "tiny")
        """
        self.model_size = model_size
        self.model = None
        self.is_loaded = False
        self._backend_type = None
    
    @classmethod
    def is_available(cls) -> bool:
        """Check if MLX Whisper is available on this system."""
        if cls._mlx_available is not None:
            return cls._mlx_available
        
        # Only available on macOS with Apple Silicon
        if sys.platform != 'darwin':
            logger.info("MLX Whisper not available: Not macOS")
            cls._mlx_available = False
            return False
        
        # Check for ARM64 architecture
        import platform
        if platform.machine() != 'arm64':
            logger.info("MLX Whisper not available: Not Apple Silicon (ARM64)")
            cls._mlx_available = False
            return False
        
        # === Setup MLX paths for PyInstaller packaged app ===
        if getattr(sys, 'frozen', False):
            import os
            
            # Build list of possible mlx lib paths
            possible_paths = []
            
            # macOS .app bundle structure
            exe_dir = Path(sys.executable).parent  # Contents/MacOS
            contents_dir = exe_dir.parent  # Contents/
            possible_paths.append(contents_dir / 'Frameworks' / 'mlx' / 'lib')
            possible_paths.append(contents_dir / 'Frameworks' / 'mlx')
            possible_paths.append(contents_dir / 'Resources' / 'mlx' / 'lib')
            possible_paths.append(contents_dir / 'Resources' / 'mlx')
            
            # PyInstaller onefile _MEIPASS structure
            if hasattr(sys, '_MEIPASS'):
                base_path = Path(sys._MEIPASS)
                possible_paths.append(base_path / 'mlx' / 'lib')
                possible_paths.append(base_path / 'mlx')
                possible_paths.append(base_path)
            
            for mlx_lib_path in possible_paths:
                metallib_path = mlx_lib_path / 'mlx.metallib'
                if metallib_path.exists():
                    # Set environment variables for MLX
                    os.environ['DYLD_LIBRARY_PATH'] = str(mlx_lib_path) + ':' + os.environ.get('DYLD_LIBRARY_PATH', '')
                    os.environ['MLX_METAL_PATH'] = str(metallib_path)
                    logger.info(f"üçé [Packaged App] Set MLX_METAL_PATH to: {metallib_path}")
                    break
            else:
                logger.warning(f"MLX metallib not found in packaged app. Searched: {[str(p) for p in possible_paths]}")
        
        # Try to import mlx.core and test Metal functionality
        try:
            import mlx.core as mx
            
            # Actually test Metal is working
            if mx.metal.is_available():
                # Quick test to ensure Metal works
                test_arr = mx.array([1.0, 2.0, 3.0])
                result = mx.sum(test_arr)
                mx.eval(result)  # Force evaluation on Metal
                logger.info("‚úÖ MLX Metal test passed")
            else:
                logger.warning("MLX Metal not available")
                cls._mlx_available = False
                return False
                
        except Exception as e:
            logger.warning(f"MLX Whisper not available (MLX core failed): {e}")
            cls._mlx_available = False
            return False
        
        # Now try to import mlx_whisper
        try:
            import mlx_whisper
            logger.info("‚úÖ MLX Whisper is available (Apple Silicon optimized)")
            cls._mlx_available = True
            return True
        except ImportError as e:
            logger.warning(f"MLX Whisper not available: {e}")
            cls._mlx_available = False
            return False
    
    def get_backend_type(self) -> str:
        """Get the current backend type being used."""
        if self._backend_type:
            return self._backend_type
        
        if not self.is_available():
            return "cpu"
        
        # MLX automatically uses the best available backend
        try:
            import mlx.core as mx
            # Check if we're using GPU (Metal)
            if mx.metal.is_available():
                self._backend_type = "mps"  # Metal/GPU
                logger.info("üçé MLX using Metal (GPU) backend")
            else:
                self._backend_type = "cpu"
                logger.info("MLX using CPU backend")
            return self._backend_type
        except Exception as e:
            logger.warning(f"Could not determine MLX backend: {e}")
            self._backend_type = "cpu"  # Fallback to CPU
            return "cpu"
    
    def load_model(self, progress_callback=None):
        """Load MLX Whisper model.
        
        Args:
            progress_callback: Optional callback(message: str) for progress updates
        """
        if self.is_loaded:
            logger.warning("Model already loaded")
            return

        if not self.is_available():
            raise RuntimeError("MLX Whisper is not available on this system")

        logger.info(f"üçé Loading MLX Whisper model: {self.model_size}")
        
        if progress_callback:
            progress_callback("Ê≠£Âú®Ê∫ñÂÇô AI Â∑•ÂÖ∑...")

        try:
            import mlx_whisper
            from huggingface_hub import snapshot_download, try_to_load_from_cache

            # MLX Whisper uses model paths like "mlx-community/whisper-large-v3-mlx"
            model_path = f"mlx-community/whisper-{self.model_size}-mlx"
            
            # Check if model is already cached
            cache_result = try_to_load_from_cache(model_path, "config.json")
            # cache_result is a file path if cached, None if not cached
            model_cached = cache_result is not None
            
            if not model_cached:
                # Model needs download - show progress
                logger.info(f"Downloading model: {model_path} (this may take several minutes)")
                if progress_callback:
                    progress_callback("AI Â∑•ÂÖ∑‰∏ãËºâ‰∏≠ÔºàÁ¥Ñ 3-5 GBÔºâÔºåË´ãÁ®çÂÄô...")
                
                try:
                    # Custom tqdm class to report download progress via callback
                    from tqdm import tqdm
                    
                    class ProgressTqdm(tqdm):
                        def __init__(self_tqdm, *args, **kwargs):
                            super().__init__(*args, **kwargs)
                            self_tqdm.callback = progress_callback
                        
                        def update(self_tqdm, n=1):
                            super().update(n)
                            if self_tqdm.total and self_tqdm.total > 0 and self_tqdm.callback:
                                # Calculate download progress
                                downloaded_mb = self_tqdm.n / (1024 * 1024)
                                total_mb = self_tqdm.total / (1024 * 1024)
                                msg = f"AI Â∑•ÂÖ∑‰∏ãËºâ‰∏≠... {downloaded_mb:.0f}MB / {total_mb:.0f}MB"
                                self_tqdm.callback(msg)
                    
                    # Download with progress tracking
                    snapshot_download(
                        repo_id=model_path, 
                        allow_patterns=["*.safetensors", "*.json", "*.npz"],
                        tqdm_class=ProgressTqdm if progress_callback else tqdm
                    )
                    logger.info("‚úÖ Model downloaded successfully")
                    if progress_callback:
                        progress_callback("AI Â∑•ÂÖ∑‰∏ãËºâÂÆåÊàêÔºÅ")
                except Exception as e:
                    logger.warning(f"Model download issue: {e}")
                    if progress_callback:
                        progress_callback("AI Â∑•ÂÖ∑‰∏ãËºâ‰∏≠...")
            else:
                logger.info("Model already cached, loading from cache...")
                if progress_callback:
                    progress_callback("Ê≠£Âú®Âä†Ëºâ AI Â∑•ÂÖ∑...")

            # The model will be used when transcribing
            logger.info(f"Model path: {model_path}")

            self.is_loaded = True
            self.model_path = model_path

            backend = self.get_backend_type()
            logger.info(f"‚ö° MLX Whisper ready on {backend.upper()}")
            
            if progress_callback:
                progress_callback("AI Â∑•ÂÖ∑Â∞±Á∑íÔºÅ")

        except Exception as e:
            logger.error(f"Failed to load MLX Whisper model: {e}")
            raise
    
    def transcribe(
        self,
        audio_path: Union[str, Path],
        language: str = "yue",
        task: str = "transcribe",
        initial_prompt: Optional[str] = None,
        word_timestamps: bool = True,
        **kwargs
    ) -> Dict:
        """
        Transcribe audio using MLX Whisper.
        
        Args:
            audio_path: Path to audio file
            language: Language code (e.g., "yue" for Cantonese)
            task: "transcribe" or "translate"
            initial_prompt: Optional prompt to guide the model
            word_timestamps: Enable word-level timestamps
            **kwargs: Additional options
            
        Returns:
            Dict with transcription results
        """
        if not self.is_loaded:
            self.load_model()
        
        audio_path = Path(audio_path)
        logger.info(f"üçé Transcribing with MLX Whisper: {audio_path.name}")
        
        # Default Cantonese vocabulary (Á¥Ñ 60 ÂÄãÂ∏∏Áî®Âª£Êù±Ë©±Âè£Ë™ûÂ≠ó)
        DEFAULT_CANTONESE_VOCAB = (
            "‰Ω¢„ÄÅÂñ∫„ÄÅÁùá„ÄÅÂòÖ„ÄÅÂíÅ„ÄÅÂï≤„ÄÅÂíó„ÄÅÂöü„ÄÅÂÜá„ÄÅË´ó„ÄÅÂîî„ÄÅÂí©„ÄÅ‰πú„ÄÅÈªû„ÄÅÈÇä„ÄÅÂôâ„ÄÅÂó∞„ÄÅÂë¢„ÄÅÂìã„ÄÅÂí™„ÄÅ"
            "Âõâ„ÄÅÂñé„ÄÅÂï¶„ÄÅ„óé„ÄÅÂêñ„ÄÅÂñá„ÄÅÂòû„ÄÅÂò¢„ÄÅÂñ±„ÄÅÂó±„ÄÅÂëÄ„ÄÅÂíã„ÄÅÂòõ„ÄÅÂï±„ÄÅÊîû„ÄÅÊì∫„ÄÅÂü∑„ÄÅÂ¨≤„ÄÅÁûì„ÄÅÊÖ≥„ÄÅ"
            "ÊèÄ„ÄÅÊêµ„ÄÅÂóå„ÄÅÈùö„ÄÅÊéÇ„ÄÅÂèª„ÄÅÁïÄ„ÄÅÂöø„ÄÅÂïñ„ÄÅÊíê„ÄÅÊè∏„ÄÅÂóÆ„ÄÅÂôÉ„ÄÅÂíß„ÄÅÂò•„ÄÅÊèà„ÄÅ‰ª≤„ÄÅÁ≥ª„ÄÅÂö´„ÄÅÂîû"
        )
        
        # Domain-specific vocabulary for specialized content
        # ‰øÆÂæ©Â≠óÂπïËæ®Ë≠òÈåØË™§ÔºöÊñ∞Â¢ûÊóÖÈÅä„ÄÅË≥ºÁâ©„ÄÅÊó•Â∏∏ÁîüÊ¥ªÂ∏∏Áî®Ë©ûÂΩô
        DOMAIN_VOCAB = {
            "music": "Ê≠åË©û„ÄÅÊóãÂæã„ÄÅÁ∑®Êõ≤„ÄÅÂíåÂº¶„ÄÅÈü≥Èöé„ÄÅ‰ΩúÊõ≤„ÄÅÂ°´Ë©û„ÄÅ‰∏ªÊ≠å„ÄÅÂâØÊ≠å„ÄÅÈñìÂ•è„ÄÅÂâçÂ•è„ÄÅÂ∞æÂ•è„ÄÅÁØÄÊãç„ÄÅÊãçÂ≠ê„ÄÅË™ø„ÄÅÂçáKey„ÄÅÈôçKey„ÄÅÂî±Áâá„ÄÅÂ∞àËºØ„ÄÅÂñÆÊõ≤„ÄÅMV„ÄÅÈü≥Ê®ÇÊúÉ„ÄÅÊºîÂî±ÊúÉ",
            "business": "ÁáüÈä∑„ÄÅÂÆ¢Êà∂„ÄÅÈä∑ÂîÆ„ÄÅÂà©ÊΩ§„ÄÅÊàêÊú¨„ÄÅÊäïË≥á„ÄÅËÇ°‰ªΩ„ÄÅÂêà‰Ωú„ÄÅË´áÂà§„ÄÅÁ∞ΩÁ¥Ñ„ÄÅÂêàÂêå„ÄÅÈ†ÖÁõÆ„ÄÅÊ•≠Âãô„ÄÅÂ∏ÇÂ†¥„ÄÅÂìÅÁâå„ÄÅÊé®Âª£„ÄÅÁ≠ñÁï•„ÄÅÁ´∂Áà≠„ÄÅÂÑ™Âã¢„ÄÅÂ¢ûÈï∑",
            "daily": "È£≤Ëå∂„ÄÅÈ£≤È£ü„ÄÅË°åË°ó„ÄÅÁùáÊà≤„ÄÅÊâìÊ©ü„ÄÅËøîÂ∑•„ÄÅÊîæÂÅá„ÄÅÊãçÊãñ„ÄÅÈ£üÈ£Ø„ÄÅÂéªË°ó„ÄÅË≤∑È§∏„ÄÅÁÖÆÈ£Ø„ÄÅÂÅöÂÆ∂Âãô„ÄÅÁùáÊõ∏„ÄÅËÅΩÊ≠å„ÄÅÈÅãÂãï„ÄÅ‰ºëÊÅØ„ÄÅÊîæÈ¨Ü„ÄÅÂ®õÊ®Ç„ÄÅÊ∂àÈÅ£",
            "tech": "Á®ãÂºè„ÄÅÁ∑®Á®ã„ÄÅËªü‰ª∂„ÄÅÁ°¨‰ª∂„ÄÅÁ≥ªÁµ±„ÄÅÁ∂≤Áµ°„ÄÅ‰º∫ÊúçÂô®„ÄÅÊï∏Êìö„ÄÅÁÆóÊ≥ï„ÄÅÁ∂≤Á´ô„ÄÅÊáâÁî®„ÄÅApp„ÄÅÂπ≥Âè∞„ÄÅÈñãÁôº„ÄÅÊ∏¨Ë©¶„ÄÅÈÉ®ÁΩ≤„ÄÅÁ∂≠Ë≠∑„ÄÅÂÑ™Âåñ„ÄÅÊõ¥Êñ∞„ÄÅÂçáÁ¥ö",
            "travel": "‰∏äÈáé„ÄÅÁßãËëâÂéü„ÄÅÈòøÁæéÊ©´‰∏Å„ÄÅÊ∑∫Ëçâ„ÄÅÊñ∞ÂÆø„ÄÅÊæÄË∞∑„ÄÅÈäÄÂ∫ß„ÄÅÁØâÂú∞„ÄÅÂè∞Â†¥„ÄÅÂØåÂ£´Â±±„ÄÅÁÆ±Ê†π„ÄÅ‰∫¨ÈÉΩ„ÄÅÂ§ßÈò™„ÄÅÊ≤ñÁπ©„ÄÅÂåóÊµ∑ÈÅì„ÄÅÈÖíÂ∫ó„ÄÅÊóÖÈ§®„ÄÅÊ©üÂ†¥„ÄÅËªäÁ´ô„ÄÅÊôØÈªû„ÄÅËßÄÂÖâ„ÄÅÊóÖÈÅä„ÄÅË°åÊùéÁÆ±„ÄÅË≠∑ÁÖß„ÄÅÁ∞ΩË≠â",
            "shopping": "ÂïÜÂ†¥„ÄÅÁôæË≤®ÂÖ¨Âè∏„ÄÅ‰æøÂà©Â∫ó„ÄÅËó•Â¶ùÂ∫ó„ÄÅË∂ÖÂ∏Ç„ÄÅÊâã‰ø°„ÄÅÁ¶ÆÁâ©„ÄÅÁ¥ÄÂøµÂìÅ„ÄÅÂåñÂ¶ùÂìÅ„ÄÅË≠∑ËÜöÂìÅ„ÄÅfigure„ÄÅÊ®°Âûã„ÄÅÂÖ¨‰ªî„ÄÅÈõªÂô®„ÄÅÈõªÂ≠êÁî¢ÂìÅ„ÄÅÂêçÁâå„ÄÅÊâìÊäò„ÄÅÊ∏õÂÉπ„ÄÅÁâπÂÉπ„ÄÅ‰ø°Áî®Âç°„ÄÅÁèæÈáë„ÄÅÊâæÊèõÂ∫ó"
        }
        
        if initial_prompt is None and language in ["yue", "zh"]:
            # Get language style from kwargs (passed from config)
            language_style = kwargs.pop("language_style", "formal")
            
            # Build prompt based on language style
            if language_style == "colloquial":
                # Âè£Ë™ûÊ®°ÂºèÔºö‰ΩøÁî®Á≤µË™ûÂè£Ë™ûÂ≠óÔºàÂº∑ÂåñÁâàÔºåÊòéÁ¢∫Ë¶ÅÊ±ÇÁ≤µË™ûË©ûÂΩô+Â∏∏Ë¶ãË™§Âà§‰øÆÊ≠£Ôºâ
                base_prompt = (
                    f"‰ª•‰∏ã‰øÇÂª£Êù±Ë©±Â∞çÁôΩÔºåË´ãÁî®Á≤µË™ûÂè£Ë™ûÂ≠óÂπïÔºåÂÖ®Á®ã‰øùÊåÅ‰∏ÄËá¥ÂòÖÂè£Ë™ûÈ¢®Ê†º„ÄÇ"
                    f"ÂøÖÈ†à‰ΩøÁî®Á≤µË™ûË©ûÂΩôÔºö{DEFAULT_CANTONESE_VOCAB}„ÄÇ"
                    "‰æãÂ¶ÇÔºöÁî®„ÄåÈÇä‰∏ÄÂÄã„ÄçÂîîÂ•ΩÁî®„ÄåÂì™‰∏ÄÂÄã„ÄçÔºåÁî®„ÄåÈªûËß£„ÄçÂîîÂ•ΩÁî®„ÄåÁÇ∫‰ªÄÈ∫º„ÄçÔºå"
                    "Áî®„Äå‰πúÂò¢„ÄçÂîîÂ•ΩÁî®„Äå‰ªÄÈ∫º„ÄçÔºåÁî®„ÄåÈÇäÂ∫¶„ÄçÂîîÂ•ΩÁî®„ÄåÂì™Ë£°„Äç„ÄÇ"
                    "Â∏∏Ë¶ãË©ûÂΩôÊ≠£Á¢∫ÂØ´Ê≥ïÔºö„ÄåË°åÊùé„Äç„ÄÅ„ÄåÈÅéÈ¶¨Ë∑Ø„Äç„ÄÅ„ÄåÊúÄÂæå‰∏ÄÊó•„Äç„ÄÅ„ÄåÂ•ΩÂ§öÂò¢Ë≤∑„Äç„ÄÅ"
                    "„Äåfigure„ÄçÔºàÊ®°ÂûãÔºâ„ÄÅ„Äåpremium„ÄçÔºàÂÑ™Ë≥™Ôºâ„ÄÅ„ÄåÂü∑Ë°åÊùé„Äç„ÄÅ„ÄåÈòøÁæéÊ©´‰∏Å„Äç„ÄÇ"
                    "Êï∏Â≠óÊ≠£Á¢∫ËÆÄÊ≥ïÔºö„Äå21„ÄçËÆÄ‰Ωú„Äå‰∫åÂçÅ‰∏Ä„ÄçÔºåÂîî‰øÇ„ÄåÂ∞æ21„Äç„ÄÇ"
                )
                logger.info("üìù Using colloquial Cantonese style (Âè£Ë™û) with correction hints")
            else:
                # Êõ∏Èù¢Ë™ûÊ®°ÂºèÔºö‰ΩøÁî®Ê≠£Âºè‰∏≠Êñá
                base_prompt = (
                    "‰ª•‰∏ãÊòØÂª£Êù±Ë©±Â∞çÁôΩÔºåË´ãÁî®Ê≠£ÂºèÊõ∏Èù¢‰∏≠ÊñáÂ≠óÂπïÔºå"
                    "‰∏çË¶Å‰ΩøÁî®Á≤µË™ûÂè£Ë™ûÂ≠óÔºàÂ¶Ç‰Ω¢„ÄÅÂñ∫„ÄÅÁùá„ÄÅÂòÖ„ÄÅÂíÅ„ÄÅÂï≤„ÄÅÂíóÁ≠âÔºâÔºå"
                    "ÂÖ®Á®ã‰øùÊåÅ‰∏ÄËá¥ÁöÑÊõ∏Èù¢Ë™ûÈ¢®Ê†º„ÄÇ"
                    "‰æãÂ¶ÇÔºö‰ΩøÁî®„Äå‰ªñÂú®Áúã„ÄçËÄå‰∏çÊòØ„Äå‰Ω¢Âñ∫Áùá„ÄçÔºå‰ΩøÁî®„ÄåÈÄôÊ®£„ÄçËÄå‰∏çÊòØ„ÄåÂôâ„Äç„ÄÇ"
                )
                logger.info("üìù Using formal written Chinese style (Êõ∏Èù¢Ë™û)")
            
            # Check for domain-specific vocabulary from kwargs
            domain = kwargs.pop("domain", None)
            if domain and domain in DOMAIN_VOCAB:
                domain_vocab = DOMAIN_VOCAB[domain]
                base_prompt = f"{base_prompt}„ÄÇ{domain}Áõ∏ÈóúË©ûÂΩôÔºö{domain_vocab}"
                logger.info(f"üìö Using domain vocab: {domain}")
            
            # Check for custom prompt from kwargs (passed from config)
            custom_prompt = kwargs.pop("custom_prompt", "")
            if custom_prompt:
                initial_prompt = f"{base_prompt}„ÄÇÁî®Êà∂ÊåáÂÆöË©ûÂΩôÔºö{custom_prompt}„ÄÇ"
                logger.info(f"üé§ Using custom prompt: {custom_prompt[:50]}...")
            else:
                initial_prompt = f"{base_prompt}„ÄÇ"
        

        
        try:
            import mlx_whisper
            import os  # Import os at the top of try block
            
            # Disable tqdm progress bars in packaged environment to avoid Broken pipe errors
            # when stdout is closed in GUI apps
            if getattr(sys, 'frozen', False):
                os.environ['HF_HUB_DISABLE_PROGRESS_BARS'] = '1'
                os.environ['TQDM_DISABLE'] = '1'
            
            # === CRITICAL FIX: Monkeypatch mlx_whisper to use full ffmpeg path ===
            # Problem: mlx_whisper.audio.load_audio() calls subprocess.run(['ffmpeg', ...])
            # In packaged .app bundles, subprocess doesn't find 'ffmpeg' even if in PATH
            # Solution: Patch load_audio to use absolute ffmpeg path
            import mlx_whisper.audio
            from core.path_setup import get_ffmpeg_path
            
            _original_load_audio = mlx_whisper.audio.load_audio
            ffmpeg_full_path = get_ffmpeg_path()
            
            def _patched_load_audio(file: str, sr: int = 16000):
                """Patched load_audio that uses full ffmpeg path."""
                import subprocess
                import numpy as np
                import mlx.core as mx
                
                cmd = [
                    ffmpeg_full_path,  # Use absolute path instead of 'ffmpeg'
                    "-nostdin",
                    "-threads", "0",
                    "-i", file,
                    "-f", "s16le",
                    "-ac", "1",
                    "-acodec", "pcm_s16le",
                    "-ar", str(sr),
                    "-"
                ]
                
                try:
                    out = subprocess.run(cmd, capture_output=True, check=True).stdout
                except FileNotFoundError:
                    raise FileNotFoundError(
                        f"FFmpeg not found at: {ffmpeg_full_path}\n"
                        f"Please install FFmpeg: brew install ffmpeg"
                    )
                except subprocess.CalledProcessError as e:
                    raise RuntimeError(f"FFmpeg failed: {e.stderr.decode()}")
                
                # CRITICAL FIX: Return MLX array, not NumPy array!
                # This matches the original mlx_whisper.audio.load_audio return type
                return mx.array(np.frombuffer(out, np.int16)).flatten().astype(mx.float32) / 32768.0
            
            # Apply the patch
            mlx_whisper.audio.load_audio = _patched_load_audio
            logger.info(f"üîß Patched mlx_whisper to use ffmpeg at: {ffmpeg_full_path}")

            
            # Transcribe using MLX Whisper with optimized parameters
            # NOTE: MLX Whisper doesn't support beam_size/best_of yet
            # ‰øÆÂæ©Â≠óÂπïËæ®Ë≠òÈåØË™§ÔºöË™øÊï¥ÂèÉÊï∏‰ª•ÊèêÂçáÊ∫ñÁ¢∫Â∫¶
            result = mlx_whisper.transcribe(
                str(audio_path),
                path_or_hf_repo=self.model_path,
                language=language,
                task=task,
                initial_prompt=initial_prompt,
                word_timestamps=word_timestamps,
                # Optimization parameters (MLX compatible only)
                temperature=0.0,                    # Deterministic output (no randomness)
                condition_on_previous_text=True,    # Use context from previous segments
                no_speech_threshold=0.4,            # Èôç‰Ωé no_speech ÈñæÂÄºÔºà0.6‚Üí0.4ÔºâÔºåÊ∏õÂ∞ëÈùúÈü≥Ë™§Âà§
                logprob_threshold=-0.8,             # Èôç‰Ωé logprob ÈñæÂÄºÔºà-1.0‚Üí-0.8ÔºâÔºå‰øùÁïôÊõ¥Â§ö‰Ωé‰ø°ÂøÉÊÆµËêΩ
                compression_ratio_threshold=2.4,    # ÊîæÂØ¨Â£ìÁ∏ÆÁéáÈñæÂÄºÔºà2.4‚Üí2.4ÔºâÔºåÊ∏õÂ∞ëÈáçË§áÊ™¢Ê∏¨Ë™§ÊÆ∫
                **kwargs
            )
            
            # Extract segments
            segment_list = self._extract_segments(result.get('segments', []), language)
            
            # Full text
            full_text = result.get('text', '')
            
            logger.info(f"‚ö° MLX transcription complete: {len(segment_list)} segments")
            
            return {
                'text': full_text,
                'segments': segment_list,
                'language': result.get('language', language),
                'language_probability': 1.0,
                'backend': 'mlx-whisper'
            }
            
        except Exception as e:
            logger.error(f"MLX transcription failed: {e}")
            raise
    
    def _extract_segments(self, segments: List[Dict], language: str) -> List[MLXTranscriptionSegment]:
        """
        Extract segments from MLX Whisper output with confidence filtering.
        Filters out low-confidence segments (background music, noise).
        """
        segment_list = []
        filtered_count = 0
        
        for i, seg in enumerate(segments):
            # Filter out segments with high no_speech_prob (likely background noise/music)
            no_speech_prob = seg.get('no_speech_prob', 0.0)
            if no_speech_prob > 0.6:  # 60% threshold
                filtered_count += 1
                logger.debug(f"Filtered segment {i}: no_speech_prob={no_speech_prob:.2f}")
                continue
            
            words = []
            if 'words' in seg and seg['words']:
                words = [
                    {
                        'word': w.get('word', ''),
                        'start': w.get('start', 0),
                        'end': w.get('end', 0),
                        'probability': w.get('probability', 1.0)
                    }
                    for w in seg['words']
                ]
            
            segment = MLXTranscriptionSegment(
                id=len(segment_list),  # Use filtered index
                start=seg.get('start', 0),
                end=seg.get('end', 0),
                text=seg.get('text', ''),
                confidence=1.0 - no_speech_prob,  # Convert to confidence score
                language=language,
                words=words
            )
            segment_list.append(segment)
        
        if filtered_count > 0:
            logger.info(f"üîç Filtered {filtered_count} low-confidence segments")
        
        return segment_list
    
    def cleanup(self):
        """Clean up resources."""
        self.model = None
        self.is_loaded = False
        
        # Clear MLX memory
        try:
            import gc
            gc.collect()
        except:
            pass
        
        logger.info("MLX Whisper resources cleaned up")
    
    def unload_model(self):
        """Unload the model (alias for cleanup for API compatibility)."""
        self.cleanup()
    
    def get_model_info(self) -> Dict:
        """Get information about the loaded model."""
        return {
            'model_size': self.model_size,
            'is_loaded': self.is_loaded,
            'backend': 'mlx-whisper',
            'device': self.get_backend_type(),
            'platform': 'Apple Silicon',
            'available': self.is_available()
        }


def get_best_whisper_backend(config=None, model_size: str = "large-v3"):
    """
    Get the best available Whisper backend for the current system.
    
    Priority:
    1. MLX Whisper (CoreML/MPS) - Apple Silicon
    2. faster-whisper (CPU) - Fallback
    
    Args:
        config: Application config (for faster-whisper fallback)
        model_size: Model size to use
        
    Returns:
        Whisper ASR instance (either MLXWhisperASR or WhisperASR)
    """
    # Try MLX Whisper first (Apple Silicon)
    if MLXWhisperASR.is_available():
        logger.info("üçé Using MLX Whisper (Apple Silicon optimized)")
        return MLXWhisperASR(model_size=model_size)
    
    # Fallback to faster-whisper
    logger.info("Using faster-whisper (CPU) as fallback")
    from models.whisper_asr import WhisperASR
    if config is None:
        from core.config import Config
        config = Config()
    return WhisperASR(config, model_size=model_size)


# Test function
if __name__ == "__main__":
    print(f"MLX Whisper available: {MLXWhisperASR.is_available()}")
    
    if MLXWhisperASR.is_available():
        asr = MLXWhisperASR()
        print(f"Backend type: {asr.get_backend_type()}")
        print(f"Model info: {asr.get_model_info()}")
    else:
        print("MLX Whisper not available, would use faster-whisper fallback")
