"""
Style Processor for subtitle text transformation.
"""

import json
import re
from pathlib import Path
from typing import List, Dict, Optional

from utils.logger import setup_logger
# NOTE: TranslationModel is NOT imported here to avoid triggering
# full transformers loading at startup (causes torchcodec issues in PyInstaller).
# Import it lazily in _translate_with_ai() when needed.
from core.config import Config

# OpenCC for simplified to traditional conversion
try:
    from opencc import OpenCC
    HAS_OPENCC = True
except ImportError:
    HAS_OPENCC = False

logger = setup_logger()

class StyleProcessor:
    """
    Process subtitle text based on style options.
    """
    
    def __init__(self, config: Optional[Config] = None):
        self.config = config or Config() # Fallback for tests
        self.cantonese_map = {}
        self.profanity_map = {}
        self.english_map = {}  # Initialize to prevent AttributeError
        self.translation_model = None
        self.llm_processor = None  # For AI-powered style conversion
        self.translation_cache = {}  # Cache for English translations: {english: chinese}
        
        # Initialize OpenCC for S2T conversion
        if HAS_OPENCC:
            self.s2t_converter = OpenCC('s2hk')  # Simplified to Traditional (Hong Kong)
            logger.info("OpenCC S2HK converter initialized")
        else:
            self.s2t_converter = None
            logger.warning("âš ï¸  OpenCC not available - AI translations may output Simplified Chinese!")
            logger.warning("    Install with: pip install opencc-python-reimplemented")
        
        self._load_resources()
        
    def _load_resources(self):
        """Load mapping resources - uses get_resource_path for PyInstaller compatibility"""
        from core.path_setup import get_resource_path
        
        try:
            # Use get_resource_path for PyInstaller compatibility
            # This correctly resolves paths in both development and packaged app
            
            # Load Cantonese mapping
            canto_path = get_resource_path('resources/cantonese_mapping.json')
            if Path(canto_path).exists():
                with open(canto_path, 'r', encoding='utf-8') as f:
                    self.cantonese_map = json.load(f)
            else:
                logger.warning(f"Cantonese mapping not found at: {canto_path}")
            
            # Load Profanity mapping
            prof_path = get_resource_path('resources/profanity_mapping.json')
            if Path(prof_path).exists():
                with open(prof_path, 'r', encoding='utf-8') as f:
                    self.profanity_map = json.load(f)
            else:
                logger.warning(f"Profanity mapping not found at: {prof_path}")

            # Load English mapping
            eng_path = get_resource_path('resources/english_mapping.json')
            if Path(eng_path).exists():
                with open(eng_path, 'r', encoding='utf-8') as f:
                    self.english_map = json.load(f)
            else:
                self.english_map = {}
                logger.warning(f"English mapping not found at: {eng_path}")
                    
            logger.info(f"Loaded resources: Canto={len(self.cantonese_map)}, Prof={len(self.profanity_map)}, Eng={len(self.english_map)}")
            
        except Exception as e:
            logger.error(f"Failed to load resources: {e}")

    def process(self, segments: List[Dict], options: Dict, progress_callback=None) -> List[Dict]:
        """
        Main processing method - applies all style transformations.
        
        Args:
            segments: List of subtitle segments with 'start', 'end', 'text'
            options: Dict with style options from StyleControlPanel
            progress_callback: Optional callback function(current, total, message)
            
        Returns:
            Processed segments
        """
        if not segments:
            return []
            
        logger.info(f"Processing {len(segments)} segments with options: {options}")
        
        result_segments = []
        changes_made = 0
        
        # Batch AI processing (5 sentences at a time for speed)
        style = options.get('style', 'spoken')
        # AI æ ¡æ­£ï¼šæ›¸é¢èªå’ŒåŠæ›¸é¢è‡ªå‹•å•Ÿç”¨ AIï¼ˆç„¡éœ€é¡å¤–å‹¾é¸ï¼‰
        use_ai = style in ('semi', 'written')
        
        ai_converted_texts = {}  # index -> converted text
        if use_ai and style in ('semi', 'written'):
            ai_converted_texts = self._batch_ai_convert(segments, style, progress_callback)
        
        for i, seg in enumerate(segments):
            original_text = seg.get('text', '')
            text = original_text
            
            # 1. Convert Cantonese style (use batch result if available)
            if i in ai_converted_texts:
                text = ai_converted_texts[i]
            elif style != 'spoken':
                text = self._convert_cantonese_dict(text, style)  # Dictionary fallback
            
            # 2. Handle English
            eng_mode = options.get('english', 'keep')
            text = self._process_english(text, eng_mode)
            
            # 3. Format numbers
            num_mode = options.get('numbers', 'arabic')
            text = self._format_numbers(text, num_mode)
            
            # 4. Filter profanity
            prof_mode = options.get('profanity', 'keep')
            text = self._filter_profanity(text, prof_mode)
            
            # 5. Remove trailing punctuation
            text = self._remove_trailing_punctuation(text)
            
            # 6. Convert any simplified Chinese to Traditional (final step)
            if self.s2t_converter:
                text = self.s2t_converter.convert(text)
            
            # Log changes for debugging
            if text != original_text:
                changes_made += 1
                if i < 5:  # Only log first 5 changes to avoid spam
                    logger.info(f"[StyleProcessor] Seg {i}: '{original_text[:30]}' -> '{text[:30]}'")
            
            # Create new segment with processed text
            new_seg = {
                'start': seg.get('start', 0),
                'end': seg.get('end', 0),
                'text': text,
                'words': seg.get('words', [])
            }
            result_segments.append(new_seg)
        
        # 6. Split long lines if requested
        if options.get('split_long', False):
            result_segments = self._split_long_lines(result_segments)
        
        logger.info(f"Processing complete: {len(result_segments)} segments, {changes_made} changes made")
        return result_segments
    
    def _remove_trailing_punctuation(self, text: str) -> str:
        """
        Remove trailing punctuation from subtitle text.
        
        Args:
            text: Input text
            
        Returns:
            Text with trailing punctuation removed
        """
        if not text:
            return text
        
        # é¦–å…ˆæ¸…ç†æ‰€æœ‰é¡å‹å˜…æ‹¬è™Ÿï¼ˆWhisper ç¶“å¸¸ç”¢ç”Ÿï¼‰
        brackets_to_remove = '()ï¼ˆï¼‰ï¹™ï¹š[]ã€ã€‘ã€Œã€'
        for bracket in brackets_to_remove:
            text = text.replace(bracket, '')
        
        # Define punctuation to remove (Chinese and English)
        trailing_punct = 'ã€‚ï¼Œï¼ï¼Ÿï¼›ï¼šã€.!?,;:'
        
        # Remove trailing punctuation
        while text and text[-1] in trailing_punct:
            text = text[:-1]
        
        return text

    def _batch_ai_convert(self, segments: List[Dict], style: str, progress_callback=None) -> Dict[int, str]:
        """
        Batch convert segments using AI.
        Priority: MLX Qwen (Apple Silicon) > Transformers Qwen (fallback)
        Returns dict of {index: converted_text}.
        """
        result = {}
        batch_size = 5
        
        # Initialize LLM if needed - auto-detect best backend
        if self.llm_processor is None:
            try:
                import gc
                import torch
                
                # Clear GPU memory before loading model
                logger.info("Clearing GPU memory before loading Qwen model...")
                gc.collect()
                if torch.backends.mps.is_available():
                    torch.mps.empty_cache()
                    logger.info("MPS memory cleared")
                elif torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    logger.info("CUDA memory cleared")
                
                # Auto-detect hardware and get best backend
                from utils.qwen_mlx import get_qwen_for_hardware, MLXQwenLLM
                
                hw_config = get_qwen_for_hardware()
                logger.info(f"ğŸ” Hardware detection: {hw_config['description']}")
                
                if hw_config['backend'] == 'mlx':
                    # Use MLX Qwen (Apple Silicon optimized)
                    from utils.qwen_mlx import MLXQwenLLM
                    from huggingface_hub import try_to_load_from_cache
                    
                    model_id = hw_config['model_id']
                    
                    # Check if model is cached
                    cache_result = try_to_load_from_cache(model_id, "config.json")
                    model_cached = cache_result is not None
                    
                    if not model_cached:
                        # Show download confirmation dialog
                        from PySide6.QtWidgets import QMessageBox, QApplication
                        from ui.download_dialog import ModelDownloadDialog
                        
                        # Get parent window
                        parent = None
                        app = QApplication.instance()
                        if app:
                            for widget in app.topLevelWidgets():
                                if widget.isVisible():
                                    parent = widget
                                    break
                        
                        # Show confirmation
                        msg = QMessageBox(parent)
                        msg.setWindowTitle("ä¸‹è¼‰ AI æ›¸é¢èªå·¥å…·")
                        msg.setIcon(QMessageBox.Information)
                        msg.setText("éœ€è¦ä¸‹è¼‰ AI æ›¸é¢èªå·¥å…·")
                        msg.setInformativeText(
                            "é¦–æ¬¡ä½¿ç”¨æ›¸é¢èªåŠŸèƒ½éœ€è¦ä¸‹è¼‰ AI æ¨¡å‹ (ç´„ 6 GB)ã€‚\n"
                            "ä¸‹è¼‰æ™‚é–“è¦–ç¶²çµ¡é€Ÿåº¦è€Œå®š (ç´„ 2-5 åˆ†é˜)ã€‚\n\n"
                            "æ˜¯å¦ç¹¼çºŒï¼Ÿ"
                        )
                        msg.setStandardButtons(QMessageBox.Yes | QMessageBox.No)
                        msg.setDefaultButton(QMessageBox.Yes)
                        
                        if msg.exec() != QMessageBox.Yes:
                            logger.info("User cancelled Qwen download")
                            return result
                        
                        # Show download progress dialog
                        logger.info(f"Downloading MLX Qwen model: {model_id}")
                        download_dialog = ModelDownloadDialog(model_id, parent=parent)
                        download_dialog.setWindowTitle("ä¸‹è¼‰ AI æ›¸é¢èªå·¥å…·")
                        
                        # Create worker for MLX Qwen download
                        from ui.download_dialog import MLXWhisperDownloadWorker
                        
                        class MLXQwenDownloadWorker(MLXWhisperDownloadWorker):
                            def run(self):
                                try:
                                    from huggingface_hub import snapshot_download, list_repo_files
                                    from tqdm import tqdm
                                    
                                    self.progress.emit(5, "æ­£åœ¨é€£æ¥ä¼ºæœå™¨...")
                                    
                                    # Get list of files to download for better progress tracking
                                    try:
                                        all_files = list(list_repo_files(model_id))
                                        total_files = len(all_files)
                                    except Exception:
                                        total_files = 10  # Estimated fallback
                                    
                                    # Track current file being downloaded
                                    file_counter = {'current': 0, 'name': ''}
                                    
                                    class ProgressTqdm(tqdm):
                                        def __init__(self_tqdm, *args, **kwargs):
                                            super().__init__(*args, **kwargs)
                                            self_tqdm.worker = self
                                            # Extract filename from desc if available
                                            if hasattr(self_tqdm, 'desc') and self_tqdm.desc:
                                                file_counter['name'] = self_tqdm.desc
                                            file_counter['current'] += 1
                                        
                                        def update(self_tqdm, n=1):
                                            super().update(n)
                                            if self_tqdm.total and self_tqdm.total > 0:
                                                file_num = file_counter['current']
                                                # Calculate overall progress based on file count
                                                base_percent = int((file_num - 1) / total_files * 85) + 5
                                                file_percent = int((self_tqdm.n / self_tqdm.total) * (85 / total_files))
                                                percent = min(base_percent + file_percent, 95)
                                                
                                                downloaded_mb = self_tqdm.n / (1024 * 1024)
                                                total_mb = self_tqdm.total / (1024 * 1024)
                                                msg = f"æ–‡ä»¶ {file_num}/{total_files}: {downloaded_mb:.0f}MB / {total_mb:.0f}MB"
                                                self_tqdm.worker.progress.emit(percent, msg)
                                    
                                    snapshot_download(repo_id=model_id, tqdm_class=ProgressTqdm)
                                    
                                    self.progress.emit(100, "ä¸‹è¼‰å®Œæˆ")
                                    self.finished.emit(True, "ä¸‹è¼‰æˆåŠŸ")
                                    
                                except Exception as e:
                                    logger.error(f"MLX Qwen download failed: {e}")
                                    self.finished.emit(False, f"ä¸‹è¼‰å¤±æ•—: {str(e)}")
                        
                        download_dialog.worker = MLXQwenDownloadWorker("bf16")
                        download_dialog.worker.progress.connect(download_dialog._on_progress)
                        download_dialog.worker.finished.connect(download_dialog._on_finished)
                        download_dialog.worker.start()
                        
                        result_dialog = download_dialog.exec()
                        
                        if not download_dialog.was_successful():
                            logger.warning("MLX Qwen download failed or cancelled")
                            return result
                        
                        logger.info("âœ… MLX Qwen download completed")
                    
                    # Now load the model with robust error handling
                    try:
                        self.llm_processor = MLXQwenLLM(model_id=model_id)
                        self.llm_processor.load_model()
                        self._using_mlx = True
                        logger.info(f"âš¡ {hw_config['description']} loaded")
                    except Exception as load_error:
                        logger.error(f"Failed to load MLX Qwen model: {load_error}", exc_info=True)
                        
                        # Show user-friendly error message
                        from PySide6.QtWidgets import QMessageBox, QApplication
                        parent = None
                        app = QApplication.instance()
                        if app:
                            for widget in app.topLevelWidgets():
                                if widget.isVisible():
                                    parent = widget
                                    break
                        
                        msg = QMessageBox(parent)
                        msg.setWindowTitle("AI å·¥å…·è¼‰å…¥å¤±æ•—")
                        msg.setIcon(QMessageBox.Warning)
                        msg.setText("æ›¸é¢èª AI å·¥å…·ç„¡æ³•è¼‰å…¥")
                        msg.setInformativeText(
                            f"éŒ¯èª¤ï¼š{str(load_error)[:100]}\n\n"
                            "å°‡ä½¿ç”¨å­—å…¸æ¨¡å¼é€²è¡Œè½‰æ›ï¼ˆé€Ÿåº¦è¼ƒå¿«ä½†æº–ç¢ºåº¦è¼ƒä½ï¼‰ã€‚\n\n"
                            "å¦‚éœ€å®Œæ•´ AI åŠŸèƒ½ï¼Œè«‹é‡å•Ÿæ‡‰ç”¨æˆ–è¯ç¹«æŠ€è¡“æ”¯æ´ã€‚"
                        )
                        msg.setStandardButtons(QMessageBox.Ok)
                        msg.exec()
                        
                        # Fall back to dictionary mode
                        logger.warning("Falling back to dictionary mode due to model load failure")
                        return result
                else:
                    # Use Transformers Qwen (MPS/CUDA/CPU)
                    self._using_mlx = False
                    from ui.download_dialog import check_and_download_model
                    from core.hardware_detector import HardwareDetector
                    from models.qwen_llm import QwenLLM
                    
                    detector = HardwareDetector()
                    profile = detector.detect()
                    
                    model_ready = check_and_download_model(
                        profile.llm_a_model,
                        profile.llm_a_quantization,
                        parent=None
                    )
                    
                    if not model_ready:
                        logger.warning("Model not ready, using dictionary")
                        return result
                    
                    self.llm_processor = QwenLLM(self.config, profile)
                    self.llm_processor.load_models()
                    logger.info(f"âœ… {hw_config['description']} loaded")
                    
            except Exception as e:
                logger.warning(f"LLM init failed: {e}")
                return result
        
        # CRITICAL CHECK: If LLM failed to initialize, don't proceed with AI conversion
        if self.llm_processor is None:
            logger.warning("LLM processor is None, cannot perform AI conversion - using dictionary fallback")
            return result
        
        # Process in batches
        total_batches = (len(segments) + batch_size - 1) // batch_size
        
        for batch_idx, batch_start in enumerate(range(0, len(segments), batch_size)):
            batch_end = min(batch_start + batch_size, len(segments))
            batch_texts = [segments[i].get('text', '') for i in range(batch_start, batch_end)]
            
            # Report progress
            if progress_callback:
                progress_callback(batch_idx, total_batches, f"AI è½‰æ› {batch_idx + 1}/{total_batches}...")
            
            # Combine texts with markers
            combined = "\n".join([f"{i+1}. {t}" for i, t in enumerate(batch_texts)])

            # Professional prompt for thorough conversion (simplified - no English/number rules)
            prompt = f"""ä½ æ˜¯ä¸€ä½å°ˆæ¥­ä¸­æ–‡ç·¨è¼¯èˆ‡å­—å¹•è½‰å¯«å¸«ã€‚ä½ çš„ä»»å‹™æ˜¯æŠŠã€Œç²µèªå£èªå­—å¹•ã€å¾¹åº•è½‰è­¯æˆã€Œè‡ªç„¶æµæš¢çš„æ›¸é¢ä¸­æ–‡ã€ã€‚

ã€æ ¸å¿ƒç›®æ¨™ã€‘
- å®Œå…¨æ›¸é¢åŒ–ï¼šæŠŠå£èªã€ç²µèªèªæ°£è©ã€å£é ­ç¦ªã€æ½®èªæ”¹æˆæ­£å¼æ›¸é¢è¡¨é”ã€‚
- ä¸æ”¹æ„æ€ï¼šä¿ç•™åŸå¥è³‡è¨Šã€èªæ°£å¼·å¼±ï¼Œä½†ç”¨æ›¸é¢èªå‘ˆç¾ã€‚
- é©åˆåšå­—å¹•ï¼šå¥å­è¦ç°¡æ½”ã€æ˜“è®€ã€è‡ªç„¶ã€‚
- **è‹±æ–‡å¿…é ˆä¿ç•™**ï¼šæ‰€æœ‰è‹±æ–‡å–®è©ã€å“ç‰Œã€äººåã€è¡“èªç­‰ï¼Œçµ•å°ä¸è¦ç¿»è­¯æˆä¸­æ–‡ã€‚

ã€è½‰è­¯è¦å‰‡ã€‘
1. ç§»é™¤/æ”¹å¯«ç²µèªèªæ°£è©èˆ‡å¡«å……è©ï¼šå¦‚ã€Œå–ã€å•¦ã€å›‰ã€å’©ã€ã—ã€å“ã€å‘€ã€å–‡ã€å•«ã€ç­‰ã€‚
2. ç²—å£è™•ç†ï¼šæ”¹æˆè¼ƒæ–‡æ˜çš„åŒç­‰èªæ°£ï¼ˆä¾‹å¦‚ã€Œå¥½æ’šç…©ã€â†’ã€Œéå¸¸ç…©äººã€ï¼‰ã€‚
3. å¥æœ«æ¨™é»è¦æ›¸é¢ï¼šç–‘å•ç”¨ã€Œï¼Ÿã€ã€æ„Ÿå˜†ç”¨ã€Œï¼ã€ï¼Œå…¶é¤˜ç”¨ã€Œã€‚ã€æˆ–ã€Œï¼Œã€ã€‚
4. ä¸è¦æ·»åŠ æ–°è³‡è¨Šã€ä¸è¦è§£é‡‹ã€ä¸è¦è©•è«–ã€‚
5. åªè¼¸å‡ºè½‰è­¯å¾Œæ–‡å­—ï¼Œä¿æŒç·¨è™Ÿæ ¼å¼ã€‚
6. ä¿å®ˆè½‰æ›ï¼šå¦‚æœå””ç¢ºå®šï¼Œä¿ç•™åŸè©ï¼Œå””å¥½çŒœæ¸¬æˆ–å‰µé€ æ–°è©ã€‚
7. **è‹±æ–‡å¿…é ˆä¿ç•™**ï¼šæ‰€æœ‰è‹±æ–‡å–®è©ã€äººåã€å“ç‰Œã€è¡“èªã€æ•¸å­—ç­‰ï¼Œä¸€å¾‹ä¿æŒåŸæ¨£ï¼Œçµ•å°ä¸è¦ç¿»è­¯æˆä¸­æ–‡ã€‚

ã€å¸¸è¦‹è½‰æ›ã€‘
ä¿‚â†’æ˜¯ã€å–ºâ†’åœ¨ã€å””â†’ä¸ã€å†‡â†’æ²’æœ‰ã€å˜…â†’çš„ã€å’—â†’äº†ã€åšŸâ†’ä¾†ã€ä½¢â†’ä»–/å¥¹
å¥½å½©â†’å¹¸é‹ã€é ­å…ˆâ†’å‰›æ‰ã€ç´æ—¥â†’æ˜¨å¤©ã€è½æ—¥â†’æ˜å¤©ã€ä»Šæ—¥â†’ä»Šå¤©ã€è€Œå®¶â†’ç¾åœ¨
å€‹é˜â†’å°æ™‚ã€èšŠâ†’å…ƒã€å³ä¿‚â†’å°±æ˜¯ã€é»è§£â†’ç‚ºä»€éº¼ã€ä¹œå˜¢â†’ä»€éº¼ã€é‚Šåº¦â†’å“ªè£¡

ã€é‡è¦ã€‘è‹±æ–‡ä¿ç•™ç¯„ä¾‹ï¼š
- "Apple" ä¿æŒ "Apple"ï¼Œä¸è¦è®Šæˆã€Œè˜‹æœã€
- "iPhone" ä¿æŒ "iPhone"ï¼Œä¸è¦è®Šæˆã€Œæ„›ç˜‹ã€
- "CEO" ä¿æŒ "CEO"ï¼Œä¸è¦è®Šæˆã€ŒåŸ·è¡Œé•·ã€
- "AI" ä¿æŒ "AI"ï¼Œä¸è¦è®Šæˆã€Œäººå·¥æ™ºèƒ½ã€

ã€é¢¨æ ¼ã€‘ç¹é«”ä¸­æ–‡æ›¸é¢èªï¼Œæ¸…æ™°è‡ªç„¶ã€‚åš´æ ¼åº¦ï¼šæœ€é«˜ï¼Œå‡¡æ˜¯å£èªåŒ–è¡¨é”ä¸€å¾‹æ”¹æˆæ›¸é¢èªã€‚

ã€è¼¸å…¥ã€‘
{combined}

ã€è¼¸å‡ºã€‘ï¼ˆåªè¼¸å‡ºçµæœï¼Œä¿æŒç·¨è™Ÿï¼‰"""
            
            try:
                # Generate response - use correct method based on backend
                if getattr(self, '_using_mlx', False):
                    response = self.llm_processor.generate(
                        prompt,
                        max_tokens=1024,
                        temperature=0
                    )
                else:
                    response = self.llm_processor._generate(
                        prompt, 
                        self.llm_processor._model_a, 
                        self.llm_processor._tokenizer_a,
                        max_new_tokens=1024,
                        temperature=0  # Zero temp for fully deterministic output
                    )
                
                # === DEBUG: Log raw AI response ===
                logger.info(f"=== RAW AI RESPONSE (Batch {batch_idx + 1}) ===")
                logger.info(response[:500] if len(response) > 500 else response)
                logger.info("=== END RAW RESPONSE ===")
                
                # Parse numbered response
                for line in response.strip().split('\n'):
                    line = line.strip()
                    if line and line[0].isdigit():
                        parts = line.split('.', 1)
                        if len(parts) == 2:
                            try:
                                num = int(parts[0]) - 1
                                text = parts[1].strip()
                                
                                # === åš´æ ¼æ¸…ç† AI è¼¸å‡º ===
                                # 1. å¦‚æœæœ‰ç®­é ­ç¬¦è™Ÿï¼Œåªå–ç®­é ­å¾Œé¢å˜…å…§å®¹
                                if 'â†’' in text:
                                    text = text.split('â†’')[-1].strip()
                                if '->' in text:
                                    text = text.split('->')[-1].strip()
                                
                                # 2. ç§»é™¤æ‰€æœ‰é¡å‹æ‹¬è™Ÿ
                                for bracket in '()ï¼ˆï¼‰ï¹™ï¹š[]ã€ã€‘ã€Œã€':
                                    text = text.replace(bracket, '')
                                
                                # 3. æ¸…é™¤ç•°å¸¸å°¾éƒ¨å­—ç¬¦
                                while text and text[-1] in ')ï¼‰ã€ã€‘æ˜¯å‘¢å•¦':
                                    text = text[:-1].strip()
                                
                                # 4. å»é™¤å¤šé¤˜ç©ºç™½
                                text = ' '.join(text.split())
                                
                                if 0 <= num < len(batch_texts) and text:
                                    result[batch_start + num] = text
                            except ValueError:
                                pass
                
                logger.info(f"Batch {batch_idx + 1}: processed {len(batch_texts)} segments")
                
            except Exception as e:
                logger.warning(f"Batch processing failed: {e}")
        
        # Report completion
        if progress_callback:
            progress_callback(total_batches, total_batches, "AI è½‰æ›å®Œæˆï¼")
        
        # Post-process with dictionary to fix AI missed conversions
        post_fix_map = {
            # === åŸºæœ¬ç²µèªâ†’æ›¸é¢èªè½‰æ› ===
            "èšŠ": "å…ƒ",
            "å€‹é˜": "å°æ™‚",
            "å³ä¿‚": "å°±æ˜¯",
            "å–º": "åœ¨",
            "å˜…": "çš„",
            "å’—": "äº†",
            "å†‡": "æ²’æœ‰",
            "å””": "ä¸",
            "ä¿‚": "æ˜¯",
            "ä½¢": "ä»–",
            "åšŸ": "ä¾†",
            "æµ": "æ‰¾",
            "å¥½å½©": "å¹¸é‹",
            "é ­å…ˆ": "å‰›æ‰",
            "ç´æ—¥": "æ˜¨å¤©",
            "è½æ—¥": "æ˜å¤©",
            "ä»Šæ—¥": "ä»Šå¤©",
            "è€Œå®¶": "ç¾åœ¨",
            "é»è§£": "ç‚ºä»€éº¼",
            "ä¹œå˜¢": "ä»€éº¼",
            "é‚Šåº¦": "å“ªè£¡",
            "å‘¢å•²": "é€™äº›",
            "å””ä¿‚": "ä¸æ˜¯",
            "åŠ åŸ‹": "åŠ ä¸Š",
            "å•²": "ä¸€äº›",
            
            # === AI éŒ¯èª¤è½‰æ›ä¿®æ­£ ===
            # AI å¯èƒ½æœƒç”¢ç”ŸéŒ¯èª¤å˜…è«§éŸ³å­—
            "è„«äº†": "é™¤äº†",      # é™¤äº†è¢«éŒ¯èª¤è½‰æˆè„«äº†
            "ä¾¿å®œæ™‚": "å¹³æ™‚",    # å¹³æ™‚è¢«éŒ¯èª¤è½‰æˆä¾¿å®œæ™‚
            "é€¢ä¿‚": "å‡¡æ˜¯",      # å‡¡æ˜¯è¢«éŒ¯èª¤è½‰æˆé€¢ä¿‚
            "é€¢æ˜¯": "å‡¡æ˜¯",      # å‡¡æ˜¯è¢«éŒ¯èª¤è½‰æˆé€¢æ˜¯
            # æ³¨æ„: "å€‹å€‹" ä¿‚æœ‰æ•ˆæ›¸é¢èª (æ„æ€ä¿‚ã€Œæ¯ä¸€å€‹ã€)ï¼Œå””æ‡‰è©²æ›¿æ›
            # æ³¨æ„: "å•²" æœƒè¢«ä¸Šé¢å˜… "å•²": "ä¸€äº›" è™•ç†
            
            # === å¸¸è¦‹ OCR/ASR éŒ¯èª¤ä¿®æ­£ ===
            "è¦–é¡§": "è¦ºå¾—",
            "å‘Šé‹ä½œ": "å°±é‹ä½œ",
            "å˜—": "æ˜¯",          # ä¿®æ­£ï¼šä¹‹å‰éŒ¯èª¤åœ°å¯«æˆ "ä¿‚"
            "ç¸¾å„ª": "å¾ˆå¤š",
        }
        
        for idx in result:
            text = result[idx]
            for canto, written in post_fix_map.items():
                if canto in text:
                    text = text.replace(canto, written)
            result[idx] = text
        
        logger.info(f"Post-processed {len(result)} segments with dictionary cleanup")
        
        return result
    
    def _convert_cantonese_dict(self, text: str, style: str) -> str:
        """Dictionary-based Cantonese conversion (no AI)."""
        if style == 'spoken':
            return text
        
        keep_words_semi = ['ç‡', 'éš', 'å•²', 'å’', 'å’—', 'å˜…', 'å†‡', 'å””']
        result = text
        sorted_keys = sorted(self.cantonese_map.keys(), key=len, reverse=True)
        
        for canto_word, written_word in [(k, self.cantonese_map[k]) for k in sorted_keys]:
            if style == 'semi' and canto_word in keep_words_semi:
                continue
            if canto_word in result:
                result = result.replace(canto_word, written_word)
        
        return result

    def _convert_cantonese(self, text: str, style: str, use_ai: bool = False) -> str:
        """
        Convert Cantonese text based on style.
        
        Args:
            text: Input text
            style: 'spoken', 'semi', or 'written'
            use_ai: Use AI (QwenLLM) for context-aware conversion
            
        Returns:
            Converted text
        """
        if style == 'spoken':
            # Keep original Cantonese
            return text
        
        # AI conversion with Qwen2.5-3B (better quality than 1.5B)
        if use_ai and style in ('semi', 'written'):
            try:
                if self.llm_processor is None:
                    # Check if model needs to be downloaded first
                    try:
                        from ui.download_dialog import check_and_download_model
                        from core.hardware_detector import HardwareDetector
                        
                        detector = HardwareDetector()
                        profile = detector.detect()
                        
                        # Show download dialog if model not cached
                        model_ready = check_and_download_model(
                            profile.llm_a_model,
                            profile.llm_a_quantization,
                            parent=None
                        )
                        
                        if not model_ready:
                            logger.warning("Model download cancelled or failed, using dictionary")
                            # Fall through to dictionary conversion
                        else:
                            # Load the model
                            logger.info("Initializing Qwen2.5-3B for AI style conversion...")
                            from models.qwen_llm import QwenLLM
                            self.llm_processor = QwenLLM(self.config, profile)
                            self.llm_processor.load_models()
                            logger.info("Qwen2.5-3B loaded successfully")
                    except Exception as load_error:
                        logger.warning(f"Failed to load model for AI conversion: {load_error}")
                        # Fall through to dictionary conversion
                
                # CRITICAL: Check llm_processor is ready before using
                if self.llm_processor is not None:
                    # Use specialized prompt for colloquial-to-written conversion
                    from prompts.cantonese_prompts import COLLOQUIAL_TO_WRITTEN_PROMPT
                    prompt = COLLOQUIAL_TO_WRITTEN_PROMPT.format(text=text)
                    
                    # Generate response directly
                    converted = self.llm_processor._generate(prompt, self.llm_processor._model_a, self.llm_processor._tokenizer_a,  max_new_tokens=512, temperature=0.3)
                    
                    # Clean up LLM output - remove common prefixes
                    if converted:
                        converted = converted.strip()
                        # Remove common output prefixes
                        prefixes_to_remove = [
                            "æ›¸é¢èªç¿»è­¯çµæœï¼š", "æ›¸é¢èªï¼š", "ç¿»è­¯çµæœï¼š", 
                            "çµæœï¼š", "ç¿»è­¯ï¼š", "æ›¸é¢èªç‰ˆæœ¬ï¼š"
                        ]
                        for prefix in prefixes_to_remove:
                            if converted.startswith(prefix):
                                converted = converted[len(prefix):].strip()
                        
                        if converted and converted != text.strip():
                            logger.info(f"AI conversion: '{text[:30]}...' -> '{converted[:30]}...'")
                            return converted
                        else:
                            logger.warning(f"AI returned same text or empty, using dictionary")
                else:
                    logger.warning("LLM processor not available, using dictionary fallback")
                    
            except Exception as e:
                logger.warning(f"AI conversion failed, falling back to dictionary: {e}")
        
        # Dictionary-based conversion (fallback or default)
        # For semi-written, keep certain colloquial words
        keep_words_semi = ['ç‡', 'éš', 'å•²', 'å’', 'å’—', 'å˜…', 'å†‡', 'å””']
        
        result = text
        conversions = []
        
        # Sort by length (longest first) to avoid partial replacements
        sorted_keys = sorted(self.cantonese_map.keys(), key=len, reverse=True)
        
        for canto_word, written_word in [(k, self.cantonese_map[k]) for k in sorted_keys]:
            if style == 'semi' and canto_word in keep_words_semi:
                continue  # Skip conversion for semi-written style
            
            if canto_word in result:
                result = result.replace(canto_word, written_word)
                conversions.append(f"{canto_word}â†’{written_word}")
        
        if conversions:
            logger.debug(f"[Cantonese] Conversions: {', '.join(conversions[:5])}")
        
        return result

    def _filter_profanity(self, text: str, mode: str) -> str:
        """Filter profanity based on mode."""
        if mode == 'keep':
            return text
        
        result = text    
        # Sort by length to match phrases first
        sorted_keys = sorted(self.profanity_map.keys(), key=len, reverse=True)
        
        for prof_word in sorted_keys:
            if prof_word in result:
                if mode == 'mask':
                    result = result.replace(prof_word, 'â˜…' * len(prof_word))
                elif mode == 'mild':
                    mild_replacement = self.profanity_map[prof_word]
                    result = result.replace(prof_word, mild_replacement)
        
        return result

    def _format_numbers(self, text: str, mode: str) -> str:
        """Format numbers to Arabic or Chinese."""
        chinese_nums = 'é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒè¬'
        
        if mode == 'chinese':
            # Convert Arabic to Chinese
            digit_map = {'0': 'é›¶', '1': 'ä¸€', '2': 'äºŒ', '3': 'ä¸‰', 
                        '4': 'å››', '5': 'äº”', '6': 'å…­', '7': 'ä¸ƒ',
                        '8': 'å…«', '9': 'ä¹'}
            
            def replace_digit(match):
                return ''.join(digit_map.get(d, d) for d in match.group(0))
            
            return re.sub(r'\d+', replace_digit, text)
        
        elif mode == 'arabic':
            # CRITICAL: Do NOT convert common words containing "ä¸€", "äºŒ", etc.
            # Exclusion list for common words that should NOT be converted
            exclude_words = [
                'ä¸€å®š', 'ä¸€èµ·', 'ä¸€æ¨£', 'ä¸€ç›´', 'ä¸€åˆ‡', 'ç¬¬ä¸€', 'çµ±ä¸€', 'å”¯ä¸€', 
                'ä¸€ä¸‹', 'ä¸€æ¬¡', 'ä¸€å€‹', 'ä¸€é»', 'ä¸€äº›', 'ä¸€èˆ¬', 'ä¸€é‚Š', 'ä¸€æ—¦',
                'äºŒæ‰‹', 'äºŒæ¬¡', 'ä¸äºŒ', 'ååˆ†', 'ä¹æˆ', 'ä¸ƒåäºŒ', 'ä¸‰åå…­',
                'æ˜ŸæœŸä¸€', 'æ˜ŸæœŸäºŒ', 'æ˜ŸæœŸä¸‰', 'æ˜ŸæœŸå››', 'æ˜ŸæœŸäº”', 'æ˜ŸæœŸå…­', 'æ˜ŸæœŸæ—¥',
                'ç¬¬ä¸‰', 'ç¬¬å››', 'ç¬¬äº”', 'ç¬¬å…­', 'ç¬¬ä¸ƒ', 'ç¬¬å…«', 'ç¬¬ä¹', 'ç¬¬å'
            ]
            
            # Create protection: temporarily replace excluded words with placeholders
            protected = {}
            for i, word in enumerate(exclude_words):
                if word in text:
                    placeholder = f"__PROTECTED_{i}__"
                    text = text.replace(word, placeholder)
                    protected[placeholder] = word
            
            # Convert Chinese to Arabic (basic patterns)
            # Map Chinese digits
            digit_map = {'é›¶': 0, 'ä¸€': 1, 'äºŒ': 2, 'ä¸‰': 3, 'å››': 4,
                        'äº”': 5, 'å…­': 6, 'ä¸ƒ': 7, 'å…«': 8, 'ä¹': 9}
            
            def convert_chinese_number(match):
                num_str = match.group(0)
                
                # Handle tens (å, äºŒå, ä¹åä¹, etc.)
                if 'å' in num_str:
                    if num_str == 'å':
                        return '10'
                    elif num_str[0] == 'å':  # åä¸€, åäºŒ...
                        ones = digit_map.get(num_str[1], 0) if len(num_str) > 1 else 0
                        return str(10 + ones)
                    else:  # äºŒå, ä¹åä¹...
                        tens = digit_map.get(num_str[0], 0)
                        ones = digit_map.get(num_str[2], 0) if len(num_str) > 2 else 0
                        return str(tens * 10 + ones)
                
                # Handle simple digit sequences ONLY if it's a standalone number
                # Single character numbers like "ä¸€" "äºŒ" should NOT be converted
                # unless they are truly numeric (e.g. "ä¸€æœˆ" -> keep, but standalone "ä¸€" in number context -> convert)
                if len(num_str) == 1:
                    # Single digit: likely part of a word, don't convert
                    return num_str
                    
                result = ''
                for char in num_str:
                    if char in digit_map:
                        result += str(digit_map[char])
                return result if result else num_str
            
            # Match Chinese number patterns (at least 2 consecutive digits or with å)
            pattern = r'[é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹]{2,}|[é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]å[é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹]?'
            text = re.sub(pattern, convert_chinese_number, text)
            
            # Restore protected words
            for placeholder, original in protected.items():
                text = text.replace(placeholder, original)
        
        return text

    # ... (other methods) ...

    def _process_english(self, text: str, mode: str) -> str:
        """Process English text with smart AI translation."""
        logger.debug(f"_process_english called: mode={mode}, text='{text[:50]}'")
        
        if mode == 'keep':
            return text
            
        # Detect English words
        if not re.search(r'[a-zA-Z]', text):
            logger.debug("No English detected, returning original")
            return text
        
        logger.info(f"English detected in '{text[:30]}...', mode={mode}")
        
        if mode == 'translate':
            # Smart translation: extract English portions and translate them
            return self._smart_translate_english(text)
            
        elif mode == 'bilingual':
            # Bilingual: Show both original and translated
            translated = self._smart_translate_english(text)
            if translated != text:
                return f"{text}\n{translated}"
            return text
            
        return text
    
    def _smart_translate_english(self, text: str) -> str:
        """
        Smart English translation for mixed Chinese-English text.
        Extracts English segments and translates them contextually.
        """
        # Pattern WITHOUT word boundaries (\b doesn't work with Chinese)
        # Match English words including hyphens and apostrophes
        english_pattern = re.compile(r"[a-zA-Z]+(?:[\s\-'][a-zA-Z]+)*", re.IGNORECASE)
        
        matches = list(english_pattern.finditer(text))
        
        if not matches:
            return text
        
        logger.info(f"Found {len(matches)} English segments in: '{text}'")
        
        # Build translation map first (avoid modifying while iterating)
        translations = {}  # {english_text: translation}
        
        for match in matches:
            english_text = match.group(0).strip()
            
            if not english_text or english_text in translations:
                continue
            
            cache_key = english_text.lower()
            
            # Check global cache first
            if cache_key in self.translation_cache:
                translations[english_text] = self.translation_cache[cache_key]
                logger.debug(f"Using cache: '{english_text}' -> '{translations[english_text]}'")
                continue
            
            translation = None
            
            # First, check dictionary for exact match
            if cache_key in self.english_map:
                translation = self.english_map[cache_key]
                logger.info(f"Dictionary exact: '{english_text}' -> '{translation}'")
            else:
                # Try word-by-word translation for multi-word phrases
                words = english_text.split()
                if len(words) > 1:
                    translated_words = []
                    all_found = True
                    for word in words:
                        word_clean = word.lower().strip("-'.,!?")
                        if word_clean in self.english_map:
                            translated_words.append(self.english_map[word_clean])
                        else:
                            all_found = False
                            break
                    
                    if all_found and translated_words:
                        translation = ''.join(translated_words)
                        logger.info(f"Dictionary word-by-word: '{english_text}' -> '{translation}'")
                
                # If still no translation, use AI translation
                if not translation:
                    logger.info(f"Not in dictionary, using AI: '{english_text}'")
                    translation = self._translate_with_ai(english_text)
                    
                    # Validate AI result - check for repetition/corruption
                    if translation:
                        # If translation contains repeated original text, it's corrupted
                        if english_text.lower() in translation.lower() and translation != english_text:
                            # Extract just the new part
                            translation = translation.replace(english_text, '').strip()
                            if not translation:
                                translation = english_text
                                logger.warning(f"AI returned corrupted result, keeping: '{english_text}'")
                    
                    # If AI also fails, keep original
                    if not translation or translation == english_text:
                        translation = english_text
                        logger.warning(f"AI translation failed, keeping: '{english_text}'")
            
            translations[english_text] = translation
            self.translation_cache[cache_key] = translation
        
        # Now replace all occurrences
        result = text
        for english_text, translation in translations.items():
            if english_text != translation:
                result = result.replace(english_text, translation)
                logger.debug(f"Replaced: '{english_text}' -> '{translation}'")
        
        if result != text:
            logger.info(f"Final: '{text}' -> '{result}'")
        return result
        return result

    def _should_use_ai_translation(self, text: str) -> bool:
        """Check if we should use AI translation (e.g. sentence vs single word)."""
        # If text has more than 2 words, use AI
        return len(text.split()) > 2

    def _translate_with_ai(self, text: str) -> str:
        """
        Hybrid translation strategy: Dictionary â†’ Qwen LLM â†’ MarianMT.
        
        Args:
            text: English text to translate
            
        Returns:
            Translated Chinese text, or original if translation fails
        """
        # === LAYER 1: Dictionary (fastest, 100% accurate) ===
        dict_result = self._dictionary_translate(text)
        if dict_result != text:
            logger.info(f"[Dictionary] '{text}' -> '{dict_result}'")
            return dict_result
        
        # === LAYER 2: Qwen LLM (smart, context-aware) ===
        # Only use if already loaded (from Cantonese correction stage)
        if self.llm_processor is not None and self.llm_processor._model_a is not None:
            try:
                prompt = f"""å°‡ä»¥ä¸‹è‹±æ–‡ç¿»è­¯æˆç¹é«”ä¸­æ–‡ã€‚åªè¼¸å‡ºç¿»è­¯çµæœï¼Œä¸è¦è§£é‡‹ã€‚

è‹±æ–‡ï¼š{text}
ç¹é«”ä¸­æ–‡ï¼š"""
                
                result = self.llm_processor._generate(
                    prompt,
                    self.llm_processor._model_a,
                    self.llm_processor._tokenizer_a,
                    max_new_tokens=128,
                    temperature=0  # Deterministic output
                )
                
                # Clean up result
                if result:
                    result = result.strip()
                    # Remove common prefixes
                    for prefix in ['ç¹é«”ä¸­æ–‡ï¼š', 'ç¿»è­¯ï¼š', 'çµæœï¼š']:
                        if result.startswith(prefix):
                            result = result[len(prefix):].strip()

                    if result and result != text:
                        # Ensure Traditional Chinese output (in case LLM outputs Simplified)
                        if self.s2t_converter:
                            result = self.s2t_converter.convert(result)

                        logger.info(f"[Qwen LLM] '{text}' -> '{result}'")
                        # Cache for future use
                        self.translation_cache[text.lower()] = result
                        return result
                        
            except Exception as e:
                logger.warning(f"Qwen translation failed: {e}")
        
        # === LAYER 3: MarianMT (fallback) ===
        try:
            if not self.translation_model:
                logger.info("Initializing MarianMT Translation Model...")
                from models.translation_model import TranslationModel
                self.translation_model = TranslationModel(self.config)

            result = self.translation_model.translate(text)

            if result and result.strip() and result != text:
                # MarianMT outputs Simplified Chinese, convert to Traditional immediately
                if self.s2t_converter:
                    result = self.s2t_converter.convert(result)
                    logger.info(f"[MarianMT+S2T] '{text}' -> '{result}'")
                else:
                    logger.warning(f"[MarianMT] OpenCC not available, output may be Simplified: '{text}' -> '{result}'")

                # Cache for future use
                self.translation_cache[text.lower()] = result
                return result
            else:
                logger.warning(f"MarianMT returned empty or same, keeping original")
                return text

        except Exception as e:
            logger.error(f"MarianMT failed: {e}", exc_info=True)
            return text
    
    def _dictionary_translate(self, text: str) -> str:
        """
        Fallback dictionary translation.
        
        Args:
            text: Text to translate
            
        Returns:
            Translated text using dictionary
        """
        # Try word-by-word translation
        words = text.split()
        translated_words = []
        
        for word in words:
            word_lower = word.lower().strip('.,!?;:')
            if word_lower in self.english_map:
                translated_words.append(self.english_map[word_lower])
            else:
                translated_words.append(word)  # Keep original if not in dictionary
        
        result = ' '.join(translated_words)
        
        # If no translation happened, return original
        return result if result != text else text

    def _split_long_lines(self, segments: List[Dict]) -> List[Dict]:
        """Split long lines into two segments."""
        new_segments = []
        MAX_LEN = 25  # Threshold for splitting
        
        for seg in segments:
            text = seg['text']
            if len(text) > MAX_LEN:
                # Simple split at middle
                mid = len(text) // 2
                # Try to find punctuation near middle
                split_idx = mid
                
                # Search for punctuation
                punctuations = 'ï¼Œã€‚ï¼ï¼Ÿã€,.;?! '
                best_dist = float('inf')
                
                for i, char in enumerate(text):
                    if char in punctuations:
                        dist = abs(i - mid)
                        if dist < best_dist:
                            best_dist = dist
                            split_idx = i + 1
                
                # Split
                part1 = text[:split_idx].strip()
                part2 = text[split_idx:].strip()
                
                duration = seg['end'] - seg['start']
                mid_time = seg['start'] + (len(part1) / len(text)) * duration
                
                if part1:
                    new_segments.append({
                        'start': seg['start'],
                        'end': mid_time,
                        'text': part1,
                        'words': [] # Lost word info for now
                    })
                if part2:
                    new_segments.append({
                        'start': mid_time,
                        'end': seg['end'],
                        'text': part2,
                        'words': []
                    })
            else:
                new_segments.append(seg)
                
        return new_segments
