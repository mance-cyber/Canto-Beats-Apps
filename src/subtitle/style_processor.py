"""
Style Processor for subtitle text transformation.
"""

import json
import re
from pathlib import Path
from typing import List, Dict, Optional

from utils.logger import setup_logger
# NOTE: TranslationModel is NOT imported here to avoid triggering
# full transformers loading at startup (causes torchcodec issues in PyInstaller).
# Import it lazily in _translate_with_ai() when needed.
from core.config import Config

# OpenCC for simplified to traditional conversion
try:
    from opencc import OpenCC
    HAS_OPENCC = True
except ImportError:
    HAS_OPENCC = False

logger = setup_logger()

class StyleProcessor:
    """
    Process subtitle text based on style options.
    """
    
    def __init__(self, config: Optional[Config] = None):
        self.config = config or Config() # Fallback for tests
        self.cantonese_map = {}
        self.profanity_map = {}
        self.english_map = {}  # Initialize to prevent AttributeError
        self.translation_model = None
        self.llm_processor = None  # For AI-powered style conversion
        self.translation_cache = {}  # Cache for English translations: {english: chinese}
        
        # Initialize OpenCC for S2T conversion
        if HAS_OPENCC:
            self.s2t_converter = OpenCC('s2hk')  # Simplified to Traditional (Hong Kong)
            logger.info("OpenCC S2HK converter initialized")
        else:
            self.s2t_converter = None
            logger.warning("âš ï¸  OpenCC not available - AI translations may output Simplified Chinese!")
            logger.warning("    Install with: pip install opencc-python-reimplemented")
        
        self._load_resources()
        
    def _load_resources(self):
        """Load mapping resources - uses get_resource_path for PyInstaller compatibility"""
        from core.path_setup import get_resource_path
        
        try:
            # Use get_resource_path for PyInstaller compatibility
            # This correctly resolves paths in both development and packaged app
            
            # Load Cantonese mapping
            canto_path = get_resource_path('resources/cantonese_mapping.json')
            if Path(canto_path).exists():
                with open(canto_path, 'r', encoding='utf-8') as f:
                    self.cantonese_map = json.load(f)
            else:
                logger.warning(f"Cantonese mapping not found at: {canto_path}")
            
            # Load Profanity mapping
            prof_path = get_resource_path('resources/profanity_mapping.json')
            if Path(prof_path).exists():
                with open(prof_path, 'r', encoding='utf-8') as f:
                    self.profanity_map = json.load(f)
            else:
                logger.warning(f"Profanity mapping not found at: {prof_path}")

            # Load English mapping
            eng_path = get_resource_path('resources/english_mapping.json')
            if Path(eng_path).exists():
                with open(eng_path, 'r', encoding='utf-8') as f:
                    self.english_map = json.load(f)
            else:
                self.english_map = {}
                logger.warning(f"English mapping not found at: {eng_path}")
                    
            logger.info(f"Loaded resources: Canto={len(self.cantonese_map)}, Prof={len(self.profanity_map)}, Eng={len(self.english_map)}")
            
        except Exception as e:
            logger.error(f"Failed to load resources: {e}")

    def process(self, segments: List[Dict], options: Dict, progress_callback=None) -> List[Dict]:
        """
        Main processing method - applies all style transformations.
        
        Args:
            segments: List of subtitle segments with 'start', 'end', 'text'
            options: Dict with style options from StyleControlPanel
            progress_callback: Optional callback function(current, total, message)
            
        Returns:
            Processed segments
        """
        if not segments:
            return []
            
        logger.info(f"Processing {len(segments)} segments with options: {options}")
        
        result_segments = []
        changes_made = 0
        
        # Batch AI processing (5 sentences at a time for speed)
        style = options.get('style', 'spoken')
        # AI æ ¡æ­£ï¼šæ›¸é¢èªå’ŒåŠæ›¸é¢è‡ªå‹•å•Ÿç”¨ AIï¼ˆç„¡éœ€é¡å¤–å‹¾é¸ï¼‰
        use_ai = style in ('semi', 'written')
        
        ai_converted_texts = {}  # index -> converted text
        if use_ai and style in ('semi', 'written'):
            result = self._batch_ai_convert(segments, style, progress_callback)
            if result is not None:
                ai_converted_texts = result
            else:
                logger.warning("_batch_ai_convert returned None, using dictionary fallback")
        
        for i, seg in enumerate(segments):
            original_text = seg.get('text', '')
            text = original_text

            # 1. Convert Cantonese style (use batch result if available)
            if use_ai and style in ('semi', 'written'):
                # When using AI mode, skip segments not in ai_converted_texts
                # (they were removed as duplicates or failed processing)
                if i not in ai_converted_texts:
                    logger.debug(f"Skipping segment {i} (removed as duplicate or failed AI processing)")
                    continue
                text = ai_converted_texts[i]
                # Homophone corrections already applied in _batch_ai_convert preprocessing
            elif style != 'spoken':
                text = self._convert_cantonese_dict(text, style)  # Dictionary fallback for semi/written
                # Apply homophone corrections for dictionary-only mode
                text = self._apply_homophone_corrections(text, style)
            else:
                # Spoken mode: apply homophone corrections
                text = self._apply_homophone_corrections(text, style)
            
            # 2. Handle English
            eng_mode = options.get('english', 'keep')
            text = self._process_english(text, eng_mode)
            
            # 3. Format numbers
            num_mode = options.get('numbers', 'arabic')
            text = self._format_numbers(text, num_mode)
            
            # 4. Filter profanity
            prof_mode = options.get('profanity', 'keep')
            text = self._filter_profanity(text, prof_mode)
            
            # 5. Handle punctuation based on user option
            punct_mode = options.get('punctuation', 'keep')
            if punct_mode == 'remove':
                text = self._remove_all_punctuation(text)
            else:
                # 5b. Remove trailing punctuation only (default behavior)
                text = self._remove_trailing_punctuation(text)
            
            # 6. Convert any simplified Chinese to Traditional (final step)
            if self.s2t_converter:
                text = self.s2t_converter.convert(text)
            
            # Log changes for debugging
            if text != original_text:
                changes_made += 1
                if i < 5:  # Only log first 5 changes to avoid spam
                    logger.info(f"[StyleProcessor] Seg {i}: '{original_text[:30]}' -> '{text[:30]}'")
            
            # Create new segment with processed text
            new_seg = {
                'start': seg.get('start', 0),
                'end': seg.get('end', 0),
                'text': text,
                'words': seg.get('words', [])
            }
            result_segments.append(new_seg)
        
        # 6. Split long lines if requested
        if options.get('split_long', False):
            max_chars = options.get('max_chars', 14)  # Get from options
            result_segments = self._split_long_lines(result_segments, max_chars)
        
        logger.info(f"Processing complete: {len(result_segments)} segments, {changes_made} changes made")
        return result_segments
    
    def _remove_trailing_punctuation(self, text: str) -> str:
        """
        Remove trailing punctuation from subtitle text.
        
        Args:
            text: Input text
            
        Returns:
            Text with trailing punctuation removed
        """
        if not text:
            return text
        
        # é¦–å…ˆæ¸…ç†æ‰€æœ‰é¡å‹å˜…æ‹¬è™Ÿï¼ˆWhisper ç¶“å¸¸ç”¢ç”Ÿï¼‰
        brackets_to_remove = '()ï¼ˆï¼‰ï¹™ï¹š[]ã€ã€‘ã€Œã€'
        for bracket in brackets_to_remove:
            text = text.replace(bracket, '')
        
        # Define punctuation to remove (Chinese and English)
        trailing_punct = 'ã€‚ï¼Œï¼ï¼Ÿï¼›ï¼šã€.!?,;:'
        
        # Remove trailing punctuation
        while text and text[-1] in trailing_punct:
            text = text[:-1]
        
        return text

    def _apply_homophone_corrections(self, text: str, style: str = 'spoken') -> str:
        """
        Apply homophone corrections to fix common Whisper transcription errors.
        Style-aware: some corrections only apply in spoken mode.
        
        Args:
            text: Input text with potential homophone errors
            style: 'spoken', 'semi', or 'written'
            
        Returns:
            Text with homophone errors corrected
        """
        if not text:
            return text
        
        # Universal homophone fixes (apply in ALL modes)
        universal_fixes = {
            # Common transcription errors
            "åŸè²»": "æœˆè²»",
            "è²¡æºåå–®": "è£å“¡åå–®",
            "è²¡æº": "è£å“¡",
            "åšæ®º": "ææ®º",
            "ç¦å…¥å»": "æ’³å…¥å»",
            "å‹èª¼": "çŒ¶è±«",
            "å‘Šå": "å ±å",
            "è¬›æ¿ƒ": "è¬›NO",
            "æ¿ƒå˜…åº•æ°£": "NOå˜…åº•æ°£",
            "æ¿ƒåšçš„åº•æ°£": "NOçš„åº•æ°£",  # æ›¸é¢èªç‰ˆæœ¬
            "æ¿ƒçš„åº•æ°£": "NOçš„åº•æ°£",
            "åŠä»½é‡": "åŠä»½ç³§",
            "ç¬¬äºŒä»½é‡": "ç¬¬äºŒä»½ç³§",
            "ä»½é‡": "ä»½ç³§",  # é€šç”¨ä¿®æ­£
            "aå•²ul": "Aesop",
            "powå•²er": "powder",
            "olivian": "Olive Young",
            "iPaå•²": "iPad",
            "ä¸çŸ¥é“é“": "ä¸çŸ¥é“",
            "çŸ¥é“é“": "çŸ¥é“",
            "ç„¡äºº": "ç„¡å°",  # Context: ç„¡å°è‰¯å“
            "ä¸çœ‹ä¸çœ‹": "å””æ¨å””æ¨",
            "å…³é—­ç”µè¯": "è§£åƒ±",  # call off
            "call off": "å–æ¶ˆ",  # è‹±æ–‡ç›´è­¯éŒ¯èª¤
            "ä¿¾äººcall off": "è¢«å–æ¶ˆ",
            "æˆ‘æ‰æ˜¯": "æˆ‘æ˜¯",  # "æˆ‘æ‰æ˜¯Linxia" æ‡‰è©²æ˜¯ "æˆ‘æ˜¯Linxia"
            "å³åˆ»å‘Šå": "å³åˆ»å ±å",
            "ç«‹åˆ»ç¦å…¥": "ç«‹åˆ»æ’³å…¥",
            "æ’³å…¥å»": "é»å…¥å»",  # æ­£ç¢ºç²µèªç”¨æ³•

            # å¾é•·å½±ç‰‡åˆ†æç™¼ç¾çš„éŒ¯èª¤
            "è™•æ–¼ä¸€ç¨®": "é™·å…¥ä¸€å€‹",  # ã€Œè™•æ–¼ä¸€ç¨®å…©é›£ã€â†’ã€Œé™·å…¥ä¸€å€‹å…©é›£ã€
            "é‚£å¡Šé¢": "å—°å¡Šé¢",  # æ›¸é¢èªéŒ¯èª¤
            "é‚£ç›ç‡ˆ": "å—°ç›ç‡ˆ",
            "é‚£å€‹": "å—°å€‹",  # é™¤éå·²ç¶“æ˜¯æ›¸é¢èªæ¨¡å¼
            "é€™æ¨£åš": "å’æ¨£åš",
            "é€™æ¨£çš„": "å’æ¨£å˜…",
            "è¡å‹•æ¶ˆè²»": "è¡å‹•æ¶ˆè²»",  # ä¿æŒæ­£ç¢º
            "æ·¡å‰”": "æ¸…æ·¡",  # ç²µèªâ†’æ›¸é¢èª

            # Simplified to Traditional Chinese fixes
            "çµ": "éˆ",
            "è¿ˆå…‹å°”": "Michael",
            "ä¸€ä¸ª": "ä¸€å€‹",
            "è¿™ä¸ª": "é€™å€‹",
            "è§‰å¾—": "è¦ºå¾—",
        }
        
        # Spoken-mode-only fixes (æ›¸é¢èªâ†’ç²µèªå£èª)
        # These should NOT apply in written mode
        spoken_only_fixes = {
            "å“ªä¸€å€‹": "é‚Šä¸€å€‹",  # Whisper outputs æ›¸é¢èª, fix to å£èª
        }
        
        original = text
        
        # Apply universal fixes
        for wrong, correct in universal_fixes.items():
            if wrong in text:
                text = text.replace(wrong, correct)
        
        # Apply spoken-only fixes ONLY in spoken mode
        if style == 'spoken':
            for wrong, correct in spoken_only_fixes.items():
                if wrong in text:
                    text = text.replace(wrong, correct)
        
        if text != original:
            logger.debug(f"[Homophone] Fixed: '{original[:30]}...' -> '{text[:30]}...'")
        
        return text

    def _remove_all_punctuation(self, text: str) -> str:
        """
        Remove all punctuation marks from subtitle text.
        
        Args:
            text: Input text
            
        Returns:
            Text with all punctuation removed
        """
        if not text:
            return text
        
        original_text = text
        
        # All punctuation to remove (Chinese and English)
        all_punct = 'ï¼Œã€‚ï¼ï¼Ÿã€ï¼›ï¼š,.!?;:ï¼›â€”â€¦ã€Œã€ã€ã€ã€ã€‘ã€Šã€‹ã€ˆã€‰()ï¼ˆï¼‰ï¹™ï¹š[]'
        
        for p in all_punct:
            text = text.replace(p, '')
        
        if text != original_text:
            logger.info(f"[PUNCT] Removed punctuation: '{original_text[:30]}...' -> '{text[:30]}...'")
        
        return text

    def _batch_ai_convert(self, segments: List[Dict], style: str, progress_callback=None) -> Dict[int, str]:
        """
        Batch convert segments using AI.
        Priority: MLX Qwen (Apple Silicon) > Transformers Qwen (fallback)
        Returns dict of {index: converted_text}.
        """
        result = {}
        batch_size = 3  # Reduced from 5 for more reliable AI processing

        # DEBUG: Log the style value
        logger.info(f"ğŸ¨ [STYLE DEBUG] _batch_ai_convert called with style='{style}'")

        # ========================================
        # ã€å‰è™•ç†ã€‘åœ¨ AI è½‰æ›ä¹‹å‰ï¼Œå…ˆä¿®æ­£åŒéŸ³å­—éŒ¯èª¤
        # ========================================
        preprocessed_segments = []
        for seg in segments:
            text = seg.get('text', '')
            # æ‡‰ç”¨åŒéŸ³å­—ä¿®æ­£ï¼ˆspoken modeï¼Œå› ç‚ºæ­¤æ™‚é‚„æ˜¯å£èªï¼‰
            text = self._apply_homophone_corrections(text, style='spoken')
            preprocessed_seg = seg.copy()
            preprocessed_seg['text'] = text
            preprocessed_segments.append(preprocessed_seg)

        # ä½¿ç”¨é è™•ç†å¾Œçš„ segments
        segments = preprocessed_segments
        
        # Initialize LLM if needed - auto-detect best backend
        if self.llm_processor is None:
            try:
                import gc
                import torch
                
                # Clear GPU memory before loading model
                logger.info("Clearing GPU memory before loading Qwen model...")
                gc.collect()
                if torch.backends.mps.is_available():
                    torch.mps.empty_cache()
                    logger.info("MPS memory cleared")
                elif torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    logger.info("CUDA memory cleared")
                
                # Auto-detect hardware and get best backend
                from utils.qwen_mlx import get_qwen_for_hardware, MLXQwenLLM
                
                hw_config = get_qwen_for_hardware()
                logger.info(f"ğŸ” Hardware detection: {hw_config['description']}")
                
                if hw_config['backend'] == 'mlx':
                    # Use MLX Qwen (Apple Silicon optimized)
                    from utils.qwen_mlx import MLXQwenLLM
                    from huggingface_hub import try_to_load_from_cache
                    
                    model_id = hw_config['model_id']
                    
                    # Check if model is cached
                    cache_result = try_to_load_from_cache(model_id, "config.json")
                    model_cached = cache_result is not None
                    
                    if not model_cached:
                        # æ¨¡å‹æœªç¼“å­˜ - åº”è¯¥å·²åœ¨ä¸»çº¿ç¨‹é¢„å…ˆä¸‹è½½ï¼ˆmain_window._ensure_llm_model_readyï¼‰
                        # å¦‚æœèµ°åˆ°è¿™é‡Œè¯´æ˜ä¸»çº¿ç¨‹çš„æ£€æŸ¥è¢«è·³è¿‡äº†
                        logger.warning("MLX Qwen model not cached - should be downloaded from main thread")
                        logger.warning("Falling back to dictionary mode (UI dialogs cannot be shown from worker thread)")
                        return result

                    # æ¨¡å‹å·²ç¼“å­˜ï¼Œç›´æ¥åŠ è½½
                    try:
                        self.llm_processor = MLXQwenLLM(model_id=model_id)
                        self.llm_processor.load_model()
                        self._using_mlx = True
                        logger.info(f"âš¡ {hw_config['description']} loaded")
                    except Exception as load_error:
                        logger.error(f"Failed to load MLX Qwen model: {load_error}", exc_info=True)
                        # ä¸æ˜¾ç¤º UIï¼Œç›´æ¥è¿”å›ï¼ˆè®©è°ƒç”¨è€…ä½¿ç”¨å­—å…¸æ¨¡å¼ï¼‰
                        logger.warning("Falling back to dictionary mode due to model load failure")
                        return result
                else:
                    # Use Transformers Qwen (MPS/CUDA/CPU)
                    self._using_mlx = False
                    from ui.download_dialog import check_and_download_model
                    from core.hardware_detector import HardwareDetector
                    from models.qwen_llm import QwenLLM
                    
                    detector = HardwareDetector()
                    profile = detector.detect()
                    
                    model_ready = check_and_download_model(
                        profile.llm_a_model,
                        profile.llm_a_quantization,
                        parent=None
                    )
                    
                    if not model_ready:
                        logger.warning("Model not ready, using dictionary")
                        return result
                    
                    self.llm_processor = QwenLLM(self.config, profile)
                    self.llm_processor.load_models()
                    logger.info(f"âœ… {hw_config['description']} loaded")
                    
            except Exception as e:
                logger.warning(f"LLM init failed: {e}")
                return result
        
        # CRITICAL CHECK: If LLM failed to initialize, don't proceed with AI conversion
        if self.llm_processor is None:
            logger.warning("LLM processor is None, cannot perform AI conversion - using dictionary fallback")
            return result
        
        # Process in batches with sliding window context
        total_batches = (len(segments) + batch_size - 1) // batch_size
        context_window = 2  # å‰å¾Œå„æä¾› 2 å¥ä½œç‚ºä¸Šä¸‹æ–‡

        for batch_idx, batch_start in enumerate(range(0, len(segments), batch_size)):
            batch_end = min(batch_start + batch_size, len(segments))
            batch_texts = [segments[i].get('text', '') for i in range(batch_start, batch_end)]

            # Report progress
            if progress_callback:
                progress_callback(batch_idx, total_batches, f"AI è½‰æ› {batch_idx + 1}/{total_batches}...")

            # ========================================
            # ã€æ»‘å‹•çª—å£ã€‘æä¾›å‰å¾Œä¸Šä¸‹æ–‡çµ¦ LLM
            # ========================================
            # å‰æ–‡ (preceding context)
            context_before = []
            context_before_start = max(0, batch_start - context_window)
            if context_before_start < batch_start:
                context_before = [segments[i].get('text', '') for i in range(context_before_start, batch_start)]

            # å¾Œæ–‡ (following context)
            context_after = []
            context_after_end = min(len(segments), batch_end + context_window)
            if batch_end < context_after_end:
                context_after = [segments[i].get('text', '') for i in range(batch_end, context_after_end)]

            # Combine texts with markers
            combined = "\n".join([f"{i+1}. {t}" for i, t in enumerate(batch_texts)])

            # æ§‹å»ºå®Œæ•´ä¸Šä¸‹æ–‡ï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
            context_prompt = ""
            if context_before:
                context_before_text = "\n".join([f"  {t}" for t in context_before])
                context_prompt += f"\nã€å‰æ–‡ä¸Šä¸‹æ–‡ã€‘ï¼ˆåƒ…ä¾›åƒè€ƒï¼Œä¸è¦è½‰æ›ï¼‰\n{context_before_text}\n"

            if context_after:
                context_after_text = "\n".join([f"  {t}" for t in context_after])
                context_prompt += f"\nã€å¾ŒçºŒå…§å®¹ã€‘ï¼ˆåƒ…ä¾›åƒè€ƒï¼Œä¸è¦è½‰æ›ï¼‰\n{context_after_text}\n"

            # Different prompts for semi vs written style
            if style == 'semi':
                # åŠæ›¸é¢èªï¼šè½‰æ›éƒ¨åˆ†ç²µèªå­—ï¼Œä½†ä¿ç•™æœ€æ ¸å¿ƒå˜…ç²µèªç‰¹è‰²
                prompt = f"""ä½ æ˜¯ä¸€ä½å°ˆæ¥­ç²µèªå­—å¹•ç·¨è¼¯ã€‚ä»»å‹™æ˜¯å°‡å£èªå­—å¹•è½‰æˆã€ŒåŠæ›¸é¢èªã€é¢¨æ ¼ â€” ä»‹ä¹ç´”å£èªåŒç´”æ›¸é¢ä¹‹é–“ã€‚

ã€æ ¸å¿ƒç›®æ¨™ã€‘
- éƒ¨åˆ†è½‰æ›ï¼šå°‡å¸¸è¦‹ç²µèªå­—è½‰æˆæ›¸é¢èªï¼Œä½†ä¿ç•™ç²µèªå˜…æ ¸å¿ƒç‰¹è‰²
- ç§»é™¤èªæ°£è©ï¼šå›‰ã€å–ã€å˜ã€ã—ã€å“ã€å–‡ ç­‰éåº¦å£èªåŒ–å˜…èªæ°£è©
- è‹±æ–‡å¿…é ˆä¿ç•™ï¼šæ‰€æœ‰è‹±æ–‡å–®è©ä¸€å¾‹ä¿æŒåŸæ¨£
- é˜¿æ‹‰ä¼¯æ•¸å­—ä¿ç•™ï¼š5è¬ã€10è¬ã€2000 ä¿æŒåŸæ¨£
- **ğŸ”¥ åš´æ ¼ä¿æŒæ–·å¥é‚Šç•Œ**ï¼šæ¯ä¸€è¡Œè¼¸å…¥å°æ‡‰ä¸€è¡Œè¼¸å‡ºï¼Œä¸è¦åˆä½µæˆ–æ‹†åˆ†å¥å­

ã€éœ€è¦è½‰æ›ã€‘
ä¿‚â†’æ˜¯ã€å–ºâ†’åœ¨ã€ä½¢â†’ä»–/å¥¹ã€ä½¢å“‹â†’ä»–å€‘ã€åšŸâ†’ä¾†ã€å’—â†’äº†
é‚Šåº¦â†’å“ªè£¡ã€é»è§£â†’ç‚ºä»€éº¼ã€ä¹œå˜¢â†’ä»€éº¼ã€å‘¢å€‹â†’é€™å€‹ã€å—°å€‹â†’é‚£å€‹
ä»Šæ—¥â†’ä»Šå¤©ã€è½æ—¥â†’æ˜å¤©ã€ç´æ—¥/å°‹æ—¥â†’æ˜¨å¤©ã€è€Œå®¶â†’ç¾åœ¨

ã€å¿…é ˆä¿ç•™ - ç²µèªæ ¸å¿ƒå­—ã€‘
å˜…ï¼ˆä¿ç•™ï¼Œå””å¥½è®Šæˆã€Œçš„ã€ï¼‰ã€å””ï¼ˆä¿ç•™ï¼Œå””å¥½è®Šæˆã€Œä¸ã€ï¼‰ã€å†‡ï¼ˆä¿ç•™ï¼Œå””å¥½è®Šæˆã€Œæ²’æœ‰ã€ï¼‰
å•²ï¼ˆä¿ç•™ï¼‰ã€å’ï¼ˆä¿ç•™ï¼‰ã€ç‡ï¼ˆä¿ç•™ï¼‰ã€éšï¼ˆä¿ç•™ï¼‰
{context_prompt}
ã€éœ€è¦è½‰æ›çš„å…§å®¹ã€‘
{combined}

ã€è¼¸å‡ºã€‘ï¼ˆåªè¼¸å‡ºè½‰æ›çµæœï¼Œä¿æŒç·¨è™Ÿ 1ã€2ã€3...ï¼‰"""
            else:
                # æ›¸é¢èªï¼šå¾¹åº•è½‰æ›æ‰€æœ‰ç²µèªå­—
                prompt = f"""ä½ æ˜¯ä¸€ä½å°ˆæ¥­ä¸­æ–‡ç·¨è¼¯èˆ‡å­—å¹•è½‰å¯«å¸«ã€‚ä½ çš„ä»»å‹™æ˜¯æŠŠã€Œç²µèªå£èªå­—å¹•ã€å¾¹åº•è½‰è­¯æˆã€Œè‡ªç„¶æµæš¢çš„æ›¸é¢ä¸­æ–‡ã€ã€‚

ã€ğŸ”¥ è¦–è§’è¨­å®š â€” çµ•å°ä¸å¯é•åã€‘
- èªªè©±è€…ï¼šä¸€ä½ YouTuberï¼ˆç¬¬ä¸€äººç¨±ï¼‰ï¼Œæ­£åœ¨æè¿°è‡ªå·±çš„æ—¥å¸¸ç”Ÿæ´»
- æ‰€æœ‰ã€Œæˆ‘å“‹ã€â†’ã€Œæˆ‘å€‘ã€ã€ã€Œæˆ‘ã€ä¿æŒã€Œæˆ‘ã€
- **çµ•å°ç¦æ­¢**ï¼šæŠŠã€Œæˆ‘ã€æ”¹æˆã€Œä»–/å¥¹/ä½ ã€ã€æŠŠã€Œæˆ‘å€‘ã€æ”¹æˆã€Œä»–å€‘ã€
- å¦‚æœå¥å­æ˜¯ã€Œæˆ‘å»äº†...ã€ï¼Œè½‰æ›å¾Œå¿…é ˆä»æ˜¯ã€Œæˆ‘å»äº†...ã€ï¼Œä¸èƒ½è®Šæˆã€Œä»–å»äº†...ã€

ã€çµ•å°è¦æ±‚ã€‘
- **å¿…é ˆä½¿ç”¨ç¹é«”ä¸­æ–‡**ï¼šè¼¸å‡ºå¿…é ˆæ˜¯ç¹é«”å­—ï¼ˆTraditional Chineseï¼‰ï¼Œçµ•å°ä¸å¯ä»¥ä½¿ç”¨ç°¡é«”å­—ï¼ˆSimplified Chineseï¼‰ã€‚
- ç¹é«”å­—ç¯„ä¾‹ï¼šå„˜ç®¡ã€è¦ºå¾—ã€å˜—è©¦ã€é©—è­‰ã€è¨Šè™Ÿã€ç™¼ç¾ã€å•é¡Œã€æ‡‰è©²ã€é€™å€‹ã€ä¸€å€‹
- **ç°¡é«”å­—é»‘åå–®**ï¼ˆçµ•å°ç¦æ­¢ï¼‰ï¼šå°½ç®¡ã€è§‰å¾—ã€å°è¯•ã€éªŒè¯ã€è®¯å·ã€å‘ç°ã€é—®é¢˜ã€åº”è¯¥ã€è¿™ä¸ªã€ä¸€ä¸ª

ã€æ ¸å¿ƒç›®æ¨™ã€‘
- å®Œå…¨æ›¸é¢åŒ–ï¼šæŠŠå£èªã€ç²µèªèªæ°£è©ã€å£é ­ç¦ªã€æ½®èªæ”¹æˆæ­£å¼æ›¸é¢è¡¨é”ã€‚
- ä¸æ”¹æ„æ€ï¼šä¿ç•™åŸå¥è³‡è¨Šã€èªæ°£å¼·å¼±ï¼Œä½†ç”¨æ›¸é¢èªå‘ˆç¾ã€‚
- é©åˆåšå­—å¹•ï¼šå¥å­è¦ç°¡æ½”ã€æ˜“è®€ã€è‡ªç„¶ã€‚
- **è‹±æ–‡å¿…é ˆä¿ç•™**ï¼šæ‰€æœ‰è‹±æ–‡å–®è©ã€å“ç‰Œã€äººåã€è¡“èªç­‰ï¼Œçµ•å°ä¸è¦ç¿»è­¯æˆä¸­æ–‡ã€‚

ã€è½‰è­¯è¦å‰‡ã€‘
1. **å®Œå…¨ç§»é™¤ç²µèªèªæ°£è©**ï¼šã€Œå–ã€å•¦ã€å›‰ã€å’©ã€ã—ã€å“ã€å‘€ã€å–‡ã€å•«ã€å˜›ã€ç­‰å¿…é ˆå®Œå…¨ç§»é™¤ï¼Œä¸è¦ä¿ç•™ã€‚
2. ç²—å£è™•ç†ï¼šæ”¹æˆè¼ƒæ–‡æ˜çš„åŒç­‰èªæ°£ï¼ˆä¾‹å¦‚ã€Œå¥½æ’šç…©ã€â†’ã€Œéå¸¸ç…©äººã€ï¼‰ã€‚
3. å¥æœ«æ¨™é»è¦æ›¸é¢ï¼šç–‘å•ç”¨ã€Œï¼Ÿã€ã€æ„Ÿå˜†ç”¨ã€Œï¼ã€ï¼Œå…¶é¤˜ç”¨ã€Œã€‚ã€æˆ–ã€Œï¼Œã€ã€‚
4. ä¸è¦æ·»åŠ æ–°è³‡è¨Šã€ä¸è¦è§£é‡‹ã€ä¸è¦è©•è«–ã€‚
5. åªè¼¸å‡ºè½‰è­¯å¾Œæ–‡å­—ï¼Œä¿æŒç·¨è™Ÿæ ¼å¼ã€‚
6. **çµ•å°ä¸å¯é‡è¤‡**ï¼šå¦‚æœå‰ä¸€å¥å·²ç¶“èªªéç›¸åŒå…§å®¹ï¼Œä¸è¦å†æ¬¡è¼¸å‡ºã€‚
7. **è‹±æ–‡å¿…é ˆä¿ç•™**ï¼šæ‰€æœ‰è‹±æ–‡å–®è©ã€äººåã€å“ç‰Œã€è¡“èªã€æ•¸å­—ç­‰ï¼Œä¸€å¾‹ä¿æŒåŸæ¨£ï¼Œçµ•å°ä¸è¦ç¿»è­¯æˆä¸­æ–‡ã€‚
8. **ğŸ”¥ åš´æ ¼ä¿æŒæ–·å¥é‚Šç•Œ**ï¼šæ¯ä¸€è¡Œè¼¸å…¥å°æ‡‰ä¸€è¡Œè¼¸å‡ºï¼Œçµ•å°ä¸è¦åˆä½µå¤šè¡Œæˆ–æ‹†åˆ†å–®è¡Œã€‚æ¯å€‹ç·¨è™Ÿçš„å…§å®¹å¿…é ˆå®Œæ•´è½‰è­¯ï¼Œä¸è¦æˆªæ–·å¥å­ã€‚

ã€å¸¸è¦‹è½‰æ› - å¿…é ˆå…¨éƒ¨åŸ·è¡Œã€‘
ä¿‚â†’æ˜¯ã€å–ºâ†’åœ¨ã€å””â†’ä¸ã€å†‡â†’æ²’æœ‰ã€å˜…â†’çš„ã€å’—â†’äº†ã€åšŸâ†’ä¾†ã€ä½¢â†’ä»–/å¥¹
å¥½å½©â†’å¹¸é‹ã€é ­å…ˆâ†’å‰›æ‰ã€ç´æ—¥â†’æ˜¨å¤©ã€è½æ—¥â†’æ˜å¤©ã€ä»Šæ—¥â†’ä»Šå¤©ã€è€Œå®¶â†’ç¾åœ¨
å€‹é˜â†’å°æ™‚ã€èšŠâ†’å…ƒã€å³ä¿‚â†’å°±æ˜¯ã€é»è§£â†’ç‚ºä»€éº¼ã€ä¹œå˜¢â†’ä»€éº¼ã€é‚Šåº¦â†’å“ªè£¡
ç‡â†’çœ‹ã€éšâ†’æ¼‚äº®ã€å•²â†’äº›ã€å’â†’é€™æ¨£ã€å””è©²â†’è¬è¬/è«‹
æ‹â†’æ‹¿ã€æ¾â†’æ‰¾ã€æ”â†’æ‹¿ã€ç•€â†’çµ¦ã€å±‹ä¼â†’å®¶ã€å¥½ä¼¼â†’å¥½åƒ

ã€ğŸ”¥ è‹±æ–‡/å°ˆæœ‰åè©ä¿ç•™ â€” çµ•å°ä¸å¯ç¿»è­¯ã€‘
- "Apple" ä¿æŒ "Apple"ï¼Œä¸è¦è®Šæˆã€Œè˜‹æœã€
- "iPhone" ä¿æŒ "iPhone"ï¼Œä¸è¦è®Šæˆã€Œæ„›ç˜‹ã€
- "CEO" ä¿æŒ "CEO"ï¼Œä¸è¦è®Šæˆã€ŒåŸ·è¡Œé•·ã€
- "AI" ä¿æŒ "AI"ï¼Œä¸è¦è®Šæˆã€Œäººå·¥æ™ºèƒ½ã€
- "flock" ä¿æŒ "flock"ï¼Œä¸è¦è®Šæˆã€Œç¾Šç¾¤ã€
- "freelance" ä¿æŒ "freelance"ï¼Œä¸è¦è®Šæˆã€Œè‡ªç”±è·æ¥­ã€
- "creator" ä¿æŒ "creator"ï¼Œä¸è¦è®Šæˆã€Œå‰µä½œè€…ã€
- "studio" ä¿æŒ "studio"ï¼Œä¸è¦è®Šæˆã€Œå·¥ä½œå®¤ã€æˆ–ã€Œå­¸ç”Ÿã€
- "color test" ä¿æŒ "color test"ï¼Œä¸è¦è®Šæˆã€Œè‰²å½©æ¸¬è©¦ã€
- **åŸå‰‡**ï¼šæ‰€æœ‰è‹±æ–‡å–®è©ã€å“ç‰Œåã€äººåã€å°ˆæ¥­è¡“èªä¸€å¾‹ä¿ç•™åŸæ–‡

ã€é‡è¦ã€‘æ•¸å­—ä¿ç•™ç¯„ä¾‹ï¼š
- "5è¬" ä¿æŒ "5è¬"ï¼Œä¸è¦è®Šæˆã€Œäº”è¬ã€
- "10è¬" ä¿æŒ "10è¬"ï¼Œä¸è¦è®Šæˆã€Œåè¬ã€
- "2000" ä¿æŒ "2000"ï¼Œä¸è¦è®Šæˆã€ŒäºŒåƒã€
- æ‰€æœ‰é˜¿æ‹‰ä¼¯æ•¸å­—ä¸€å¾‹ä¿æŒåŸæ¨£ï¼Œçµ•å°ä¸è¦è½‰æˆä¸­æ–‡æ•¸å­—

ã€é¢¨æ ¼ã€‘ç¹é«”ä¸­æ–‡æ›¸é¢èªï¼Œæ¸…æ™°è‡ªç„¶ã€‚åš´æ ¼åº¦ï¼šæœ€é«˜ï¼Œå‡¡æ˜¯å£èªåŒ–è¡¨é”ä¸€å¾‹æ”¹æˆæ›¸é¢èªã€‚
{context_prompt}
ã€éœ€è¦è½‰æ›çš„å…§å®¹ã€‘
{combined}

ã€è¼¸å‡ºã€‘ï¼ˆåªè¼¸å‡ºè½‰æ›çµæœï¼Œä¿æŒç·¨è™Ÿ 1ã€2ã€3...ï¼Œä¸è¦è¼¸å‡ºå‰æ–‡æˆ–å¾ŒçºŒå…§å®¹ï¼‰"""
            
            try:
                # Generate response - use correct method based on backend
                if getattr(self, '_using_mlx', False):
                    response = self.llm_processor.generate(
                        prompt,
                        max_tokens=1024,
                        temperature=0
                    )
                else:
                    # Fallback for non-MLX LLM (shouldn't happen with MLXQwenLLM)
                    response = self.llm_processor.generate(
                        prompt,
                        max_tokens=1024,
                        temperature=0
                    )
                
                # === DEBUG: Log raw AI response ===
                logger.info(f"=== RAW AI RESPONSE (Batch {batch_idx + 1}) ===")
                logger.info(response[:500] if len(response) > 500 else response)
                logger.info("=== END RAW RESPONSE ===")
                
                # Parse numbered response
                for line in response.strip().split('\n'):
                    line = line.strip()
                    if line and line[0].isdigit():
                        parts = line.split('.', 1)
                        if len(parts) == 2:
                            try:
                                num = int(parts[0]) - 1
                                text = parts[1].strip()
                                
                                # === åš´æ ¼æ¸…ç† AI è¼¸å‡º ===
                                # 0. ç§»é™¤ markdown å¼·èª¿æ¨™è¨˜ ** å’Œ *
                                text = text.replace('**', '')
                                # Remove single * only if it appears to be markdown (not multiplication)
                                text = re.sub(r'(?<![\d\s])\*(?![\d\s])', '', text)
                                
                                # 1. å¦‚æœæœ‰ç®­é ­ç¬¦è™Ÿï¼Œåªå–ç®­é ­å¾Œé¢å˜…å…§å®¹
                                if 'â†’' in text:
                                    text = text.split('â†’')[-1].strip()
                                if '->' in text:
                                    text = text.split('->')[-1].strip()
                                
                                # 2. ç§»é™¤æ‰€æœ‰é¡å‹æ‹¬è™Ÿ
                                for bracket in '()ï¼ˆï¼‰ï¹™ï¹š[]ã€ã€‘ã€Œã€':
                                    text = text.replace(bracket, '')
                                
                                # 3. æ¸…é™¤ç•°å¸¸å°¾éƒ¨å­—ç¬¦ï¼ˆä¸åŒ…æ‹¬ã€Œæ˜¯ã€å› ç‚ºæ˜¯æœ‰æ•ˆæ›¸é¢èªï¼‰
                                while text and text[-1] in ')ï¼‰ã€ã€‘å‘¢å•¦':
                                    text = text[:-1].strip()
                                
                                # 4. å»é™¤å¤šé¤˜ç©ºç™½
                                text = ' '.join(text.split())
                                
                                # âš ï¸ ã€é—œéµã€‘å¼·åˆ¶è½‰æ›ç‚ºç¹é«”ä¸­æ–‡ï¼ˆçµ•å°ç¦å¿Œç°¡é«”å­—ï¼‰
                                if self.s2t_converter:
                                    text = self.s2t_converter.convert(text)
                                    logger.debug(f"[S2T] Converted AI output to Traditional: '{text[:30]}'")
                                
                                if 0 <= num < len(batch_texts) and text:
                                    result[batch_start + num] = text
                            except ValueError:
                                pass
                
                # Log how many were successfully parsed
                parsed_count = sum(1 for i in range(batch_start, batch_end) if i in result)
                logger.info(f"Batch {batch_idx + 1}/{total_batches}: parsed {parsed_count}/{len(batch_texts)} segments")
                
                # If batch completely failed, retry once with smaller input
                if parsed_count == 0 and len(response.strip()) == 0:
                    logger.warning(f"Batch {batch_idx + 1} returned empty, retrying...")
                    # Retry with same batch
                    try:
                        response = self.llm_processor.generate(prompt, max_tokens=1024, temperature=0.1)
                        if response and response.strip():
                            logger.info(f"Retry succeeded for batch {batch_idx + 1}")
                            # Parse again
                            for line in response.strip().split('\n'):
                                line = line.strip()
                                if line and line[0].isdigit():
                                    parts = line.split('.', 1)
                                    if len(parts) == 2:
                                        try:
                                            num = int(parts[0]) - 1
                                            text = parts[1].strip()
                                            if 0 <= num < len(batch_texts) and text:
                                                result[batch_start + num] = text
                                        except ValueError:
                                            pass
                    except Exception as retry_e:
                        logger.warning(f"Retry also failed: {retry_e}")
                
            except Exception as e:
                logger.warning(f"Batch processing failed: {e}", exc_info=True)
        
        # Report completion
        if progress_callback:
            progress_callback(total_batches, total_batches, "AI è½‰æ›å®Œæˆï¼")
        
        # Post-process with dictionary to fix AI missed conversions
        # === SEMI MODE: è½‰æ›éƒ¨åˆ†ç²µèªå­—ï¼Œä¿ç•™æ ¸å¿ƒå­— ===
        post_fix_map_semi = {
            # åŠæ›¸é¢èªï¼šè½‰æ›å‘¢å•²å­—
            "ä¿‚": "æ˜¯",
            "å–º": "åœ¨",
            "ä½¢": "ä»–",
            "ä½¢å“‹": "ä»–å€‘",
            "åšŸ": "ä¾†",
            "å’—": "äº†",
            "å‘¢å€‹": "é€™å€‹",
            "å—°å€‹": "é‚£å€‹",
            "å‘¢å•²": "é€™äº›",
            "å—°å•²": "é‚£äº›",
            "é‚Šåº¦": "å“ªè£¡",
            "é»è§£": "ç‚ºä»€éº¼",
            "ä¹œå˜¢": "ä»€éº¼",
            "ä»Šæ—¥": "ä»Šå¤©",
            "è½æ—¥": "æ˜å¤©",
            "ç´æ—¥": "æ˜¨å¤©",
            "å°‹æ—¥": "æ˜¨å¤©",
            "è€Œå®¶": "ç¾åœ¨",
            "ä¾å®¶": "ç¾åœ¨",
            # ä¿ç•™ï¼šå˜…ã€å””ã€å†‡ã€å•²ã€å’ã€ç‡ã€éšï¼ˆå‘¢å•²ä¿‚ç²µèªæ ¸å¿ƒç‰¹è‰²ï¼‰
            # === å¸¸è¦‹ OCR/ASR éŒ¯èª¤ä¿®æ­£ ===
            "åŸè²»": "æœˆè²»",
            "è²¡æºåå–®": "è£å“¡åå–®",
            "è²¡æº": "è£å“¡",
        }
        
        # === WRITTEN MODE: Full conversions ===
        post_fix_map_written = {
            # === åŸºæœ¬ç²µèªâ†’æ›¸é¢èªè½‰æ›ï¼ˆæœ€é«˜å„ªå…ˆç´šï¼‰===
            "ä¿‚": "æ˜¯",
            "å–º": "åœ¨",
            "å˜…": "çš„",
            "å’—": "äº†",
            "å†‡": "æ²’æœ‰",
            "å””": "ä¸",
            "ä½¢": "ä»–",
            "åšŸ": "ä¾†",
            "å•²": "äº›",
            
            # === é›™å­—/å¤šå­—è©å„ªå…ˆ ===
            "å‘¢å€‹": "é€™å€‹",
            "å—°å€‹": "é‚£å€‹",
            "å‘¢å•²": "é€™äº›",
            "å—°å•²": "é‚£äº›",
            "å‘¢åº¦": "é€™è£¡",
            "å—°åº¦": "é‚£è£¡",
            "å‘¢æ¢": "é€™æ¢",
            "å—°æ¢": "é‚£æ¢",
            "å‘¢å¼µ": "é€™å¼µ",
            "å—°å¼µ": "é‚£å¼µ",
            "å‘¢é–“": "é€™é–“",
            "å—°é–“": "é‚£é–“",
            "å‘¢æ¬¡": "é€™æ¬¡",
            "å—°æ¬¡": "é‚£æ¬¡",
            "å‘¢æ’": "æœ€è¿‘",
            "å””ä¿‚": "ä¸æ˜¯",
            "å””å¥½": "ä¸è¦",
            "å””æœƒ": "ä¸æœƒ",
            "å””çŸ¥": "ä¸çŸ¥é“",
            "å””ä½¿": "ä¸ç”¨",
            "å””è©²": "è¬è¬",
            "å””é§›": "ä¸ç”¨",
            "ä½¢å“‹": "ä»–å€‘",
            "æˆ‘å“‹": "æˆ‘å€‘",
            "ä½ å“‹": "ä½ å€‘",
            "åŒåŸ‹": "è€Œä¸”",
            "åŠ åŸ‹": "åŠ ä¸Š",
            "åŸ‹åšŸ": "éä¾†",
            "è¿”åšŸ": "å›ä¾†",
            "éåšŸ": "éä¾†",
            "å‡ºåšŸ": "å‡ºä¾†",
            "å…¥åšŸ": "é€²ä¾†",
            "ä¸ŠåšŸ": "ä¸Šä¾†",
            "è½åšŸ": "ä¸‹ä¾†",
            "å³ä¿‚": "å°±æ˜¯",
            "å³åˆ»": "ç«‹åˆ»",
            "é‚Šä¸€å€‹": "å“ªä¸€å€‹",
            "å’æ¨£": "é€™æ¨£",
            "é»è§£": "ç‚ºä»€éº¼",
            "é»æ¨£": "æ€æ¨£",
            "ä¹œå˜¢": "ä»€éº¼",
            "å¹¾æ™‚": "ä»€éº¼æ™‚å€™",
            "é‚Šåº¦": "å“ªè£¡",
            "é‚Šå€‹": "å“ªå€‹",
            "å—°é™£": "é‚£æ™‚",
            "å‘¢é™£": "ç¾åœ¨",
            "é ­å…ˆ": "å‰›æ‰",
            "ç´æ—¥": "æ˜¨å¤©",
            "è½æ—¥": "æ˜å¤©",
            "ä»Šæ—¥": "ä»Šå¤©",
            "å°‹æ—¥": "æ˜¨å¤©",
            "èˆŠé™£æ™‚": "ä»¥å‰",
            "è€Œå®¶": "ç¾åœ¨",
            "ä¾å®¶": "ç¾åœ¨",
            "ä¹‹å¾Œ": "å¾Œä¾†",
            "è·Ÿä½": "ç„¶å¾Œ",
            "ç‡": "çœ‹",
            "éš": "æ¼‚äº®",
            "å’": "é€™æ¨£",
            "å¥½å½©": "å¹¸é‹",
            "å¥½è€": "å¾ˆä¹…",
            "å¥½å¤š": "å¾ˆå¤š",
            "å¹¾å¤š": "å¤šå°‘",
            "å•±å•±": "å‰›å‰›",
            "å’ª": "ä¸è¦",
            "è©¦ä¸‹": "è©¦è©¦",
            "è«—ä¸‹": "æƒ³æƒ³",
            "ç‡ä¸‹": "çœ‹çœ‹",
            "è¡Œè·¯": "èµ°è·¯",
            "é£Ÿé£¯": "åƒé£¯",
            "é£Ÿå˜¢": "åƒæ±è¥¿",
            "é£²å˜¢": "å–æ±è¥¿",
            "ç“è¦º": "ç¡è¦º",
            "è‘—è¡«": "ç©¿è¡£",
            "é™¤è¡«": "è„«è¡£",
            "æµ": "æ‰¾",
            "ç•€": "çµ¦",
            "æ¾": "æ‰¾",
            "æ”": "æ‹¿",
            "åŸ·": "æ”¶æ‹¾",
            "ææ‚": "å®Œæˆ",
            "æå®š": "å®Œæˆ",
            "å¾—é–’": "æœ‰ç©º",
            "å†‡å•é¡Œ": "æ²’å•é¡Œ",
            
            # === å¸¸è¦‹ OCR/ASR éŒ¯èª¤ä¿®æ­£ ===
            "åŸè²»": "æœˆè²»",
            "è²¡æºåå–®": "è£å“¡åå–®",
            "è²¡æº": "è£å“¡",
            "è¦–é¡§": "è¦ºå¾—",
            "å‘Šé‹ä½œ": "å°±é‹ä½œ",
            "ç¸¾å„ª": "å¾ˆå¤š",
        }
        
        # Choose dictionary based on style
        # æ›¸é¢èªï¼šå®Œå…¨è½‰æ› (99+ rules) â†’ ä½¿ç”¨ post_fix_map_written
        # åŠæ›¸é¢èªï¼šä¿ç•™å£èª (14 rules) â†’ ä½¿ç”¨ post_fix_map_semi
        if style == 'semi':
            post_fix_map = post_fix_map_semi
            logger.info(f"ğŸ¨ [STYLE] Using SEMI post_fix_map ({len(post_fix_map)} rules) - keeping oral words")
        else:  # written
            post_fix_map = post_fix_map_written
            logger.info(f"ğŸ¨ [STYLE] Using WRITTEN post_fix_map ({len(post_fix_map)} rules) - full conversion")
        
        # Sort by key length (longest first) to avoid partial replacement issues
        sorted_keys = sorted(post_fix_map.keys(), key=len, reverse=True)

        # Apply dictionary ONLY to segments that exist in result
        # (Avoid re-adding deleted duplicate indices)
        for i in range(len(segments)):
            # Skip segments not in result (they were removed as duplicates or failed AI processing)
            if i not in result:
                continue

            text = result[i]

            # Apply dictionary replacements
            for canto in sorted_keys:
                written = post_fix_map[canto]
                if canto in text:
                    text = text.replace(canto, written)

            result[i] = text
        
        logger.info(f"Post-processed {len(result)} segments with dictionary cleanup")
        
        # ========================================
        # ã€ç¬¬ä¸€éšæ®µã€‘èªæ°£è©ç§»é™¤ + æ“´å……ç²µèªè©è½‰æ›
        # ========================================
        
        # èªæ°£è©ç§»é™¤ï¼ˆå¥å°¾ï¼‰
        particles_to_remove = ['å‘€', 'å•¦', 'å–', 'å›‰', 'ã—', 'å’©', 'å“', 'å–‡', 'å•«', 'å˜›']
        
        # æ“´å……ç²µèªè©è½‰æ›ï¼ˆåªåœ¨ written æ¨¡å¼ï¼‰
        if style == 'written':
            additional_conversions = {
                # é‡è©/åŠ©è©ï¼ˆéœ€è¬¹æ…è™•ç†ã€Œå€‹ã€å› ç‚ºã€Œä¸€å€‹ã€ã€Œé€™å€‹ã€å·²è™•ç†ï¼‰
                "å˜…æµ·é®®": "çš„æµ·é®®",
                "å˜…å‘³": "çš„å‘³",
                "å˜…æ™‚å€™": "çš„æ™‚å€™",
                "å˜…åœ°æ–¹": "çš„åœ°æ–¹",
                "å€‹æµ·é®®": "çš„æµ·é®®",  
                "å€‹å‘³": "çš„å‘³",
                
                # å‹•è©
                "æ‹äº†": "æ‹¿äº†",
                "æ‹ä½": "æ‹¿è‘—",
                "æ‹è¿”": "æ‹¿å›",
                "æ‹èµ·": "æ‹¿èµ·",
                "æ¾": "æ‰¾",
                "æ¾åˆ°": "æ‰¾åˆ°",
                "æ”": "æ‹¿",
                "æ”ä½": "æ‹¿è‘—",
                "ç•€": "çµ¦",
                "ç•€éŒ¢": "çµ¦éŒ¢",
                
                # åè©
                "å±‹ä¼": "å®¶",
                "è€ç«‡": "çˆ¶è¦ª",
                "è€æ¯": "æ¯è¦ª",
                
                # å‰¯è©/é€£æ¥è©
                "å¥½ä¼¼": "å¥½åƒ",
                "å’æ¨£": "é€™æ¨£",
                "å’å¤š": "é€™éº¼å¤š",
                "å¹¾å’": "å¤šéº¼",
                "å°±å¥½ä¼¼": "å°±å¥½åƒ",
                
                # ç²µèªã€Œä¾†çš„ã€ç§»é™¤ï¼ˆèªæ…‹å•é¡Œï¼‰
                "ä¾†çš„": "",
                "æ˜ŸæœŸå…­ä¾†çš„": "æ˜ŸæœŸå…­",
                "æ˜ŸæœŸæ—¥ä¾†çš„": "æ˜ŸæœŸæ—¥",
            }
            
            # æ‡‰ç”¨é¡å¤–è½‰æ›ï¼ˆå¾é•·åˆ°çŸ­æ’åºé¿å…éƒ¨åˆ†æ›¿æ›ï¼‰
            sorted_additional = sorted(additional_conversions.keys(), key=len, reverse=True)
            for idx, text in result.items():
                for canto, written in [(k, additional_conversions[k]) for k in sorted_additional]:
                    if canto in text:
                        text = text.replace(canto, written)
                result[idx] = text
        
        # ç§»é™¤å¥å°¾èªæ°£è©
        for idx, text in result.items():
            original = text
            # ç§»é™¤å¥å°¾èªæ°£è©ï¼ˆå¯èƒ½æœ‰å¤šå€‹ï¼‰
            while text and text[-1] in particles_to_remove:
                text = text[:-1].strip()
            
            # ç§»é™¤å¥ä¸­çš„èªæ°£è©ï¼ˆå¸¶é€—è™Ÿçš„æƒ…æ³ï¼‰
            for particle in particles_to_remove:
                text = text.replace(f"{particle}ï¼Œ", "ï¼Œ")
                text = text.replace(f"{particle},", ",")
            
            if text != original:
                logger.debug(f"Removed particles: '{original}' -> '{text}'")
                result[idx] = text
        
        logger.info("Phase 1 complete: Particle removal + Cantonese word expansion")
        
        # ========================================
        # ã€ç¬¬äºŒéšæ®µã€‘åŒéŸ³å­—ä¿®æ­£ + é‡è¤‡å»é™¤
        # ========================================
        
        # å¸¸è¦‹åŒéŸ³å­—/èªéŸ³è­˜åˆ¥éŒ¯èª¤ä¿®æ­£
        homophone_fixes = {
            "å¤©æ°£ä¾†äº†": "å¤©äº®äº†",
            "ä¸­é€”å‡æœŸ": "é€±æœ«å‡æœŸ",
            "ä¸è¦è¦": "æ˜¯ä¸æ˜¯è¦",
            "å¾Œä¾†æ²’æœ‰": "ä¹‹å¾Œæ²’æœ‰",
            "å¾Œä¾†æœƒ": "ä¹‹å¾Œæœƒ",
            "å¾Œä¾†éƒ½": "ä¹‹å¾Œéƒ½",
            "å¾Œä¾†å¯ä»¥": "ä¹‹å¾Œå¯ä»¥",
            "ç²¾å“åº—": "å®¶å…·åº—",  # èªå¢ƒï¼šè²·æ¡Œå­
            "åƒé£¯å°": "é¤æ¡Œ",
            "å€‹ä½": "ä½ç½®",  # ã€Œæ”¶éŠ€æ©Ÿçš„å€‹ä½ã€-> ã€Œæ”¶éŠ€å°çš„ä½ç½®ã€
            "åŸè²»": "æœˆè²»",
            "è²¡æº": "è£å“¡",
            "è²¡æºåå–®": "è£å“¡åå–®",
        }
        
        for idx, text in result.items():
            for wrong, correct in homophone_fixes.items():
                if wrong in text:
                    text = text.replace(wrong, correct)
                    logger.debug(f"Homophone fix: '{wrong}' -> '{correct}'")
            result[idx] = text
        
        # é‡è¤‡èªå¥æª¢æ¸¬èˆ‡å»é™¤
        # æª¢æ¸¬é€£çºŒå…©å€‹ segment çš„ç›¸ä¼¼åº¦
        segments_to_remove = set()
        prev_text = None
        prev_idx = None
        
        for idx in sorted(result.keys()):
            text = result[idx].strip()
            
            if prev_text and text:
                # è¨ˆç®—ç›¸ä¼¼åº¦ï¼ˆç°¡å–®å­—ç¬¦ä¸²åŒ¹é…ï¼‰
                if text == prev_text:
                    # å®Œå…¨é‡è¤‡ï¼Œæ¨™è¨˜ç§»é™¤ç•¶å‰
                    segments_to_remove.add(idx)
                    logger.warning(f"Duplicate detected: segment {idx} == {prev_idx}")
                elif len(text) > 10 and len(prev_text) > 10:
                    # é•·å¥å­ï¼Œæª¢æŸ¥æ˜¯å¦åŒ…å«
                    if text in prev_text or prev_text in text:
                        # ä¸€å€‹åŒ…å«å¦ä¸€å€‹ï¼Œä¿ç•™è¼ƒé•·çš„
                        if len(text) > len(prev_text):
                            segments_to_remove.add(prev_idx)
                            logger.warning(f"Duplicate detected: segment {prev_idx} contained in {idx}")
                        else:
                            segments_to_remove.add(idx)
                            logger.warning(f"Duplicate detected: segment {idx} contained in {prev_idx}")
            
            prev_text = text
            prev_idx = idx
        
        # ç§»é™¤é‡è¤‡
        for idx in segments_to_remove:
            del result[idx]
        
        if segments_to_remove:
            logger.info(f"Phase 2 complete: Removed {len(segments_to_remove)} duplicate segments")
        else:
            logger.info("Phase 2 complete: No duplicates found")
        
        # ========================================
        # ã€ç¬¬ä¸‰éšæ®µã€‘èªå¥å®Œæ•´æ€§ + ç‰¹æ®ŠéŒ¯åˆ¥å­—
        # ========================================
        
        # æª¢æ¸¬ä¸å®Œæ•´èªå¥ï¼ˆçµå°¾ç‚ºé€£æ¥è©ä½†ç„¡ä¸‹æ–‡ï¼‰
        incomplete_endings = [
            'ä¸¦ä¸”å¦‚æ­¤', 'åŒæœ‰', 'å’Œæœ‰', 'ä¸¦ä¸”æœ‰', 'è€Œä¸”æœ‰',  # å¾å¯¦éš›éŒ¯èª¤ä¸­ç™¼ç¾
            'ä½†åŒæ™‚', 'è€Œä¸”', 'ä»¥åŠ', 'é‚„æœ‰', 'ç„¶å¾Œ', 'æ¥è‘—',
            'ä¸¦ä¸”', 'åŒæ™‚', 'å› æ­¤', 'æ‰€ä»¥', 'ä½†æ˜¯',  # å¸¸è¦‹é€£æ¥è©
            'ä¸é', 'åªæ˜¯', 'åªä¸é'  # è½‰æŠ˜è©
        ]
        
        for idx, text in result.items():
            for ending in incomplete_endings:
                if text.endswith(ending):
                    # ç§»é™¤ä¸å®Œæ•´çš„çµå°¾
                    text = text[:-len(ending)].strip()
                    logger.warning(f"Incomplete sentence: removed '{ending}' from segment {idx}")
                    result[idx] = text
        
        logger.info("Phase 3 complete: Incomplete sentence cleanup")

        # ========================================
        # ã€ç¬¬å››éšæ®µã€‘ç°¡é«”å­—å¼·åˆ¶è½‰æ›ï¼ˆé›™é‡é˜²ç¦¦ï¼‰
        # ========================================
        # å³ä½¿ Prompt è¦æ±‚ç¹é«”ï¼ŒLLM ä»å¯èƒ½è¼¸å‡ºç°¡é«”å­—
        # ä½¿ç”¨ OpenCC å¼·åˆ¶è½‰æ›æ‰€æœ‰ç°¡é«”å­—ç‚ºç¹é«”

        if self.s2t_converter:
            simplified_detected_count = 0
            for idx, text in result.items():
                # ä½¿ç”¨ OpenCC å¼·åˆ¶è½‰æ›
                text_traditional = self.s2t_converter.convert(text)
                if text != text_traditional:
                    simplified_detected_count += 1
                    logger.warning(f"âŒ Simplified Chinese detected in segment {idx}: '{text[:50]}'")
                    logger.info(f"âœ… Auto-converted to Traditional: '{text_traditional[:50]}'")
                    result[idx] = text_traditional

            if simplified_detected_count > 0:
                logger.warning(f"âš ï¸ Total {simplified_detected_count} segments had simplified Chinese and were auto-fixed")
            else:
                logger.info("âœ… No simplified Chinese detected")

        logger.info("Phase 4 complete: Simplified to Traditional Chinese conversion")

        # âš ï¸ ã€èˆŠçš„ç°¡é«”å­—æª¢æ¸¬ã€‘ä¿ç•™ä½œç‚ºå‚™ä»½
        if False and self.s2t_converter:
            # å¸¸è¦‹ç°¡é«”å­—åˆ—è¡¨ï¼ˆç”¨æ–¼å¿«é€Ÿæª¢æ¸¬ï¼‰
            simplified_chars = 'å°½è§‰å°è¯•éªŒè¯è®¯å·å‘ç°é—®é¢˜åº”è¯¥è¿™ä¸ªæ€ä¹ˆæ ·æ—¶é—´'
            detected_count = 0
            for idx, text in result.items():
                if any(char in text for char in simplified_chars):
                    detected_count += 1
                    logger.error(f"âŒ CRITICAL: Simplified Chinese detected in segment {idx}: '{text[:50]}'")
                    # å¼·åˆ¶å†è½‰ä¸€æ¬¡
                    result[idx] = self.s2t_converter.convert(text)
                    logger.info(f"âœ… Auto-fixed to Traditional: '{result[idx][:50]}'")
            
            if detected_count > 0:
                logger.warning(f"âš ï¸ Total {detected_count} segments had simplified Chinese and were auto-fixed")
        
        return result

    def _convert_cantonese_dict(self, text: str, style: str) -> str:
        """Dictionary-based Cantonese conversion (no AI)."""
        if style == 'spoken':
            return text
        
        # åŠæ›¸é¢èªï¼šåªä¿ç•™æœ€æ ¸å¿ƒå˜…ç²µèªå­—
        # è½‰æ›ï¼šä¿‚ã€å–ºã€ä½¢ã€å’—ã€åšŸ ç­‰
        # ä¿ç•™ï¼šå˜…ã€å””ã€å†‡ï¼ˆå‘¢å•²ä¿‚ç²µèªå˜…éˆé­‚ï¼ŒåŒ…å«å‘¢å•²å­—å˜…è©çµ„éƒ½è¦ä¿ç•™ï¼‰
        # ä¿ç•™ä½†å””åšå­—ç¬¦åŒ¹é…ï¼šå•²ã€å’ã€ç‡ã€éšï¼ˆå–®å­—ä¿ç•™ï¼Œä½†è©çµ„å¯è½‰æ›ï¼‰
        keep_chars_semi = set('å˜…å””å†‡')  # å­—ç¬¦ç´šä¿ç•™
        keep_words_semi = ['å•²', 'å’', 'ç‡', 'éš']  # å–®å­—ä¿ç•™
        
        result = text
        sorted_keys = sorted(self.cantonese_map.keys(), key=len, reverse=True)
        
        for canto_word, written_word in [(k, self.cantonese_map[k]) for k in sorted_keys]:
            # å¦‚æœè©çµ„åŒ…å«æ ¸å¿ƒå­—ç¬¦ï¼ˆå˜…ã€å””ã€å†‡ï¼‰ï¼Œè·³éè½‰æ›
            if style == 'semi' and any(c in keep_chars_semi for c in canto_word):
                continue
            # å¦‚æœä¿‚å–®å­—ä¿ç•™è©ï¼Œè·³éè½‰æ›
            if style == 'semi' and canto_word in keep_words_semi:
                continue
            if canto_word in result:
                result = result.replace(canto_word, written_word)
        
        return result

    def _convert_cantonese(self, text: str, style: str, use_ai: bool = False) -> str:
        """
        Convert Cantonese text based on style.
        
        Args:
            text: Input text
            style: 'spoken', 'semi', or 'written'
            use_ai: Use AI (QwenLLM) for context-aware conversion
            
        Returns:
            Converted text
        """
        if style == 'spoken':
            # Keep original Cantonese
            return text
        
        # AI conversion with Qwen2.5-3B (better quality than 1.5B)
        if use_ai and style in ('semi', 'written'):
            try:
                if self.llm_processor is None:
                    # Check if model needs to be downloaded first
                    try:
                        from ui.download_dialog import check_and_download_model
                        from core.hardware_detector import HardwareDetector
                        
                        detector = HardwareDetector()
                        profile = detector.detect()
                        
                        # Show download dialog if model not cached
                        model_ready = check_and_download_model(
                            profile.llm_a_model,
                            profile.llm_a_quantization,
                            parent=None
                        )
                        
                        if not model_ready:
                            logger.warning("Model download cancelled or failed, using dictionary")
                            # Fall through to dictionary conversion
                        else:
                            # Load the model
                            logger.info("Initializing Qwen2.5-3B for AI style conversion...")
                            from models.qwen_llm import QwenLLM
                            self.llm_processor = QwenLLM(self.config, profile)
                            self.llm_processor.load_models()
                            logger.info("Qwen2.5-3B loaded successfully")
                    except Exception as load_error:
                        logger.warning(f"Failed to load model for AI conversion: {load_error}")
                        # Fall through to dictionary conversion
                
                # CRITICAL: Check llm_processor is ready before using
                if self.llm_processor is not None:
                    # Use specialized prompt for colloquial-to-written conversion
                    from prompts.cantonese_prompts import COLLOQUIAL_TO_WRITTEN_PROMPT
                    prompt = COLLOQUIAL_TO_WRITTEN_PROMPT.format(text=text)
                    
                    # Generate response directly using MLXQwenLLM's generate method
                    converted = self.llm_processor.generate(prompt, max_tokens=512, temperature=0.3)
                    
                    # Clean up LLM output - remove common prefixes
                    if converted:
                        converted = converted.strip()
                        # Remove common output prefixes
                        prefixes_to_remove = [
                            "æ›¸é¢èªç¿»è­¯çµæœï¼š", "æ›¸é¢èªï¼š", "ç¿»è­¯çµæœï¼š", 
                            "çµæœï¼š", "ç¿»è­¯ï¼š", "æ›¸é¢èªç‰ˆæœ¬ï¼š"
                        ]
                        for prefix in prefixes_to_remove:
                            if converted.startswith(prefix):
                                converted = converted[len(prefix):].strip()
                        
                        if converted and converted != text.strip():
                            logger.info(f"AI conversion: '{text[:30]}...' -> '{converted[:30]}...'")
                            return converted
                        else:
                            logger.warning(f"AI returned same text or empty, using dictionary")
                else:
                    logger.warning("LLM processor not available, using dictionary fallback")
                    
            except Exception as e:
                logger.warning(f"AI conversion failed, falling back to dictionary: {e}")
        
        # Dictionary-based conversion (fallback or default)
        # åŠæ›¸é¢èªï¼šåªä¿ç•™æœ€æ ¸å¿ƒå˜…ç²µèªå­—
        # è½‰æ›ï¼šä¿‚ã€å–ºã€ä½¢ã€å’—ã€åšŸ ç­‰
        # ä¿ç•™ï¼šå˜…ã€å””ã€å†‡ï¼ˆå‘¢å•²ä¿‚ç²µèªå˜…éˆé­‚ï¼ŒåŒ…å«å‘¢å•²å­—å˜…è©çµ„éƒ½è¦ä¿ç•™ï¼‰
        # ä¿ç•™ä½†å””åšå­—ç¬¦åŒ¹é…ï¼šå•²ã€å’ã€ç‡ã€éšï¼ˆå–®å­—ä¿ç•™ï¼Œä½†è©çµ„å¯è½‰æ›ï¼‰
        keep_chars_semi = set('å˜…å””å†‡')  # å­—ç¬¦ç´šä¿ç•™
        keep_words_semi = ['å•²', 'å’', 'ç‡', 'éš']  # å–®å­—ä¿ç•™
        
        result = text
        conversions = []
        
        # Sort by length (longest first) to avoid partial replacements
        sorted_keys = sorted(self.cantonese_map.keys(), key=len, reverse=True)
        
        for canto_word, written_word in [(k, self.cantonese_map[k]) for k in sorted_keys]:
            # å¦‚æœè©çµ„åŒ…å«æ ¸å¿ƒå­—ç¬¦ï¼ˆå˜…ã€å””ã€å†‡ï¼‰ï¼Œè·³éè½‰æ›
            if style == 'semi' and any(c in keep_chars_semi for c in canto_word):
                continue
            # å¦‚æœä¿‚å–®å­—ä¿ç•™è©ï¼Œè·³éè½‰æ›
            if style == 'semi' and canto_word in keep_words_semi:
                continue  # Skip conversion for semi-written style
            
            if canto_word in result:
                result = result.replace(canto_word, written_word)
                conversions.append(f"{canto_word}â†’{written_word}")
        
        if conversions:
            logger.debug(f"[Cantonese] Conversions: {', '.join(conversions[:5])}")
        
        return result

    def _filter_profanity(self, text: str, mode: str) -> str:
        """Filter profanity based on mode."""
        if mode == 'keep':
            return text
        
        result = text    
        # Sort by length to match phrases first
        sorted_keys = sorted(self.profanity_map.keys(), key=len, reverse=True)
        
        for prof_word in sorted_keys:
            if prof_word in result:
                if mode == 'mask':
                    result = result.replace(prof_word, 'â˜…' * len(prof_word))
                elif mode == 'mild':
                    mild_replacement = self.profanity_map[prof_word]
                    result = result.replace(prof_word, mild_replacement)
        
        return result

    def _format_numbers(self, text: str, mode: str) -> str:
        """Format numbers to Arabic or Chinese."""
        chinese_nums = 'é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒè¬'
        
        if mode == 'chinese':
            # Convert Arabic to Chinese
            digit_map = {'0': 'é›¶', '1': 'ä¸€', '2': 'äºŒ', '3': 'ä¸‰', 
                        '4': 'å››', '5': 'äº”', '6': 'å…­', '7': 'ä¸ƒ',
                        '8': 'å…«', '9': 'ä¹'}
            
            def replace_digit(match):
                return ''.join(digit_map.get(d, d) for d in match.group(0))
            
            return re.sub(r'\d+', replace_digit, text)
        
        elif mode == 'arabic':
            # CRITICAL: Do NOT convert common words containing "ä¸€", "äºŒ", etc.
            # Exclusion list for common words that should NOT be converted
            exclude_words = [
                'ä¸€å®š', 'ä¸€èµ·', 'ä¸€æ¨£', 'ä¸€ç›´', 'ä¸€åˆ‡', 'ç¬¬ä¸€', 'çµ±ä¸€', 'å”¯ä¸€', 
                'ä¸€ä¸‹', 'ä¸€æ¬¡', 'ä¸€å€‹', 'ä¸€é»', 'ä¸€äº›', 'ä¸€èˆ¬', 'ä¸€é‚Š', 'ä¸€æ—¦',
                'äºŒæ‰‹', 'äºŒæ¬¡', 'ä¸äºŒ', 'ååˆ†', 'ä¹æˆ', 'ä¸ƒåäºŒ', 'ä¸‰åå…­',
                'æ˜ŸæœŸä¸€', 'æ˜ŸæœŸäºŒ', 'æ˜ŸæœŸä¸‰', 'æ˜ŸæœŸå››', 'æ˜ŸæœŸäº”', 'æ˜ŸæœŸå…­', 'æ˜ŸæœŸæ—¥',
                'ç¬¬ä¸‰', 'ç¬¬å››', 'ç¬¬äº”', 'ç¬¬å…­', 'ç¬¬ä¸ƒ', 'ç¬¬å…«', 'ç¬¬ä¹', 'ç¬¬å'
            ]
            
            # Create protection: temporarily replace excluded words with placeholders
            protected = {}
            for i, word in enumerate(exclude_words):
                if word in text:
                    placeholder = f"__PROTECTED_{i}__"
                    text = text.replace(word, placeholder)
                    protected[placeholder] = word
            
            # Convert Chinese to Arabic (basic patterns)
            # Map Chinese digits
            digit_map = {'é›¶': 0, 'ä¸€': 1, 'äºŒ': 2, 'ä¸‰': 3, 'å››': 4,
                        'äº”': 5, 'å…­': 6, 'ä¸ƒ': 7, 'å…«': 8, 'ä¹': 9}
            
            def convert_chinese_number(match):
                num_str = match.group(0)
                
                # Handle tens (å, äºŒå, ä¹åä¹, etc.)
                if 'å' in num_str:
                    if num_str == 'å':
                        return '10'
                    elif num_str[0] == 'å':  # åä¸€, åäºŒ...
                        ones = digit_map.get(num_str[1], 0) if len(num_str) > 1 else 0
                        return str(10 + ones)
                    else:  # äºŒå, ä¹åä¹...
                        tens = digit_map.get(num_str[0], 0)
                        ones = digit_map.get(num_str[2], 0) if len(num_str) > 2 else 0
                        return str(tens * 10 + ones)
                
                # Handle simple digit sequences ONLY if it's a standalone number
                # Single character numbers like "ä¸€" "äºŒ" should NOT be converted
                # unless they are truly numeric (e.g. "ä¸€æœˆ" -> keep, but standalone "ä¸€" in number context -> convert)
                if len(num_str) == 1:
                    # Single digit: likely part of a word, don't convert
                    return num_str
                    
                result = ''
                for char in num_str:
                    if char in digit_map:
                        result += str(digit_map[char])
                return result if result else num_str
            
            # Match Chinese number patterns (at least 2 consecutive digits or with å)
            pattern = r'[é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹]{2,}|[é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]å[é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹]?'
            text = re.sub(pattern, convert_chinese_number, text)
            
            # Restore protected words
            for placeholder, original in protected.items():
                text = text.replace(placeholder, original)
        
        return text

    # ... (other methods) ...

    def _process_english(self, text: str, mode: str) -> str:
        """Process English text with smart AI translation."""
        logger.debug(f"_process_english called: mode={mode}, text='{text[:50]}'")
        
        if mode == 'keep':
            return text
            
        # Detect English words
        if not re.search(r'[a-zA-Z]', text):
            logger.debug("No English detected, returning original")
            return text
        
        logger.info(f"English detected in '{text[:30]}...', mode={mode}")
        
        if mode == 'translate':
            # Smart translation: extract English portions and translate them
            return self._smart_translate_english(text)
            
        elif mode == 'bilingual':
            # Bilingual: Show both original and translated
            translated = self._smart_translate_english(text)
            if translated != text:
                return f"{text}\n{translated}"
            return text
            
        return text
    
    def _smart_translate_english(self, text: str) -> str:
        """
        Smart English translation for mixed Chinese-English text.
        Extracts English segments and translates them contextually.
        """
        # Pattern WITHOUT word boundaries (\b doesn't work with Chinese)
        # Match English words including hyphens and apostrophes
        english_pattern = re.compile(r"[a-zA-Z]+(?:[\s\-'][a-zA-Z]+)*", re.IGNORECASE)
        
        matches = list(english_pattern.finditer(text))
        
        if not matches:
            return text
        
        logger.info(f"Found {len(matches)} English segments in: '{text}'")
        
        # Build translation map first (avoid modifying while iterating)
        translations = {}  # {english_text: translation}
        
        for match in matches:
            english_text = match.group(0).strip()
            
            if not english_text or english_text in translations:
                continue
            
            cache_key = english_text.lower()
            
            # Check global cache first
            if cache_key in self.translation_cache:
                translations[english_text] = self.translation_cache[cache_key]
                logger.debug(f"Using cache: '{english_text}' -> '{translations[english_text]}'")
                continue
            
            translation = None
            
            # First, check dictionary for exact match
            if cache_key in self.english_map:
                translation = self.english_map[cache_key]
                logger.info(f"Dictionary exact: '{english_text}' -> '{translation}'")
            else:
                # Try word-by-word translation for multi-word phrases
                words = english_text.split()
                if len(words) > 1:
                    translated_words = []
                    all_found = True
                    for word in words:
                        word_clean = word.lower().strip("-'.,!?")
                        if word_clean in self.english_map:
                            translated_words.append(self.english_map[word_clean])
                        else:
                            all_found = False
                            break
                    
                    if all_found and translated_words:
                        translation = ''.join(translated_words)
                        logger.info(f"Dictionary word-by-word: '{english_text}' -> '{translation}'")
                
                # If still no translation, use AI translation
                if not translation:
                    logger.info(f"Not in dictionary, using AI: '{english_text}'")
                    translation = self._translate_with_ai(english_text)
                    
                    # Validate AI result - check for repetition/corruption
                    if translation:
                        # If translation contains repeated original text, it's corrupted
                        if english_text.lower() in translation.lower() and translation != english_text:
                            # Extract just the new part
                            translation = translation.replace(english_text, '').strip()
                            if not translation:
                                translation = english_text
                                logger.warning(f"AI returned corrupted result, keeping: '{english_text}'")
                    
                    # If AI also fails, keep original
                    if not translation or translation == english_text:
                        translation = english_text
                        logger.warning(f"AI translation failed, keeping: '{english_text}'")
            
            translations[english_text] = translation
            self.translation_cache[cache_key] = translation
        
        # Now replace all occurrences
        result = text
        for english_text, translation in translations.items():
            if english_text != translation:
                result = result.replace(english_text, translation)
                logger.debug(f"Replaced: '{english_text}' -> '{translation}'")
        
        if result != text:
            logger.info(f"Final: '{text}' -> '{result}'")
        return result
        return result

    def _should_use_ai_translation(self, text: str) -> bool:
        """Check if we should use AI translation (e.g. sentence vs single word)."""
        # If text has more than 2 words, use AI
        return len(text.split()) > 2

    def _translate_with_ai(self, text: str) -> str:
        """
        Hybrid translation strategy: Dictionary â†’ Qwen LLM â†’ MarianMT.
        
        Args:
            text: English text to translate
            
        Returns:
            Translated Chinese text, or original if translation fails
        """
        # === LAYER 1: Dictionary (fastest, 100% accurate) ===
        dict_result = self._dictionary_translate(text)
        if dict_result != text:
            logger.info(f"[Dictionary] '{text}' -> '{dict_result}'")
            return dict_result
        
        
        # === LAYER 2: Qwen LLM (DISABLED - causes hallucination issues) ===
        # Disabled due to: hallucinations, repetitions, wrong translations
        # Example issues: "çµçµçµ...", "è¿ˆå…‹å°”Â·è¿ˆå…‹å°”", "call off -> å…³é—­ç”µè¯"
        # Now using Dictionary -> MarianMT directly for reliability
        
        # if self.llm_processor is not None and hasattr(self.llm_processor, 'model') and self.llm_processor.model is not None:
        #     try:
        #         ... (Qwen code commented out)
        #     except Exception as e:
        #         logger.warning(f"Qwen translation failed: {e}")
        
        
        # === LAYER 3: MarianMT (fallback) ===
        try:
            if not self.translation_model:
                logger.info("Initializing MarianMT Translation Model...")
                from models.translation_model import TranslationModel
                self.translation_model = TranslationModel(self.config)

            result = self.translation_model.translate(text)

            if result and result.strip() and result != text:
                # MarianMT outputs Simplified Chinese, convert to Traditional immediately
                if self.s2t_converter:
                    result = self.s2t_converter.convert(result)
                    logger.info(f"[MarianMT+S2T] '{text}' -> '{result}'")
                else:
                    logger.warning(f"[MarianMT] OpenCC not available, output may be Simplified: '{text}' -> '{result}'")

                # Cache for future use
                self.translation_cache[text.lower()] = result
                return result
            else:
                logger.warning(f"MarianMT returned empty or same, keeping original")
                return text

        except Exception as e:
            logger.error(f"MarianMT failed: {e}", exc_info=True)
            return text
    
    def _dictionary_translate(self, text: str) -> str:
        """
        Fallback dictionary translation.
        
        Args:
            text: Text to translate
            
        Returns:
            Translated text using dictionary
        """
        # Try word-by-word translation
        words = text.split()
        translated_words = []
        
        for word in words:
            word_lower = word.lower().strip('.,!?;:')
            if word_lower in self.english_map:
                translated_words.append(self.english_map[word_lower])
            else:
                translated_words.append(word)  # Keep original if not in dictionary
        
        result = ' '.join(translated_words)
        
        # If no translation happened, return original
        return result if result != text else text

    def _split_long_lines(self, segments: List[Dict], max_chars: int = 14) -> List[Dict]:
        """Split long lines into two segments.
        
        Args:
            segments: List of subtitle segments
            max_chars: Maximum characters per line (from user settings)
        """
        new_segments = []
        
        logger.info(f"[SplitLong] Splitting lines with max_chars={max_chars}")
        
        for seg in segments:
            text = seg['text']
            if len(text) > max_chars:
                # Simple split at middle
                mid = len(text) // 2
                # Try to find punctuation near middle
                split_idx = mid
                
                # Search for punctuation
                punctuations = 'ï¼Œã€‚ï¼ï¼Ÿã€,.;?! '
                best_dist = float('inf')
                
                for i, char in enumerate(text):
                    if char in punctuations:
                        dist = abs(i - mid)
                        if dist < best_dist:
                            best_dist = dist
                            split_idx = i + 1
                
                # Split
                part1 = text[:split_idx].strip()
                part2 = text[split_idx:].strip()
                
                duration = seg['end'] - seg['start']
                mid_time = seg['start'] + (len(part1) / len(text)) * duration
                
                if part1:
                    new_segments.append({
                        'start': seg['start'],
                        'end': mid_time,
                        'text': part1,
                        'words': [] # Lost word info for now
                    })
                if part2:
                    new_segments.append({
                        'start': mid_time,
                        'end': seg['end'],
                        'text': part2,
                        'words': []
                    })
            else:
                new_segments.append(seg)
                
        return new_segments
