"""
Advanced Transcription - é«˜ç´šè½‰éŒ„æ¨¡çµ„

æä¾›æ¥µè‡´æº–ç¢ºåº¦çš„è½‰éŒ„åŠŸèƒ½ï¼š
1. VAD é åˆ†å‰² - å…ˆåˆ†å‰²å†è½‰éŒ„ï¼Œç¢ºä¿ä¸åœ¨å¥ä¸­åˆ‡æ–·
2. é‡ç–Šçª—å£ - é‚Šç•Œå€åŸŸäº’ç›¸é©—è­‰
3. ä¸‰éšæ®µè½‰éŒ„ - æ¼¸é€²å¼å„ªåŒ–
4. éŒ¨é»ç³»çµ± - é«˜ä¿¡å¿ƒå€åŸŸé©—è­‰ä½ä¿¡å¿ƒå€åŸŸ
"""

import tempfile
from pathlib import Path
from typing import List, Optional, Callable, Dict, Tuple
from dataclasses import dataclass, field
import numpy as np

from utils.logger import setup_logger

logger = setup_logger()


@dataclass
class TranscriptionChunk:
    """è½‰éŒ„ç‰‡æ®µ"""
    start: float
    end: float
    text: str
    confidence: float
    words: List[Dict] = field(default_factory=list)
    is_anchor: bool = False  # æ˜¯å¦ç‚ºé«˜ä¿¡å¿ƒéŒ¨é»


@dataclass
class OverlapResult:
    """é‡ç–Šå€åŸŸé©—è­‰çµæœ"""
    start: float
    end: float
    text_primary: str
    text_secondary: str
    confidence_primary: float
    confidence_secondary: float
    consensus_text: str
    agreement_score: float  # å…©å€‹ç‰ˆæœ¬çš„ä¸€è‡´æ€§åˆ†æ•¸


class AdvancedTranscriber:
    """
    é«˜ç´šè½‰éŒ„å™¨

    æ ¸å¿ƒç­–ç•¥ï¼š
    1. VAD é åˆ†å‰²ï¼šç”¨ VAD æ‰¾åˆ°è‡ªç„¶æ–·é»ï¼Œç¢ºä¿æ¯å€‹ chunk æ˜¯å®Œæ•´èªå¥
    2. é‡ç–Šçª—å£ï¼šç›¸é„° chunk æœ‰é‡ç–Šï¼Œç”¨å…±è­˜æŠ•ç¥¨é¸æœ€ä½³ç‰ˆæœ¬
    3. ä¸‰éšæ®µæµç¨‹ï¼šç²—è½‰éŒ„ â†’ ä½ä¿¡å¿ƒé‡è½‰ â†’ LLM æ ¡æ­£
    4. éŒ¨é»ç³»çµ±ï¼šé«˜ä¿¡å¿ƒå€åŸŸä½œç‚ºåƒç…§ï¼Œé©—è­‰å‘¨åœä½ä¿¡å¿ƒå€åŸŸ
    """

    def __init__(self, config=None):
        """
        åˆå§‹åŒ–é«˜ç´šè½‰éŒ„å™¨

        Args:
            config: æ‡‰ç”¨é…ç½®
        """
        self.config = config
        self.temp_dir = Path(tempfile.gettempdir()) / "canto_beats_adv"
        self.temp_dir.mkdir(exist_ok=True)

        # è½‰éŒ„åƒæ•¸
        self.max_chunk_duration = 25.0  # æœ€å¤§ chunk é•·åº¦ï¼ˆç§’ï¼‰
        self.min_chunk_duration = 2.0   # æœ€å° chunk é•·åº¦ï¼ˆç§’ï¼‰
        self.overlap_duration = 3.0     # é‡ç–Šå€åŸŸé•·åº¦ï¼ˆç§’ï¼‰
        self.anchor_confidence_threshold = 0.85  # éŒ¨é»ä¿¡å¿ƒé–¾å€¼

    # ==================== VAD é åˆ†å‰² ====================

    def vad_presplit(
        self,
        audio_path: str,
        vad_processor=None
    ) -> List[Tuple[float, float]]:
        """
        ä½¿ç”¨ VAD é åˆ†å‰²éŸ³é »

        ç­–ç•¥ï¼š
        1. ç”¨ VAD æª¢æ¸¬æ‰€æœ‰èªéŸ³æ®µè½
        2. åˆä½µéçŸ­çš„æ®µè½
        3. åœ¨é•·éœéŸ³è™•åˆ†å‰²
        4. ç¢ºä¿æ¯å€‹ chunk ä¸è¶…é max_chunk_duration

        Args:
            audio_path: éŸ³é »è·¯å¾‘
            vad_processor: VAD è™•ç†å™¨ï¼ˆå¯é¸ï¼Œæœƒè‡ªå‹•å‰µå»ºï¼‰

        Returns:
            List of (start, end) tuples
        """
        logger.info("ğŸ”ª åŸ·è¡Œ VAD é åˆ†å‰²...")

        # ç²å–éŸ³é »æ™‚é•·
        import soundfile as sf
        audio, sr = sf.read(audio_path)
        total_duration = len(audio) / sr

        # ç²å– VAD æ®µè½
        if vad_processor is None:
            from models.vad_processor import VADProcessor
            from core.config import Config
            vad_processor = VADProcessor(
                Config() if self.config is None else self.config,
                threshold=0.3,  # è¼ƒä½é–¾å€¼ï¼Œæ¸›å°‘æ¼æª¢
                min_speech_duration_ms=100,
                min_silence_duration_ms=300,
                speech_pad_ms=200
            )
            vad_processor.load_model()

        voice_segments = vad_processor.detect_voice_segments(audio_path)
        logger.info(f"VAD æª¢æ¸¬åˆ° {len(voice_segments)} å€‹èªéŸ³æ®µè½")

        if not voice_segments:
            # æ²’æœ‰æª¢æ¸¬åˆ°èªéŸ³ï¼ŒæŒ‰å›ºå®šé•·åº¦åˆ†å‰²
            return self._fixed_split(total_duration)

        # æ™ºèƒ½åˆ†çµ„
        chunks = []
        current_chunk_start = voice_segments[0].start
        current_chunk_end = voice_segments[0].end
        prev_end = voice_segments[0].end

        for seg in voice_segments[1:]:
            gap = seg.start - prev_end
            potential_duration = seg.end - current_chunk_start

            # æ±ºå®šæ˜¯å¦é–‹å§‹æ–° chunk
            should_split = False

            # æ¢ä»¶ 1ï¼šé•·éœéŸ³ï¼ˆ> 1.5 ç§’ï¼‰
            if gap > 1.5:
                should_split = True

            # æ¢ä»¶ 2ï¼šç•¶å‰ chunk å¤ªé•·
            elif potential_duration > self.max_chunk_duration:
                should_split = True

            # æ¢ä»¶ 3ï¼šä¸­ç­‰éœéŸ³ + ç•¶å‰ chunk å·²ç¶“å¤ é•·
            elif gap > 0.8 and (current_chunk_end - current_chunk_start) > 10:
                should_split = True

            if should_split:
                # ä¿å­˜ç•¶å‰ chunk
                if current_chunk_end - current_chunk_start >= self.min_chunk_duration:
                    chunks.append((current_chunk_start, current_chunk_end))

                # é–‹å§‹æ–° chunk
                current_chunk_start = seg.start

            current_chunk_end = seg.end
            prev_end = seg.end

        # ä¿å­˜æœ€å¾Œä¸€å€‹ chunk
        if current_chunk_end - current_chunk_start >= self.min_chunk_duration:
            chunks.append((current_chunk_start, current_chunk_end))

        # è™•ç†éé•·çš„ chunk
        final_chunks = []
        for start, end in chunks:
            if end - start > self.max_chunk_duration:
                # éœ€è¦é€²ä¸€æ­¥åˆ†å‰²
                sub_chunks = self._split_long_chunk(start, end, voice_segments)
                final_chunks.extend(sub_chunks)
            else:
                final_chunks.append((start, end))

        logger.info(f"âœ… VAD é åˆ†å‰²å®Œæˆï¼š{len(final_chunks)} å€‹ chunks")
        for i, (s, e) in enumerate(final_chunks[:5]):
            logger.debug(f"  Chunk {i+1}: {s:.2f}s - {e:.2f}s ({e-s:.2f}s)")

        return final_chunks

    def _fixed_split(self, total_duration: float) -> List[Tuple[float, float]]:
        """å›ºå®šé•·åº¦åˆ†å‰²ï¼ˆå‚™ç”¨æ–¹æ¡ˆï¼‰"""
        chunks = []
        start = 0
        while start < total_duration:
            end = min(start + self.max_chunk_duration, total_duration)
            chunks.append((start, end))
            start = end
        return chunks

    def _split_long_chunk(
        self,
        start: float,
        end: float,
        voice_segments: List
    ) -> List[Tuple[float, float]]:
        """åˆ†å‰²éé•·çš„ chunk"""
        duration = end - start
        num_splits = int(np.ceil(duration / self.max_chunk_duration))
        target_duration = duration / num_splits

        # æ‰¾åˆ°é€™å€‹ç¯„åœå…§çš„æ‰€æœ‰èªéŸ³æ®µè½
        relevant_segments = [
            seg for seg in voice_segments
            if seg.start >= start and seg.end <= end
        ]

        if len(relevant_segments) < 2:
            # æ²’æœ‰è¶³å¤ çš„æ®µè½ï¼Œå‡åˆ†
            chunks = []
            for i in range(num_splits):
                chunk_start = start + i * target_duration
                chunk_end = start + (i + 1) * target_duration
                chunks.append((chunk_start, min(chunk_end, end)))
            return chunks

        # åœ¨éœéŸ³è™•åˆ†å‰²
        chunks = []
        current_start = start
        accumulated_duration = 0

        for i, seg in enumerate(relevant_segments[:-1]):
            accumulated_duration = seg.end - current_start
            gap = relevant_segments[i + 1].start - seg.end

            # å¦‚æœç´¯ç©æ™‚é•·æ¥è¿‘ç›®æ¨™ä¸”æœ‰éœéŸ³ï¼Œåˆ†å‰²
            if accumulated_duration >= target_duration * 0.8 and gap > 0.3:
                chunks.append((current_start, seg.end))
                current_start = relevant_segments[i + 1].start

        # æœ€å¾Œä¸€å€‹ chunk
        chunks.append((current_start, end))

        return chunks

    # ==================== é‡ç–Šçª—å£è½‰éŒ„ ====================

    def transcribe_with_overlap(
        self,
        audio_path: str,
        asr_model,
        progress_callback: Optional[Callable] = None
    ) -> List[TranscriptionChunk]:
        """
        é‡ç–Šçª—å£è½‰éŒ„

        ç­–ç•¥ï¼š
        1. æ¯ 15 ç§’é–‹å§‹ä¸€å€‹æ–°çš„ 30 ç§’çª—å£
        2. é‡ç–Šå€åŸŸï¼ˆ3 ç§’ï¼‰é€²è¡Œå…±è­˜æŠ•ç¥¨
        3. é¸æ“‡ä¿¡å¿ƒåˆ†æ•¸æ›´é«˜çš„ç‰ˆæœ¬

        Args:
            audio_path: éŸ³é »è·¯å¾‘
            asr_model: ASR æ¨¡å‹
            progress_callback: é€²åº¦å›èª¿

        Returns:
            è½‰éŒ„çµæœåˆ—è¡¨
        """
        logger.info("ğŸ”„ åŸ·è¡Œé‡ç–Šçª—å£è½‰éŒ„...")

        import soundfile as sf
        audio, sr = sf.read(audio_path)
        total_duration = len(audio) / sr

        # ç”Ÿæˆé‡ç–Šçª—å£
        window_size = 30.0  # ç§’
        step_size = 15.0    # æ­¥é•·ï¼ˆ= çª—å£ - é‡ç–Šï¼‰

        windows = []
        start = 0
        while start < total_duration:
            end = min(start + window_size, total_duration)
            windows.append((start, end))
            start += step_size
            if end >= total_duration:
                break

        logger.info(f"ç”Ÿæˆ {len(windows)} å€‹é‡ç–Šçª—å£")

        # è½‰éŒ„æ¯å€‹çª—å£
        all_results = []
        for i, (win_start, win_end) in enumerate(windows):
            if progress_callback:
                progress_callback(int((i / len(windows)) * 80))

            # æå–éŸ³é »ç‰‡æ®µ
            start_sample = int(win_start * sr)
            end_sample = int(win_end * sr)
            chunk_audio = audio[start_sample:end_sample]

            # ä¿å­˜è‡¨æ™‚æ–‡ä»¶
            chunk_path = self.temp_dir / f"overlap_chunk_{i}.wav"
            sf.write(str(chunk_path), chunk_audio, sr)

            # è½‰éŒ„
            try:
                result = asr_model.transcribe(
                    str(chunk_path),
                    language='yue'
                )

                segments = result.get('segments', [])
                for seg in segments:
                    # èª¿æ•´æ™‚é–“æˆ³ï¼ˆç›¸å°æ–¼æ•´å€‹éŸ³é »ï¼‰
                    chunk = TranscriptionChunk(
                        start=win_start + seg.start,
                        end=win_start + seg.end,
                        text=seg.text,
                        confidence=getattr(seg, 'confidence', 0.8),
                        words=getattr(seg, 'words', [])
                    )
                    all_results.append((i, chunk))  # ä¿å­˜çª—å£ç´¢å¼•

            except Exception as e:
                logger.warning(f"çª—å£ {i} è½‰éŒ„å¤±æ•—: {e}")

            # æ¸…ç†è‡¨æ™‚æ–‡ä»¶
            chunk_path.unlink(missing_ok=True)

        # åˆä½µé‡ç–Šå€åŸŸ
        final_results = self._merge_overlapping_results(all_results, windows)

        logger.info(f"âœ… é‡ç–Šçª—å£è½‰éŒ„å®Œæˆï¼š{len(final_results)} å€‹æ®µè½")
        return final_results

    def _merge_overlapping_results(
        self,
        all_results: List[Tuple[int, TranscriptionChunk]],
        windows: List[Tuple[float, float]]
    ) -> List[TranscriptionChunk]:
        """
        åˆä½µé‡ç–Šå€åŸŸçš„è½‰éŒ„çµæœ

        ç­–ç•¥ï¼šå°æ–¼é‡ç–Šå€åŸŸï¼Œæ¯”è¼ƒå…©å€‹ç‰ˆæœ¬ä¸¦é¸æ“‡æ›´å¥½çš„
        """
        if not all_results:
            return []

        # æŒ‰æ™‚é–“æ’åº
        sorted_results = sorted(all_results, key=lambda x: x[1].start)

        final_chunks = []
        processed_times = set()

        for window_idx, chunk in sorted_results:
            # æª¢æŸ¥é€™å€‹æ™‚é–“æ®µæ˜¯å¦å·²è™•ç†
            time_key = (round(chunk.start, 1), round(chunk.end, 1))
            if time_key in processed_times:
                continue

            # æŸ¥æ‰¾åŒä¸€æ™‚é–“æ®µçš„å…¶ä»–ç‰ˆæœ¬
            alternatives = [
                (idx, c) for idx, c in sorted_results
                if idx != window_idx
                and self._time_overlap(chunk, c) > 0.5  # é‡ç–Šè¶…é 50%
            ]

            if alternatives:
                # æœ‰æ›¿ä»£ç‰ˆæœ¬ï¼Œæ¯”è¼ƒä¸¦é¸æ“‡æœ€ä½³
                best_chunk = chunk
                best_confidence = chunk.confidence

                for _, alt_chunk in alternatives:
                    if alt_chunk.confidence > best_confidence:
                        best_chunk = alt_chunk
                        best_confidence = alt_chunk.confidence

                final_chunks.append(best_chunk)
            else:
                final_chunks.append(chunk)

            processed_times.add(time_key)

        # æŒ‰æ™‚é–“æ’åº
        final_chunks.sort(key=lambda x: x.start)

        return final_chunks

    def _time_overlap(self, chunk1: TranscriptionChunk, chunk2: TranscriptionChunk) -> float:
        """è¨ˆç®—å…©å€‹ chunk çš„æ™‚é–“é‡ç–Šæ¯”ä¾‹"""
        overlap_start = max(chunk1.start, chunk2.start)
        overlap_end = min(chunk1.end, chunk2.end)

        if overlap_end <= overlap_start:
            return 0.0

        overlap_duration = overlap_end - overlap_start
        min_duration = min(chunk1.end - chunk1.start, chunk2.end - chunk2.start)

        return overlap_duration / min_duration if min_duration > 0 else 0.0

    # ==================== ä¸‰éšæ®µè½‰éŒ„ ====================

    def three_stage_transcribe(
        self,
        audio_path: str,
        asr_model,
        vad_processor=None,
        progress_callback: Optional[Callable] = None,
        status_callback: Optional[Callable] = None
    ) -> List[TranscriptionChunk]:
        """
        ä¸‰éšæ®µè½‰éŒ„æµç¨‹

        éšæ®µ 1ï¼šå¿«é€Ÿç²—è½‰éŒ„ï¼ˆç²å–æ•´é«”çµæ§‹ï¼‰
        éšæ®µ 2ï¼šæ¨™è¨˜ä½ä¿¡å¿ƒå€åŸŸï¼Œé‡æ–°è½‰éŒ„
        éšæ®µ 3ï¼šä½¿ç”¨å®Œæ•´ä¸Šä¸‹æ–‡é€²è¡Œ LLM æ ¡æ­£

        Args:
            audio_path: éŸ³é »è·¯å¾‘
            asr_model: ASR æ¨¡å‹
            vad_processor: VAD è™•ç†å™¨
            progress_callback: é€²åº¦å›èª¿
            status_callback: ç‹€æ…‹å›èª¿

        Returns:
            æœ€çµ‚è½‰éŒ„çµæœ
        """
        logger.info("ğŸ¯ åŸ·è¡Œä¸‰éšæ®µè½‰éŒ„...")

        # ========== éšæ®µ 1ï¼šå¿«é€Ÿç²—è½‰éŒ„ ==========
        if status_callback:
            status_callback("éšæ®µ 1/3ï¼šå¿«é€Ÿè½‰éŒ„...")
        if progress_callback:
            progress_callback(10)

        logger.info("ğŸ“ éšæ®µ 1ï¼šå¿«é€Ÿç²—è½‰éŒ„")

        # ä½¿ç”¨ VAD é åˆ†å‰²
        chunks = self.vad_presplit(audio_path, vad_processor)

        # è½‰éŒ„æ¯å€‹ chunk
        stage1_results = []
        import soundfile as sf
        audio, sr = sf.read(audio_path)

        for i, (start, end) in enumerate(chunks):
            if progress_callback:
                progress_callback(10 + int((i / len(chunks)) * 30))

            # æå–éŸ³é »ç‰‡æ®µ
            start_sample = int(start * sr)
            end_sample = int(end * sr)
            chunk_audio = audio[start_sample:end_sample]

            # ä¿å­˜è‡¨æ™‚æ–‡ä»¶
            chunk_path = self.temp_dir / f"stage1_chunk_{i}.wav"
            sf.write(str(chunk_path), chunk_audio, sr)

            # è½‰éŒ„
            try:
                result = asr_model.transcribe(str(chunk_path), language='yue')
                segments = result.get('segments', [])

                for seg in segments:
                    chunk = TranscriptionChunk(
                        start=start + seg.start,
                        end=start + seg.end,
                        text=seg.text,
                        confidence=getattr(seg, 'confidence', 0.7),
                        words=getattr(seg, 'words', [])
                    )
                    stage1_results.append(chunk)

            except Exception as e:
                logger.warning(f"éšæ®µ 1 chunk {i} å¤±æ•—: {e}")

            chunk_path.unlink(missing_ok=True)

        logger.info(f"éšæ®µ 1 å®Œæˆï¼š{len(stage1_results)} å€‹æ®µè½")

        # ========== éšæ®µ 2ï¼šé‡è½‰éŒ„ä½ä¿¡å¿ƒå€åŸŸ ==========
        if status_callback:
            status_callback("éšæ®µ 2/3ï¼šå„ªåŒ–ä½ä¿¡å¿ƒå€åŸŸ...")
        if progress_callback:
            progress_callback(45)

        logger.info("ğŸ” éšæ®µ 2ï¼šé‡è½‰éŒ„ä½ä¿¡å¿ƒå€åŸŸ")

        # æ¨™è¨˜éŒ¨é»å’Œä½ä¿¡å¿ƒå€åŸŸ
        anchors = []
        low_confidence = []

        for chunk in stage1_results:
            if chunk.confidence >= self.anchor_confidence_threshold:
                chunk.is_anchor = True
                anchors.append(chunk)
            elif chunk.confidence < 0.6:
                low_confidence.append(chunk)

        logger.info(f"éŒ¨é»ï¼š{len(anchors)} å€‹ï¼Œä½ä¿¡å¿ƒï¼š{len(low_confidence)} å€‹")

        # é‡è½‰éŒ„ä½ä¿¡å¿ƒå€åŸŸï¼ˆä½¿ç”¨æ›´é•·çš„ä¸Šä¸‹æ–‡ï¼‰
        stage2_results = []
        for chunk in stage1_results:
            if chunk.confidence < 0.6:
                # æ“´å±•æ™‚é–“ç¯„åœï¼ŒåŒ…å«æ›´å¤šä¸Šä¸‹æ–‡
                extended_start = max(0, chunk.start - 2.0)
                extended_end = min(len(audio) / sr, chunk.end + 2.0)

                start_sample = int(extended_start * sr)
                end_sample = int(extended_end * sr)
                extended_audio = audio[start_sample:end_sample]

                chunk_path = self.temp_dir / "stage2_retry.wav"
                sf.write(str(chunk_path), extended_audio, sr)

                try:
                    result = asr_model.transcribe(
                        str(chunk_path),
                        language='yue',
                        temperature=0.0  # æ›´ç¢ºå®šæ€§çš„è¼¸å‡º
                    )

                    segments = result.get('segments', [])
                    if segments:
                        # æ‰¾åˆ°å°æ‡‰æ™‚é–“æ®µçš„çµæœ
                        for seg in segments:
                            seg_start = extended_start + seg.start
                            seg_end = extended_start + seg.end

                            # åªä¿ç•™åŸå§‹æ™‚é–“ç¯„åœå…§çš„çµæœ
                            if seg_start >= chunk.start - 0.5 and seg_end <= chunk.end + 0.5:
                                new_chunk = TranscriptionChunk(
                                    start=seg_start,
                                    end=seg_end,
                                    text=seg.text,
                                    confidence=min(getattr(seg, 'confidence', 0.7) + 0.1, 1.0),
                                    words=getattr(seg, 'words', [])
                                )
                                stage2_results.append(new_chunk)
                                break
                        else:
                            # æ²’æ‰¾åˆ°å°æ‡‰çµæœï¼Œä¿ç•™åŸå§‹
                            stage2_results.append(chunk)
                    else:
                        stage2_results.append(chunk)

                except Exception as e:
                    logger.warning(f"éšæ®µ 2 é‡è½‰éŒ„å¤±æ•—: {e}")
                    stage2_results.append(chunk)

                chunk_path.unlink(missing_ok=True)
            else:
                stage2_results.append(chunk)

        if progress_callback:
            progress_callback(70)

        # ========== éšæ®µ 3ï¼šLLM ä¸Šä¸‹æ–‡æ ¡æ­£ ==========
        if status_callback:
            status_callback("éšæ®µ 3/3ï¼šAI ä¸Šä¸‹æ–‡æ ¡æ­£...")

        logger.info("ğŸ¤– éšæ®µ 3ï¼šLLM ä¸Šä¸‹æ–‡æ ¡æ­£")

        # ä½¿ç”¨éŒ¨é»é©—è­‰å‘¨åœçš„ä½ä¿¡å¿ƒå€åŸŸ
        stage3_results = self._anchor_based_correction(stage2_results, anchors)

        if progress_callback:
            progress_callback(90)

        logger.info(f"âœ… ä¸‰éšæ®µè½‰éŒ„å®Œæˆï¼š{len(stage3_results)} å€‹æ®µè½")
        return stage3_results

    # ==================== éŒ¨é»ç³»çµ± ====================

    def _anchor_based_correction(
        self,
        results: List[TranscriptionChunk],
        anchors: List[TranscriptionChunk]
    ) -> List[TranscriptionChunk]:
        """
        åŸºæ–¼éŒ¨é»çš„æ ¡æ­£

        ç­–ç•¥ï¼š
        1. é«˜ä¿¡å¿ƒéŒ¨é»ä½œç‚ºã€ŒçœŸå¯¦ã€åƒç…§
        2. ä½ä¿¡å¿ƒå€åŸŸèˆ‡ç›¸é„°éŒ¨é»æ¯”è¼ƒ
        3. å¦‚æœä½ä¿¡å¿ƒå€åŸŸèˆ‡éŒ¨é»å…§å®¹ä¸é€£è²«ï¼Œå˜—è©¦ä¿®æ­£
        """
        if not anchors or len(results) < 2:
            return results

        logger.info("âš“ åŸ·è¡ŒéŒ¨é»æ ¡æ­£...")

        corrected = []

        for i, chunk in enumerate(results):
            if chunk.is_anchor:
                corrected.append(chunk)
                continue

            # æ‰¾åˆ°ç›¸é„°çš„éŒ¨é»
            prev_anchor = None
            next_anchor = None

            for j in range(i - 1, -1, -1):
                if results[j].is_anchor:
                    prev_anchor = results[j]
                    break

            for j in range(i + 1, len(results)):
                if results[j].is_anchor:
                    next_anchor = results[j]
                    break

            # ä½¿ç”¨éŒ¨é»ä¸Šä¸‹æ–‡é€²è¡Œæ ¡æ­£
            if prev_anchor or next_anchor:
                corrected_chunk = self._correct_with_anchors(
                    chunk, prev_anchor, next_anchor
                )
                corrected.append(corrected_chunk)
            else:
                corrected.append(chunk)

        return corrected

    def _correct_with_anchors(
        self,
        chunk: TranscriptionChunk,
        prev_anchor: Optional[TranscriptionChunk],
        next_anchor: Optional[TranscriptionChunk]
    ) -> TranscriptionChunk:
        """
        ä½¿ç”¨éŒ¨é»ä¸Šä¸‹æ–‡æ ¡æ­£ chunk

        é€™è£¡å¯ä»¥åŠ å…¥ LLM æ ¡æ­£é‚è¼¯
        """
        # ç°¡å–®ç‰ˆæœ¬ï¼šå¦‚æœæ–‡æœ¬çœ‹èµ·ä¾†ä¸å®Œæ•´ï¼Œå˜—è©¦ä¿®æ­£
        text = chunk.text.strip()

        # æª¢æŸ¥æ˜¯å¦ä»¥èªæ°£è©é–‹é ­ï¼ˆå¯èƒ½æ˜¯æ–·å¥éŒ¯èª¤ï¼‰
        particles = {'å—', 'å‘€', 'å•¦', 'å–', 'å›‰', 'å’©', 'å•Š', 'å‘¢', 'å–‡'}
        if text and text[0] in particles and prev_anchor:
            # å¯èƒ½æ˜¯èªæ°£è©æ‡‰è©²å±¬æ–¼ä¸Šä¸€å¥
            logger.debug(f"éŒ¨é»æ ¡æ­£ï¼šæª¢æ¸¬åˆ°å¥é¦–èªæ°£è© '{text[0]}'")

        # è¿”å›åŸå§‹ chunkï¼ˆæœªä¾†å¯ä»¥åŠ å…¥æ›´è¤‡é›œçš„ LLM æ ¡æ­£ï¼‰
        return chunk

    # ==================== ç¶œåˆæ–¹æ³• ====================

    def transcribe_ultimate(
        self,
        audio_path: str,
        asr_model,
        vad_processor=None,
        enable_audio_enhance: bool = True,
        progress_callback: Optional[Callable] = None,
        status_callback: Optional[Callable] = None
    ) -> List[TranscriptionChunk]:
        """
        çµ‚æ¥µè½‰éŒ„ - æ•´åˆæ‰€æœ‰å„ªåŒ–ç­–ç•¥

        æµç¨‹ï¼š
        1. éŸ³é »é è™•ç†ï¼ˆé™å™ªã€äººè²å¢å¼·ï¼‰
        2. VAD é åˆ†å‰²
        3. é‡ç–Šçª—å£è½‰éŒ„
        4. ä¸‰éšæ®µå„ªåŒ–
        5. éŒ¨é»æ ¡æ­£

        Args:
            audio_path: éŸ³é »è·¯å¾‘
            asr_model: ASR æ¨¡å‹
            vad_processor: VAD è™•ç†å™¨
            enable_audio_enhance: æ˜¯å¦å•Ÿç”¨éŸ³é »å¢å¼·
            progress_callback: é€²åº¦å›èª¿
            status_callback: ç‹€æ…‹å›èª¿

        Returns:
            æœ€çµ‚è½‰éŒ„çµæœ
        """
        logger.info("ğŸš€ åŸ·è¡Œçµ‚æ¥µè½‰éŒ„...")

        # Step 1: éŸ³é »é è™•ç†
        if enable_audio_enhance:
            if status_callback:
                status_callback("é è™•ç†éŸ³é »...")
            if progress_callback:
                progress_callback(5)

            from utils.audio_enhancer import AudioEnhancer
            enhancer = AudioEnhancer(self.temp_dir)

            # åˆ†æéŸ³é »è³ªé‡
            quality = enhancer.analyze_audio_quality(audio_path)
            logger.info(f"éŸ³é »è³ªé‡åˆ†æ: SNR={quality['snr_estimate']:.1f}dB")

            if quality['needs_enhancement']:
                logger.info("éŸ³é »éœ€è¦å¢å¼·ï¼ŒåŸ·è¡Œé è™•ç†...")
                audio_path = enhancer.quick_enhance(audio_path)
            else:
                logger.info("éŸ³é »è³ªé‡è‰¯å¥½ï¼Œè·³éé è™•ç†")

        # Step 2: ä¸‰éšæ®µè½‰éŒ„ï¼ˆåŒ…å« VAD é åˆ†å‰²å’ŒéŒ¨é»æ ¡æ­£ï¼‰
        results = self.three_stage_transcribe(
            audio_path,
            asr_model,
            vad_processor,
            progress_callback,
            status_callback
        )

        if progress_callback:
            progress_callback(100)

        logger.info(f"âœ… çµ‚æ¥µè½‰éŒ„å®Œæˆï¼š{len(results)} å€‹æ®µè½")
        return results

    def cleanup(self):
        """æ¸…ç†è‡¨æ™‚æ–‡ä»¶"""
        import shutil
        try:
            for f in self.temp_dir.glob("*.wav"):
                f.unlink()
            logger.info("âœ… è‡¨æ™‚æ–‡ä»¶å·²æ¸…ç†")
        except Exception as e:
            logger.warning(f"æ¸…ç†å¤±æ•—: {e}")
